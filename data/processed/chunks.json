[
  {
    "chunk_id": 0,
    "doc_id": "Albert_Einstein.txt",
    "text": "albert einsteina (14 march 1879 18 april 1955) was a german-born theoretical physicist best known for developing the theory of relativity. einstein also made important contributions to quantum theory.15 his massenergy equivalence formula e mc2, which arises from special relativity, has been called the worlds most famous equation.6 he received the 1921 nobel prize in physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.7 born in the german empire, einstein moved to switzerland in 1895, forsaking his german citizenship (as a subject of the kingdom of wrttemberg)note 1 the following year. in 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the swiss federal polytechnic school in zurich, graduating in 1900. he acquired swiss citizenship a year later, which he kept for the rest of his life, and afterwards secured a permanent position at the swiss patent office in bern. in 1905, he submitted a successful phd dissertation to the university of zurich. in 1914, he moved to berlin to join the prussian academy of sciences and the humboldt university of berlin, becoming director of the kaiser wilhelm institute for physics in 1917; he also became a german citizen again, this time as a subject of the kingdom of prussia.note 1 in 1933, while einstein was visiting the united states, adolf hitler came to power in germany. horrified by the nazi persecution of his fellow jews,8 he decided to remain in the us, and was granted american citizenship in 1940.9 on the eve of world war ii, he endorsed a letter to president franklin d. roosevelt alerting him to the potential german nuclear weapons program and recommending that the us begin similar research, later carried out as the manhattan project. in 1905, sometimes described as his annus mirabilis (miracle year), he published four groundbreaking papers.10 in them, he outlined a theory of the photoelectric effect, explained brownian motion, introduced his special theory of relativity, and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. in 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. a paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.1112 it introduced the cosmological"
  },
  {
    "chunk_id": 1,
    "doc_id": "Albert_Einstein.txt",
    "text": "1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. a paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.1112 it introduced the cosmological constant and is further regarded as the first step in the field of modern theoretical cosmology. in 1917, einstein wrote a paper which introduced the concepts of spontaneous emission and stimulated emission, the latter of which is the core mechanism behind the laser and maser, and which contained a trove of information that would be beneficial to developments in physics later on, such as quantum electrodynamics and quantum optics.13 in the middle part of his career, einstein made important contributions to statistical mechanics and quantum theory. especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. with physicist satyendra nath bose, he laid the groundwork for boseeinstein statistics. for much of the last phase of his academic life, einstein worked on two endeavors that ultimately proved unsuccessful. first, he advocated against quantum theorys introduction of fundamental randomness into sciences picture of the world, objecting that god does not play dice.14 second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism. as a result, he became increasingly isolated from mainstream modern physics. many things are named after him, including the element einsteinium. in 1999, he was named times person of the century.15 life and career childhood, youth and education see also: einstein family a young boy with short hair and a round face, wearing a white collar and large bow, with vest, coat, skirt, and high boots. he is leaning against an ornate chair. einstein in 1882, age 3 albert einstein was born in ulm,16 in the kingdom of wrttemberg in the german empire, on 14 march 1879. his parents, secular ashkenazi jews, were hermann einstein, a salesman and engineer, and pauline koch. in 1880, the family moved to munichs borough of ludwigsvorstadt-isarvorstadt, where einsteins father and his uncle jakob founded elektrotechnische fabrik j. einstein cie, a company that manufactured electrical equipment based on direct current.16 when he was very young, his parents worried that he had a learning disability because he was very slow to learn to talk.17 when he was"
  },
  {
    "chunk_id": 2,
    "doc_id": "Albert_Einstein.txt",
    "text": "ludwigsvorstadt-isarvorstadt, where einsteins father and his uncle jakob founded elektrotechnische fabrik j. einstein cie, a company that manufactured electrical equipment based on direct current.16 when he was very young, his parents worried that he had a learning disability because he was very slow to learn to talk.17 when he was five and sick in bed, his father brought him a compass. this sparked his lifelong fascination with electromagnetism. he realized that something deeply hidden had to be behind things.18 einstein attended st. peters catholic elementary school in munich from the age of five. when he was eight, he was transferred to the luitpold gymnasium, where he received advanced primary and then secondary school education.19 einsteins parents, hermann and pauline in 1894, hermann and jakobs company tendered for a contract to install electric lighting in munich, but without successthey lacked the capital that would have been required to update their technology from direct current to the more efficient, alternating current alternative.20 the failure of their bid forced them to sell their munich factory and search for new opportunities elsewhere. the einstein family moved to italy, first to milan and a few months later to pavia, where they settled in palazzo cornazzani.21 einstein, then fifteen, stayed behind in munich in order to finish his schooling. his father wanted him to study electrical engineering, but he was a fractious pupil who found the gymnasiums regimen and teaching methods far from congenial. he later wrote that the schools policy of strict rote learning was harmful to creativity. at the end of december 1894, a letter from a doctor persuaded the luitpolds authorities to release him from its care, and he joined his family in pavia.22 while in italy as a teenager, he wrote an essay entitled on the investigation of the state of the ether in a magnetic field.2324 einstein excelled at physics and mathematics from an early age, and soon acquired the mathematical expertise normally only found in a child several years his senior. he began teaching himself algebra, calculus and euclidean geometry when he was twelve; he made such rapid progress that he discovered an original proof of the pythagorean theorem before his thirteenth birthday.2526 a family tutor, max talmud, said that only a short time after he had given the twelve year old einstein a geometry textbook, the boy had worked through the whole book. he thereupon devoted himself to higher"
  },
  {
    "chunk_id": 3,
    "doc_id": "Albert_Einstein.txt",
    "text": "he discovered an original proof of the pythagorean theorem before his thirteenth birthday.2526 a family tutor, max talmud, said that only a short time after he had given the twelve year old einstein a geometry textbook, the boy had worked through the whole book. he thereupon devoted himself to higher mathematics ... soon the flight of his mathematical genius was so high i could not follow.27 einstein recorded that he had mastered integral and differential calculus while still just fourteen.28 his love of algebra and geometry was so great that at twelve, he was already confident that nature could be understood as a mathematical structure.29 studio photo of a boy seated in a relaxed posture and wearing a suit, posed in front of a backdrop of scenery. einstein in 1893, age 14 at thirteen, when his range of enthusiasms had broadened to include music and philosophy,30 talmud introduced einstein to kants critique of pure reason. kant became his favorite philosopher; according to talmud, at the time he was still a child, only thirteen years old, yet kants works, incomprehensible to ordinary mortals, seemed to be clear to him.27 in 1895, at the age of sixteen, einstein sat the entrance examination for the federal polytechnic school (later the eidgenssische technische hochschule, eth) in zurich, switzerland. he failed to reach the required standard in the general part of the test,31 but performed with distinction in physics and mathematics.32 on the advice of the polytechnics principal, he completed his secondary education at the argovian cantonal school (a gymnasium) in aarau, switzerland, graduating in 1896.33 while lodging in aarau with the family of jost winteler, he fell in love with wintelers daughter, marie. (his sister, maja, later married wintelers son paul.34) einsteins matriculation certificate at the age of 17. the heading translates as the education committee of the canton of aargau. his scores were german 5, french 3, italian 5, history 6, geography 4, algebra 6, geometry 6, descriptive geometry 6, physics 6, chemistry 5, natural history 5, art drawing 4, technical drawing 4. 6 very good, 5 good, 4 sufficient, 3 insufficient, 2 poor, 1 very poor. einsteins matura certificate from canton aargau, 1896note 2 in january 1896, with his fathers approval, einstein renounced his citizenship of the german kingdom of wrttemberg in order to avoid conscription into military service.35 the matura (graduation for the successful completion of higher secondary schooling), awarded to"
  },
  {
    "chunk_id": 4,
    "doc_id": "Albert_Einstein.txt",
    "text": "poor, 1 very poor. einsteins matura certificate from canton aargau, 1896note 2 in january 1896, with his fathers approval, einstein renounced his citizenship of the german kingdom of wrttemberg in order to avoid conscription into military service.35 the matura (graduation for the successful completion of higher secondary schooling), awarded to him in september 1896, acknowledged him to have performed well across most of the curriculum, allotting him a top grade of 6 for history, physics, algebra, geometry, and descriptive geometry.36 at seventeen, he enrolled in the four-year mathematics and physics teaching diploma program at the federal polytechnic school. he befriended fellow student marcel grossmann, who would help him there to get by despite his loose study habits, and later to mathematically underpin his revolutionary insights into physics. marie winteler, a year older than him, took up a teaching post in olsberg, switzerland.34 the five other polytechnic school freshmen following the same course as einstein included just one woman, a twenty year old serbian, mileva mari. over the next few years, the pair spent many hours discussing their shared interests and learning about topics in physics that the polytechnic schools lectures did not cover. in his letters to mari, einstein confessed that exploring science with her by his side was much more enjoyable than reading a textbook in solitude. eventually the two students became not only friends but also lovers.37 historians of physics are divided on the question of the extent to which mari contributed to the insights of einsteins annus mirabilis publications. there is at least some evidence that he was influenced by her scientific ideas,373839 but there are scholars who doubt whether her impact on his thought was of any great significance at all.40414243 marriages, relationships and children albert einstein and mileva mari einstein, 1912 correspondence between einstein and mari, discovered and published in 1987, revealed that in early 1902, while mari was visiting her parents in novi sad, she gave birth to a daughter, lieserl. when mari returned to switzerland it was without the child, whose fate is uncertain. a letter of einsteins that he wrote in september 1903 suggests that the girl was either given up for adoption or died of scarlet fever in infancy.4445 einstein and mari married in january 1903. in may 1904, their son hans albert was born in bern, switzerland. their son eduard was born in zurich in july 1910. in letters that"
  },
  {
    "chunk_id": 5,
    "doc_id": "Albert_Einstein.txt",
    "text": "1903 suggests that the girl was either given up for adoption or died of scarlet fever in infancy.4445 einstein and mari married in january 1903. in may 1904, their son hans albert was born in bern, switzerland. their son eduard was born in zurich in july 1910. in letters that einstein wrote to marie winteler in the months before eduards arrival, he described his love for his wife as misguided and mourned the missed life that he imagined he would have enjoyed if he had married winteler instead: i think of you in heartfelt love every spare minute and am so unhappy as only a man can be.46 einstein, looking relaxed and holding a pipe, stands next to a smiling, well-dressed elsa who is wearing a fancy hat and fur wrap. she is looking at him. albert and elsa einstein arriving in new york, 1921 in 1912, einstein entered into a relationship with elsa lwenthal, who was both his first cousin on his mothers side and his second cousin on his fathers.474849 when mari learned of his infidelity soon after moving to berlin with him in april 1914, she returned to zurich, taking hans albert and eduard with her.37 einstein and mari were granted a divorce on 14 february 1919 on the grounds of having lived apart for five years.5051 as part of the divorce settlement, einstein agreed that if he were to win a nobel prize, he would give the money that he received to mari; he won the prize two years later.52 einstein married lwenthal in 1919.5354 in 1923, he began a relationship with a secretary named betty neumann, the niece of his close friend hans mhsam.55565758 lwenthal nevertheless remained loyal to him, accompanying him when he emigrated to the united states in 1933. in 1935, she was diagnosed with heart and kidney problems. she died in december 1936.59 albert and elsa einstein, 1930 a volume of einsteins letters released by hebrew university of jerusalem in 200660 added some other women with whom he was romantically involved. they included margarete lebach (a married austrian),61 estella katzenellenbogen (the rich owner of a florist business), toni mendel (a wealthy jewish widow) and ethel michanowski (a berlin socialite), with whom he spent time and from whom he accepted gifts while married to lwenthal.6263 after being widowed, einstein was briefly in a relationship with margarita konenkova, thought by some to be a"
  },
  {
    "chunk_id": 6,
    "doc_id": "Albert_Einstein.txt",
    "text": "owner of a florist business), toni mendel (a wealthy jewish widow) and ethel michanowski (a berlin socialite), with whom he spent time and from whom he accepted gifts while married to lwenthal.6263 after being widowed, einstein was briefly in a relationship with margarita konenkova, thought by some to be a russian spy; her husband, the russian sculptor sergei konenkov, created the bronze bust of einstein at the institute for advanced study at princeton.6465 following an episode of acute mental illness at about the age of twenty, einsteins son eduard was diagnosed with schizophrenia.66 he spent the remainder of his life either in the care of his mother or in temporary confinement in an asylum. after her death, he was committed permanently to burghlzli, the psychiatric university hospital in zurich.67 assistant at the swiss patent office (19021909) head and shoulders shot of a young, mustached man with dark, curly hair wearing a plaid suit and vest, striped shirt, and a dark tie. einstein at the swiss patent office, 1904 einstein graduated from the federal polytechnic school in 1900, duly certified as competent to teach mathematics and physics.68 his successful acquisition of swiss citizenship in february 190169 was not followed by the usual sequel of conscription; the swiss authorities deemed him medically unfit for military service. he found that swiss schools too appeared to have no use for him, failing to offer him a teaching position despite the almost two years that he spent applying for one. eventually it was with the help of marcel grossmanns father that he secured a post in bern at the swiss patent office,7071 as an assistant examiner level iii.7273 patent applications that landed on einsteins desk for his evaluation included ideas for a gravel sorter and an electric typewriter.73 his employers were pleased enough with his work to make his position permanent in 1903, although they did not think that he should be promoted until he had fully mastered machine technology.74 it is conceivable that his labors at the patent office had a bearing on his development of his special theory of relativity. he arrived at his revolutionary ideas about space, time and light through thought experiments about the transmission of signals and the synchronization of clocks, matters which also figured in some of the inventions submitted to him for assessment.10 in 1902, einstein and some friends whom he had met in bern formed a group"
  },
  {
    "chunk_id": 7,
    "doc_id": "Albert_Einstein.txt",
    "text": "his revolutionary ideas about space, time and light through thought experiments about the transmission of signals and the synchronization of clocks, matters which also figured in some of the inventions submitted to him for assessment.10 in 1902, einstein and some friends whom he had met in bern formed a group that held regular meetings to discuss science and philosophy. their choice of a name for their club, the olympia academy, was an ironic comment upon its far from olympian status. sometimes they were joined by mari, who limited her participation in their proceedings to careful listening.75 the thinkers whose works they reflected upon included henri poincar, ernst mach and david hume, all of whom significantly influenced einsteins own subsequent ideas and beliefs.76 first scientific papers (19001905) cover image of the phd dissertation of albert einstein einsteins 1905 dissertation, eine neue bestimmung der molekldimensionen (a new determination of molecular dimensions) einsteins first paper, folgerungen aus den capillarittserscheinungen (conclusions drawn from the phenomena of capillarity), in which he proposed a model of intermolecular attraction that he afterwards disavowed as worthless, was published in the journal annalen der physik in 1901.7778 his 24-page doctoral dissertation also addressed a topic in molecular physics. titled eine neue bestimmung der molekldimensionen (a new determination of molecular dimensions) and dedicated meinem freunde herr dr. marcel grossmann gewidmet (to his friend marcel grossman), it was completed on 30 april 190579 and approved by professor alfred kleiner of the university of zurich three months later. (einstein was formally awarded his phd on 15 january 1906.)798081 four other pieces of work that einstein completed in 1905his famous papers on the photoelectric effect, brownian motion, his special theory of relativity and the equivalence of mass and energyhave led to the year being celebrated as an annus mirabilis for physics akin to the miracle year of 1666 when isaac newton experienced his greatest epiphanies. the publications deeply impressed einsteins contemporaries.82 academic career in europe (19081933) einsteins sabbatical as a civil servant approached its end in 1908, when he secured a junior teaching position at the university of bern. in 1909, a lecture on relativistic electrodynamics that he gave at the university of zurich, much admired by alfred kleiner, led to zurichs luring him away from bern with a newly created associate professorship.83 promotion to a full professorship followed in april 1911, when he took up a chair at the german charles-ferdinand university"
  },
  {
    "chunk_id": 8,
    "doc_id": "Albert_Einstein.txt",
    "text": "on relativistic electrodynamics that he gave at the university of zurich, much admired by alfred kleiner, led to zurichs luring him away from bern with a newly created associate professorship.83 promotion to a full professorship followed in april 1911, when he took up a chair at the german charles-ferdinand university in prague,84 a move which required him to become an austrian citizen of the austro-hungarian empire, which was not completed.85 his time in prague saw him producing eleven research papers.86 einstein with colleagues at the eth in zurich, 1913 from 30 october to 3 november 1911, einstein attended the first solvay conference on physics.87 in july 1912, he returned to his alma mater, the eth zurich, to take up a chair in theoretical physics. his teaching activities there centered on thermodynamics and analytical mechanics, and his research interests included the molecular theory of heat, continuum mechanics and the development of a relativistic theory of gravitation. in his work on the latter topic, he was assisted by his friend marcel grossmann, whose knowledge of the kind of mathematics required was greater than his own.88 in the spring of 1913, two german visitors, max planck and walther nernst, called upon einstein in zurich in the hope of persuading him to relocate to berlin.89 they offered him membership of the prussian academy of sciences, the directorship of the planned kaiser wilhelm institute for physics and a chair at the humboldt university of berlin that would allow him to pursue his research supported by a professorial salary but with no teaching duties to burden him.48 their invitation was all the more appealing to him because berlin happened to be the home of his latest girlfriend, elsa lwenthal.89 he duly joined the academy on 24 july 1913,90 and moved into an apartment in the berlin district of dahlem on 1 april 1914.48 he was installed in his humboldt university position shortly thereafter.90 einstein with other physicists and chemists in berlin, 1920 the outbreak of the first world war in july 1914 marked the beginning of einsteins gradual estrangement from the nation of his birth. when the manifesto of the ninety-three was published in october 1914a document signed by a host of prominent german thinkers that justified germanys belligerenceeinstein was one of the few german intellectuals to distance himself from it and sign the alternative, irenic manifesto to the europeans instead.91 however, this expression of his"
  },
  {
    "chunk_id": 9,
    "doc_id": "Albert_Einstein.txt",
    "text": "the manifesto of the ninety-three was published in october 1914a document signed by a host of prominent german thinkers that justified germanys belligerenceeinstein was one of the few german intellectuals to distance himself from it and sign the alternative, irenic manifesto to the europeans instead.91 however, this expression of his doubts about german policy did not prevent him from being elected to a two-year term as president of the german physical society in 1916.92 when the kaiser wilhelm institute for physics opened its doors the following yearits foundation delayed because of the wareinstein was appointed its first director, just as planck and nernst had promised.93 einstein was elected a foreign member of the royal netherlands academy of arts and sciences in 1920,94 and a foreign member of the royal society in 1921. in 1922, he was awarded the 1921 nobel prize in physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect.7 at this point some physicists still regarded the general theory of relativity skeptically, and the nobel citation displayed a degree of doubt even about the work on photoelectricity that it acknowledged: it did not assent to einsteins notion of the particulate nature of light, which only won over the entire scientific community when s. n. bose derived the planck spectrum in 1924. that same year, einstein was elected an international honorary member of the american academy of arts and sciences.95 britains closest equivalent of the nobel award, the royal societys copley medal, was not hung around einsteins neck until 1925.1 he was elected an international member of the american philosophical society in 1930.96 einstein resigned from the prussian academy in march 1933. his accomplishments in berlin had included the completion of the general theory of relativity, proving the einsteinde haas effect, contributing to the quantum theory of radiation, and the development of boseeinstein statistics.48 putting general relativity to the test (1919) the new york times reported confirmation of the bending of light by gravitation after observations (made in prncipe and sobral) of the 29 may 1919 eclipse were presented to a joint meeting in london of the royal society and the royal astronomical society on 6 november 1919.97 in 1907, einstein reached a milestone on his long journey from his special theory of relativity to a new idea of gravitation with the formulation of his equivalence principle, which asserts"
  },
  {
    "chunk_id": 10,
    "doc_id": "Albert_Einstein.txt",
    "text": "to a joint meeting in london of the royal society and the royal astronomical society on 6 november 1919.97 in 1907, einstein reached a milestone on his long journey from his special theory of relativity to a new idea of gravitation with the formulation of his equivalence principle, which asserts that an observer in a box falling freely in a gravitational field would be unable to find any evidence that the field exists. in 1911, he used the principle to estimate the amount by which a ray of light from a distant star would be bent by the gravitational pull of the sun as it passed close to the suns photosphere (that is, the suns apparent surface). he reworked his calculation in 1913, having now found a way to model gravitation with the riemann curvature tensor of a non-euclidean four-dimensional spacetime. by the fall of 1915, his reimagining of the mathematics of gravitation in terms of riemannian geometry was complete, and he applied his new theory not just to the behavior of the sun as a gravitational lens but also to another astronomical phenomenon, the precession of the perihelion of mercury (a slow drift in the point in mercurys elliptical orbit at which it approaches the sun most closely).4898 a total eclipse of the sun that took place on 29 may 1919 provided an opportunity to put his theory of gravitational lensing to the test, and observations performed by sir arthur eddington yielded results that were consistent with his calculations. eddingtons work was reported at length in newspapers around the world. on 7 november 1919, for example, the leading british newspaper, the times, printed a banner headline that read: revolution in science new theory of the universe newtonian ideas overthrown.99 coming to terms with fame (19211923) einsteins official portrait after receiving the 1921 nobel prize for physics with eddingtons eclipse observations widely reported not just in academic journals but by the popular press as well, einstein became perhaps the worlds first celebrity scientist, a genius who had shattered a paradigm that had been basic to physicists understanding of the universe since the seventeenth century.100 einstein began his new life as an intellectual icon in america, where he arrived on 2 april 1921. he was welcomed to new york city by mayor john francis hylan, and then spent three weeks giving lectures and attending receptions.101 he spoke several times at columbia"
  },
  {
    "chunk_id": 11,
    "doc_id": "Albert_Einstein.txt",
    "text": "since the seventeenth century.100 einstein began his new life as an intellectual icon in america, where he arrived on 2 april 1921. he was welcomed to new york city by mayor john francis hylan, and then spent three weeks giving lectures and attending receptions.101 he spoke several times at columbia university and princeton, and in washington, he visited the white house with representatives of the national academy of sciences. he returned to europe via london, where he was the guest of the philosopher and statesman viscount haldane. he used his time in the british capital to meet several people prominent in british scientific, political or intellectual life, and to deliver a lecture at kings college.102103 in july 1921, he published an essay, my first impression of the u.s.a., in which he sought to sketch the american character, much as had alexis de tocqueville in democracy in america (1835).104 he wrote of his transatlantic hosts in highly approving terms: what strikes a visitor is the joyous, positive attitude to life ... the american is friendly, self-confident, optimistic, and without envy.105 in 1922, einsteins travels were to the old world rather than the new. he devoted six months to a tour of asia that saw him speaking in japan, singapore and sri lanka (then known as ceylon). after his first public lecture in tokyo, he met emperor yoshihito and his wife at the imperial palace, with thousands of spectators thronging the streets in the hope of catching a glimpse of him. (in a letter to his sons, he wrote that japanese people seemed to him to be generally modest, intelligent and considerate, and to have a true appreciation of art.106 but his picture of them in his diary was less flattering: the intellectual needs of this nation seem to be weaker than their artistic ones natural disposition? his journal also contains views of china and india which were uncomplimentary. of chinese people, he wrote that even the children are spiritless and look obtuse... it would be a pity if these chinese supplant all other races. for the likes of us the mere thought is unspeakably dreary.107108) he was greeted with even greater enthusiasm on the last leg of his tour, in which he spent twelve days in mandatory palestine, newly entrusted to british rule by the league of nations in the aftermath of the first world war. sir herbert samuel, the british"
  },
  {
    "chunk_id": 12,
    "doc_id": "Albert_Einstein.txt",
    "text": "thought is unspeakably dreary.107108) he was greeted with even greater enthusiasm on the last leg of his tour, in which he spent twelve days in mandatory palestine, newly entrusted to british rule by the league of nations in the aftermath of the first world war. sir herbert samuel, the british high commissioner, welcomed him with a degree of ceremony normally only accorded to a visiting head of state, including a cannon salute. one reception held in his honor was stormed by people determined to hear him speak: he told them that he was happy that jews were beginning to be recognized as a force in the world.106 einsteins decision to tour the eastern hemisphere in 1922 meant that he was unable to go to stockholm in the december of that year to participate in the nobel prize ceremony. his place at the traditional nobel banquet was taken by a german diplomat, who gave a speech praising him not only as a physicist but also as a campaigner for peace.109 a two-week visit to spain that he undertook in 1923 saw him collecting another award, a membership of the spanish academy of sciences signified by a diploma handed to him by king alfonso xiii. (his spanish trip also gave him a chance to meet a fellow nobel laureate, the neuroanatomist santiago ramn y cajal.)110 serving the league of nations (19221932) einstein at a session of the international committee on intellectual cooperation (league of nations) of which he was a member from 1922 to 1932 from 1922 until 1932, with the exception of a few months in 1923 and 1924, einstein was a member of the geneva-based international committee on intellectual cooperation of the league of nations, a group set up by the league to encourage scientists, artists, scholars, teachers and other people engaged in the life of the mind to work more closely with their counterparts in other countries.111112 he was appointed as a german delegate rather than as a representative of switzerland because of the machinations of two catholic activists, oskar halecki and giuseppe motta. by persuading secretary general eric drummond to deny einstein the place on the committee reserved for a swiss thinker, they created an opening for gonzague de reynold, who used his league of nations position as a platform from which to promote traditional catholic doctrine.113 einsteins former physics professor hendrik lorentz and the polish chemist marie"
  },
  {
    "chunk_id": 13,
    "doc_id": "Albert_Einstein.txt",
    "text": "drummond to deny einstein the place on the committee reserved for a swiss thinker, they created an opening for gonzague de reynold, who used his league of nations position as a platform from which to promote traditional catholic doctrine.113 einsteins former physics professor hendrik lorentz and the polish chemist marie curie were also members of the committee.114 touring south america (1925) in march and april 1925, einstein and his wife visited south america, where they spent about a week in brazil, a week in uruguay and a month in argentina.115 their tour was suggested by jorge duclout (18561927) and mauricio nirenstein (18771935)116 with the support of several argentine scholars, including julio rey pastor, jakob laub, and leopoldo lugones and was financed primarily by the council of the university of buenos aires and the asociacin hebraica argentina (argentine hebraic association) with a smaller contribution from the argentine-germanic cultural institution.117 touring the us (19301931) einstein in pasadena, california, 1931 in december 1930, einstein began another significant sojourn in the united states, drawn back to the us by the offer of a two month research fellowship at the california institute of technology. caltech supported him in his wish that he should not be exposed to quite as much attention from the media as he had experienced when visiting the us in 1921, and he therefore declined all the invitations to receive prizes or make speeches that his admirers poured down upon him. but he remained willing to allow his fans at least some of the time with him that they requested.118 after arriving in new york city, einstein was taken to various places and events, including chinatown, a lunch with the editors of the new york times, and a performance of carmen at the metropolitan opera, where he was cheered by the audience on his arrival. during the days following, he was given the keys to the city by mayor jimmy walker and met nicholas murray butler, the president of columbia university, who described einstein as the ruling monarch of the mind.119 harry emerson fosdick, pastor at new yorks riverside church, gave einstein a tour of the church and showed him a full-size statue that the church made of einstein, standing at the entrance.119 also during his stay in new york, he joined a crowd of 15,000 people at madison square garden during a hanukkah celebration.119 einstein with charlie chaplin at the hollywood"
  },
  {
    "chunk_id": 14,
    "doc_id": "Albert_Einstein.txt",
    "text": "a tour of the church and showed him a full-size statue that the church made of einstein, standing at the entrance.119 also during his stay in new york, he joined a crowd of 15,000 people at madison square garden during a hanukkah celebration.119 einstein with charlie chaplin at the hollywood premiere of chaplins city lights, january 1931 einstein next traveled to california, where he met caltech president and nobel laureate robert a. millikan. his friendship with millikan was awkward, as millikan had a penchant for patriotic militarism, where einstein was a pronounced pacifist.120 during an address to caltechs students, einstein noted that science was often inclined to do more harm than good.121 this aversion to war also led einstein to befriend author upton sinclair and film star charlie chaplin, both noted for their pacifism. carl laemmle, head of universal studios, gave einstein a tour of his studio and introduced him to chaplin. they had an instant rapport, with chaplin inviting einstein and his wife, elsa, to his home for dinner. chaplin said einsteins outward persona, calm and gentle, seemed to conceal a highly emotional temperament, from which came his extraordinary intellectual energy.122 chaplins film city lights was to premiere a few days later in hollywood, and chaplin invited einstein and elsa to join him as his special guests. walter isaacson, einsteins biographer, described this as one of the most memorable scenes in the new era of celebrity.121 chaplin visited einstein at his home on a later trip to berlin and recalled his modest little flat and the piano at which he had begun writing his theory. chaplin speculated that it was possibly used as kindling wood by the nazis.123 einstein and chaplin were cheered at the premiere of the film. chaplin said to einstein, they cheer me because they understand me, and they cheer you because no one understands you.121 emigration to the us (1933) cartoon of einstein, who has shed his pacifism wings, standing next to a pillar labeled world peace. he is rolling up his sleeves and holding a sword labeled preparedness. cartoon of einstein after shedding his pacifism wings (charles r. macauley, c. 1933) in february 1933, while on a visit to the united states, einstein knew he could not return to germany with the rise to power of the nazis under germanys new chancellor, adolf hitler.124125 while at american universities in early 1933, he undertook his"
  },
  {
    "chunk_id": 15,
    "doc_id": "Albert_Einstein.txt",
    "text": "pacifism wings (charles r. macauley, c. 1933) in february 1933, while on a visit to the united states, einstein knew he could not return to germany with the rise to power of the nazis under germanys new chancellor, adolf hitler.124125 while at american universities in early 1933, he undertook his third two-month visiting professorship at the california institute of technology in pasadena. in february and march 1933, the gestapo repeatedly raided his familys apartment in berlin.126 he and his wife elsa returned to europe in march, and during the trip, they learned that the german reichstag had passed the enabling act on 23 march, transforming hitlers government into a de facto legal dictatorship, and that they would not be able to proceed to berlin. later on, they heard that their cottage had been raided by the nazis and einsteins personal sailboat confiscated. upon landing in antwerp, belgium on 28 march, einstein immediately went to the german consulate and surrendered his passport, formally renouncing his german citizenship.127 the nazis later sold his boat and converted his cottage into a hitler youth camp.128 refugee status landing card for einsteins 26 may 1933 arrival in dover, england from ostend, belgium,129 enroute to oxford130 in april 1933, einstein discovered that the new german government had passed laws barring jews from holding any official positions, including teaching at universities.131 historian gerald holton describes how, with virtually no audible protest being raised by their colleagues, thousands of jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.132 a month later, einsteins works were among those targeted by the german student union in the nazi book burnings, with nazi propaganda minister joseph goebbels proclaiming, jewish intellectualism is dead. one german magazine included him in a list of enemies of the german regime with the phrase, not yet hanged, offering a 5,000 bounty on his head.131133 in a subsequent letter to physicist and friend max born, who had already emigrated from germany to england, einstein wrote, ... i must confess that the degree of their brutality and cowardice came as something of a surprise.131 after moving to the us, he described the book burnings as a spontaneous emotional outburst by those who shun popular enlightenment, and more than anything else in the world, fear the influence of men of intellectual independence.134 einstein was"
  },
  {
    "chunk_id": 16,
    "doc_id": "Albert_Einstein.txt",
    "text": "of their brutality and cowardice came as something of a surprise.131 after moving to the us, he described the book burnings as a spontaneous emotional outburst by those who shun popular enlightenment, and more than anything else in the world, fear the influence of men of intellectual independence.134 einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in germany. aided by the academic assistance council, founded in april 1933 by british liberal politician william beveridge to help academics escape nazi persecution, einstein was able to leave germany.135 he rented a house in de haan, belgium, where he lived for a few months. in late july 1933, he visited england for about six weeks at the invitation of the british member of parliament commander oliver locker-lampson, who had become friends with him in the preceding years.129 locker-lampson invited him to stay near his cromer home in a secluded wooden cabin on roughton heath in the parish of roughton, norfolk. to protect einstein, locker-lampson had two bodyguards watch over him; a photo of them carrying shotguns and guarding einstein was published in the daily herald on 24 july 1933.136137 winston churchill and einstein at chartwell house, 31 may 1933 locker-lampson took einstein to meet winston churchill at his home, and later, austen chamberlain and former prime minister lloyd george.138 einstein asked them to help bring jewish scientists out of germany. british historian martin gilbert notes that churchill responded immediately, and sent his friend physicist frederick lindemann to germany to seek out jewish scientists and place them in british universities.139 churchill later observed that as a result of germany having driven the jews out, they had lowered their technical standards and put the allies technology ahead of theirs.139 einstein later contacted leaders of other nations, including turkeys prime minister, ismet inn, to whom he wrote in september 1933, requesting placement of unemployed german-jewish scientists. as a result of einsteins letter, jewish invitees to turkey eventually totaled over 1,000 saved individuals.140 locker-lampson also submitted a bill to parliament to extend british citizenship to einstein, during which period einstein made a number of public appearances describing the crisis brewing in europe.141 in one of his speeches he denounced germanys treatment of jews, while at the same time he introduced a bill promoting jewish citizenship in palestine, as they were"
  },
  {
    "chunk_id": 17,
    "doc_id": "Albert_Einstein.txt",
    "text": "to extend british citizenship to einstein, during which period einstein made a number of public appearances describing the crisis brewing in europe.141 in one of his speeches he denounced germanys treatment of jews, while at the same time he introduced a bill promoting jewish citizenship in palestine, as they were being denied citizenship elsewhere.142 in his speech he described einstein as a citizen of the world who should be offered a temporary shelter in the uk.note 3143 both bills failed, however, and einstein then accepted an earlier offer from the institute for advanced study, in princeton, new jersey, us, to become a resident scholar.141 resident scholar at the institute for advanced study portrait of einstein taken in 1935 at princeton on 3 october 1933, einstein delivered a speech on the importance of academic freedom before a packed audience at the royal albert hall in london, with the times reporting he was wildly cheered throughout.135 four days later he returned to the us and took up a position at the institute for advanced study,141144 noted for having become a refuge for scientists fleeing nazi germany.145 at the time, most american universities, including harvard, princeton and yale, had minimal or no jewish faculty or students, as a result of their jewish quotas, which lasted until the late 1940s.145 einstein was still undecided about his future. he had offers from several european universities, including christ church, oxford, where he stayed for three short periods between may 1931 and june 1933130 and was offered a five-year research fellowship (called a studentship at christ church),146147 but in 1935, he arrived at the decision to remain permanently in the united states and apply for citizenship.141148 einsteins affiliation with the institute for advanced study would last until his death in 1955.149 he was one of the four first selected (along with john von neumann, kurt gdel and hermann weyl150) at the new institute. he soon developed a close friendship with gdel; the two would take long walks together discussing their work. bruria kaufman, his assistant, later became a physicist. during this period, einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully. he lived in princeton at his home from 1935 onwards. the albert einstein house was made a national historic landmark in 1976. world war ii and the manhattan project see also: einsteinszilrd letter facsimile of the"
  },
  {
    "chunk_id": 18,
    "doc_id": "Albert_Einstein.txt",
    "text": "unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully. he lived in princeton at his home from 1935 onwards. the albert einstein house was made a national historic landmark in 1976. world war ii and the manhattan project see also: einsteinszilrd letter facsimile of the einsteinszilard letter in 1939, a group of hungarian scientists that included migr physicist le szilrd attempted to alert washington, d.c. to ongoing nazi atomic bomb research. the groups warnings were discounted. einstein and szilrd, along with other refugees such as edward teller and eugene wigner, regarded it as their responsibility to alert americans to the possibility that german scientists might win the race to build an atomic bomb, and to warn that hitler would be more than willing to resort to such a weapon.151152 to make certain the us was aware of the danger, in july 1939, a few months before the beginning of world war ii in europe, szilrd and wigner visited einstein to explain the possibility of atomic bombs, which einstein, a pacifist, said he had never considered.153 he was asked to lend his support by writing a letter, with szilrd, to president franklin d. roosevelt, recommending the us pay attention and engage in its own nuclear weapons research. the letter is believed to be arguably the key stimulus for the u.s. adoption of serious investigations into nuclear weapons on the eve of the u.s. entry into world war ii.154 in addition to the letter, einstein used his connections with the belgian royal family155 and the belgian queen mother to get access with a personal envoy to the white houses oval office. some say that as a result of einsteins letter and his meetings with roosevelt, the us entered the race to develop the bomb, drawing on its immense material, financial, and scientific resources to initiate the manhattan project. for einstein, war was a disease ... and he called for resistance to war. by signing the letter to roosevelt, some argue he went against his pacifist principles.156 in 1954, a year before his death, einstein said to his old friend, linus pauling, i made one great mistake in my lifewhen i signed the letter to president roosevelt recommending that atom bombs be made; but there was some justificationthe danger that the germans would make them ...157 in 1955, einstein and ten other intellectuals and scientists, including british philosopher"
  },
  {
    "chunk_id": 19,
    "doc_id": "Albert_Einstein.txt",
    "text": "old friend, linus pauling, i made one great mistake in my lifewhen i signed the letter to president roosevelt recommending that atom bombs be made; but there was some justificationthe danger that the germans would make them ...157 in 1955, einstein and ten other intellectuals and scientists, including british philosopher bertrand russell, signed a manifesto highlighting the danger of nuclear weapons.158 in 1960 einstein was included posthumously as a charter member of the world academy of art and science (waas),159 an organization founded by distinguished scientists and intellectuals who committed themselves to the responsible and ethical advances of science, particularly in light of the development of nuclear weapons. us citizenship einstein accepting a us citizenship certificate from judge phillip forman in 1940 einstein became an american citizen in 1940. not long after settling into his career at the institute for advanced study in princeton, new jersey, he expressed his appreciation of the meritocracy in american culture compared to europe. he recognized the right of individuals to say and think what they pleased without social barriers. as a result, individuals were encouraged, he said, to be more creative, a trait he valued from his early education.160 einstein joined the national association for the advancement of colored people (naacp) in princeton, where he campaigned for the civil rights of african americans. he considered racism americas worst disease,133161 seeing it as handed down from one generation to the next.162 as part of his involvement, he corresponded with civil rights activist w. e. b. du bois and was prepared to testify on his behalf during his trial as an alleged foreign agent in 1951.163 when einstein offered to be a character witness for du bois, the judge decided to drop the case.164 in 1946, einstein visited lincoln university in pennsylvania, a historically black college, where he was awarded an honorary degree. lincoln was the first university in the united states to grant college degrees to african americans; alumni include langston hughes and thurgood marshall. einstein gave a speech about racism in america, adding, i do not intend to be quiet about it.165 a resident of princeton recalls that einstein had once paid the college tuition for a black student.164 einstein has said, being a jew myself, perhaps i can understand and empathize with how black people feel as victims of discrimination.161 isaacson writes that when marian anderson, the black contralto, came to princeton for"
  },
  {
    "chunk_id": 20,
    "doc_id": "Albert_Einstein.txt",
    "text": "of princeton recalls that einstein had once paid the college tuition for a black student.164 einstein has said, being a jew myself, perhaps i can understand and empathize with how black people feel as victims of discrimination.161 isaacson writes that when marian anderson, the black contralto, came to princeton for a concert in 1937, the nassau inn refused her a room. so einstein invited her to stay at his house on main street, in what was a deeply personal as well as symbolic gesture ... whenever she returned to princeton, she stayed with einstein, her last visit coming just two months before he died.166 personal views political views main article: political views of albert einstein casual group shot of four men and two women standing on a brick pavement. albert einstein and elsa einstein arriving in new york in 1921. accompanying them are zionist leaders chaim weizmann (future president of israel), weizmanns wife vera weizmann, menahem ussishkin, and ben-zion mossinson. in 1918, einstein was one of the signatories of the founding proclamation of the german democratic party, a liberal party.167168 later in his life, einsteins political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as why socialism?.169170 his opinions on the bolsheviks also changed with time. in 1925, he criticized them for not having a well-regulated system of government and called their rule a regime of terror and a tragedy in human history. he later adopted a more moderated view, criticizing their methods but praising them, which is shown by his 1929 remark on vladimir lenin: in lenin i honor a man, who in total sacrifice of his own person has committed his entire energy to realizing social justice. i do not find his methods advisable. one thing is certain, however: men like him are the guardians and renewers of mankinds conscience.171 einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics.141 he strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation.172 he wrote i advocate world government because i am convinced that there is no other possible way of eliminating the most terrible danger in which man has ever found himself.173 the fbi created a secret dossier on einstein in 1932; by the time of his"
  },
  {
    "chunk_id": 21,
    "doc_id": "Albert_Einstein.txt",
    "text": "framework of a world federation.172 he wrote i advocate world government because i am convinced that there is no other possible way of eliminating the most terrible danger in which man has ever found himself.173 the fbi created a secret dossier on einstein in 1932; by the time of his death, it was 1,427 pages long.174 einstein was deeply impressed by mahatma gandhi, with whom he corresponded. he described gandhi as a role model for the generations to come.175 the initial connection was established on 27 september 1931, when wilfrid israel took his indian guest v. a. sundaram to meet his friend einstein at his summer home in the town of caputh. sundaram was gandhis disciple and special envoy, whom wilfrid israel met while visiting india and visiting the indian leaders home in 1925. during the visit, einstein wrote a short letter to gandhi that was delivered to him through his envoy, and gandhi responded quickly with his own letter. although in the end einstein and gandhi were unable to meet as they had hoped, the direct connection between them was established through wilfrid israel.176 relationship with zionism main article: political views of albert einstein zionism einstein was a figurehead leader in the establishment of the hebrew university of jerusalem,177 which opened in 1925.178 earlier, in 1921, he was asked by the biochemist and president of the world zionist organization, chaim weizmann, to help raise funds for the planned university.179 he made suggestions for the creation of an institute of agriculture, a chemical institute and an institute of microbiology in order to fight the various ongoing epidemics such as malaria, which he called an evil that was undermining a third of the countrys development.180 he also promoted the establishment of an oriental studies institute, to include language courses given in both hebrew and arabic.181 einstein was not a nationalist and opposed the creation of an independent jewish state.182 he felt that the waves of arriving jews of the aliyah could live alongside existing arabs in palestine. the state of israel was established without his help in 1948; einstein was limited to a marginal role in the zionist movement.183 upon the death of israeli president weizmann in november 1952, prime minister david ben-gurion offered einstein the largely ceremonial position of president of israel at the urging of ezriel carlebach.184185 the offer was presented by israels ambassador in washington, abba eban, who"
  },
  {
    "chunk_id": 22,
    "doc_id": "Albert_Einstein.txt",
    "text": "to a marginal role in the zionist movement.183 upon the death of israeli president weizmann in november 1952, prime minister david ben-gurion offered einstein the largely ceremonial position of president of israel at the urging of ezriel carlebach.184185 the offer was presented by israels ambassador in washington, abba eban, who explained that the offer embodies the deepest respect which the jewish people can repose in any of its sons. einstein wrote that he was deeply moved, but at once saddened and ashamed that he could not accept it.186 einstein did not want the office, and israel did not want him to accept, but felt obliged to make the offer. yitzhak navon, ben-gurions political secretary, and later president, reports ben-gurion as saying tell me what to do if he says yes! ive had to offer the post to him because its impossible not to. but if he accepts, we are in for trouble.187 religious and philosophical views main article: religious and philosophical views of albert einstein opening of einsteins speech (11 april 1943) for the united jewish appeal (recording by radio universidad nacional de la plata, argentina) ladies (coughs) and gentlemen, our age is proud of the progress it has made in mans intellectual development. the search and striving for truth and knowledge is one of the highest of mans qualities ... per lee smolin, i believe what allowed einstein to achieve so much was primarily a moral quality. he simply cared far more than most of his colleagues that the laws of physics have to explain everything in nature coherently and consistently.188 einstein expounded his spiritual outlook in a wide array of writings and interviews.189 he said he had sympathy for the impersonal pantheistic god of baruch spinozas philosophy.190 he did not believe in a personal god who concerns himself with fates and actions of human beings, a view which he described as nave.191 he clarified, however, that i am not an atheist,192 preferring to call himself an agnostic,193194 or a deeply religious nonbeliever.191 he wrote that a spirit is manifest in the laws of the universea spirit vastly superior to that of man, and one in the face of which we with our modest powers must feel humble. in this way the pursuit of science leads to a religious feeling of a special sort.195 einstein was primarily affiliated with non-religious humanist and ethical culture groups in both the uk"
  },
  {
    "chunk_id": 23,
    "doc_id": "Albert_Einstein.txt",
    "text": "to that of man, and one in the face of which we with our modest powers must feel humble. in this way the pursuit of science leads to a religious feeling of a special sort.195 einstein was primarily affiliated with non-religious humanist and ethical culture groups in both the uk and us. he served on the advisory board of the first humanist society of new york,196 and was an honorary associate of the rationalist association, which publishes new humanist in britain. for the 75th anniversary of the new york society for ethical culture, he stated that the idea of ethical culture embodied his personal conception of what is most valuable and enduring in religious idealism. he observed, without ethical culture there is no salvation for humanity.197 in a german-language letter to philosopher eric gutkind, dated 3 january 1954, einstein wrote: the word god is for me nothing more than the expression and product of human weaknesses, the bible a collection of honorable, but still primitive legends which are nevertheless pretty childish. no interpretation no matter how subtle can (for me) change this. ... for me the jewish religion like all other religions is an incarnation of the most childish superstitions. and the jewish people to whom i gladly belong and with whose mentality i have a deep affinity have no different quality for me than all other people. ... i cannot see anything chosen about them.198 einstein had been sympathetic toward vegetarianism for a long time. in a letter in 1930 to hermann huth, vice-president of the german vegetarian federation (deutsche vegetarier-bund), he wrote: although i have been prevented by outward circumstances from observing a strictly vegetarian diet, i have long been an adherent to the cause in principle. besides agreeing with the aims of vegetarianism for aesthetic and moral reasons, it is my view that a vegetarian manner of living by its purely physical effect on the human temperament would most beneficially influence the lot of mankind.199 he became a vegetarian himself only during the last part of his life. in march 1954 he wrote in a letter: so i am living without fats, without meat, without fish, but am feeling quite well this way. it almost seems to me that man was not born to be a carnivore.200 albert einstein ... also read blavatsky and attended lectures by rudolf steiner.201 love of music einstein playing the violin, 1927"
  },
  {
    "chunk_id": 24,
    "doc_id": "Albert_Einstein.txt",
    "text": "i am living without fats, without meat, without fish, but am feeling quite well this way. it almost seems to me that man was not born to be a carnivore.200 albert einstein ... also read blavatsky and attended lectures by rudolf steiner.201 love of music einstein playing the violin, 1927 einstein developed an appreciation for music at an early age. in his late journals he wrote: if i were not a physicist, i would probably be a musician. i often think in music. i live my daydreams in music. i see my life in terms of music ... i get most joy in life out of music.202203 his mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into german culture. according to conductor leon botstein, einstein began playing when he was 5. however, he did not enjoy it at that age.204 when he turned 13, he discovered mozarts violin sonatas, whereupon he became enamored of mozarts compositions and studied music more willingly. einstein taught himself to play without ever practicing systematically. he said that love is a better teacher than a sense of duty.204 at the age of 17, he was heard by a school examiner in aarau while playing beethovens violin sonatas. the examiner stated afterward that his playing was remarkable and revealing of great insight. what struck the examiner, writes botstein, was that einstein displayed a deep love of the music, a quality that was and remains in short supply. music possessed an unusual meaning for this student.204 music took on a pivotal and permanent role in einsteins life from that period on. although the idea of becoming a professional musician himself was not on his mind at any time, among those with whom einstein played chamber music were a few professionals, including kurt appelbaum, and he performed for private audiences and friends. chamber music had also become a regular part of his social life while living in bern, zurich, and berlin, where he played with max planck and his son, among others. he is sometimes erroneously credited as the editor of the 1937 edition of the kchel catalog of mozarts work; that edition was prepared by alfred einstein, who may have been a distant relation.205206 mozart was a special favorite; he said that mozarts music is"
  },
  {
    "chunk_id": 25,
    "doc_id": "Albert_Einstein.txt",
    "text": "planck and his son, among others. he is sometimes erroneously credited as the editor of the 1937 edition of the kchel catalog of mozarts work; that edition was prepared by alfred einstein, who may have been a distant relation.205206 mozart was a special favorite; he said that mozarts music is so pure it seems to have been ever-present in the universe. however, he preferred bach to beethoven, once saying: give me bach, rather, and then more bach.207 in 1931, while engaged in research at the california institute of technology, he visited the zoellner family conservatory in los angeles, where he played some of beethoven and mozarts works with members of the zoellner quartet.208209 near the end of his life, when the young juilliard quartet visited him in princeton, he played his violin with them, and the quartet was impressed by einsteins level of coordination and intonation.204 death on 17 april 1955, einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by rudolph nissen in 1948.210 he took the draft of a speech he was preparing for a television appearance commemorating the state of israels seventh anniversary with him to the hospital, but he did not live to complete it.211 einstein refused surgery, saying, i want to go when i want. it is tasteless to prolong life artificially. i have done my share; it is time to go. i will do it elegantly.212 he died in the princeton hospital early the next morning at the age of 76, having continued to work until near the end.213 during the autopsy, the pathologist thomas stoltz harvey removed einsteins brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made einstein so intelligent.214 einsteins remains were cremated in trenton, new jersey,215 and his ashes were scattered at an undisclosed location.216217 in a memorial lecture delivered on 13 december 1965 at unesco headquarters, nuclear physicist j. robert oppenheimer summarized his impression of einstein as a person: he was almost wholly without sophistication and wholly without worldliness ... there was always with him a wonderful purity at once childlike and profoundly stubborn.218 einstein bequeathed his personal archives, library, and intellectual assets to the hebrew university of jerusalem in israel.219 scientific career throughout his life, einstein published hundreds of books and articles.16220 he"
  },
  {
    "chunk_id": 26,
    "doc_id": "Albert_Einstein.txt",
    "text": "sophistication and wholly without worldliness ... there was always with him a wonderful purity at once childlike and profoundly stubborn.218 einstein bequeathed his personal archives, library, and intellectual assets to the hebrew university of jerusalem in israel.219 scientific career throughout his life, einstein published hundreds of books and articles.16220 he published more than 300 scientific papers and 150 non-scientific ones.11220 on 5 december 2014, universities and archives announced the release of einsteins papers, comprising more than 30,000 unique documents.221222 in addition to the work he did by himself, he also collaborated with other scientists on additional projects, including the boseeinstein statistics, the einstein refrigerator and others.223224 statistical mechanics thermodynamic fluctuations and statistical physics main articles: statistical mechanics, thermal fluctuations, and statistical physics einsteins first paper,77225 submitted in 1900 to annalen der physik, was on capillary attraction. it was published in 1901 with the title folgerungen aus den capillarittserscheinungen, which translates as conclusions from the capillarity phenomena. two papers he published in 19021903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. these papers were the foundation for the 1905 paper on brownian motion, which showed that brownian movement can be construed as firm evidence that molecules exist. his research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.225 theory of critical opalescence main article: critical opalescence einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. at the critical point, this derivative is zero, leading to large fluctuations. the effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. einstein relates this to rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue.226 einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and rayleigh scattering originate from the atomistic constitution of matter. 1905 annus mirabilis papers the annus mirabilis papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), brownian motion, the special theory of relativity, and e mc2 that einstein published in the annalen der physik scientific journal in 1905. these four"
  },
  {
    "chunk_id": 27,
    "doc_id": "Albert_Einstein.txt",
    "text": "atomistic constitution of matter. 1905 annus mirabilis papers the annus mirabilis papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), brownian motion, the special theory of relativity, and e mc2 that einstein published in the annalen der physik scientific journal in 1905. these four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. the four papers are: title (translated) area of focus received published significance on a heuristic viewpoint concerning the production and transformation of light227 photoelectric effect 18 march 9 june resolved an unsolved puzzle by suggesting that energy is exchanged only in discrete amounts (quanta).228 this idea was pivotal to the early development of quantum theory.229 on the motion of small particles suspended in a stationary liquid, as required by the molecular kinetic theory of heat230 brownian motion 11 may 18 july explained empirical evidence for the atomic theory, supporting the application of statistical physics. on the electrodynamics of moving bodies231 special relativity 30 june 26 september reconciled maxwells equations for electricity and magnetism with the laws of mechanics by introducing changes to mechanics, resulting from analysis based on the independence of the speed of light from the motion of the observer.232 discredited the concept of a luminiferous ether.233 does the inertia of a body depend upon its energy content?234 matterenergy equivalence 27 september 21 november equivalence of matter and energy, e mc2, the existence of rest energy, and the basis of nuclear energy. special relativity main article: history of special relativity einsteins zur elektrodynamik bewegter krper231 (on the electrodynamics of moving bodies) was received on 30 june 1905 and published 26 september of that same year. it reconciled conflicts between maxwells equations (the laws of electricity and magnetism) and the laws of newtonian mechanics by introducing changes to the laws of mechanics.235 observationally, the effects of these changes are most apparent at high speeds (where objects are moving at speeds close to the speed of light). the theory developed in this paper later became known as einsteins special theory of relativity. this paper predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion. this paper also argued that the idea of a luminiferous aetherone of the leading theoretical"
  },
  {
    "chunk_id": 28,
    "doc_id": "Albert_Einstein.txt",
    "text": "predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion. this paper also argued that the idea of a luminiferous aetherone of the leading theoretical entities in physics at the timewas superfluous.note 4 in his paper on massenergy equivalence, einstein produced e mc2 as a consequence of his special relativity equations.236 einsteins 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with max planck.note 5237 einstein originally framed special relativity in terms of kinematics (the study of moving bodies). in 1908, hermann minkowski reinterpreted special relativity in geometric terms as a theory of spacetime. einstein adopted minkowskis formalism in his 1915 general theory of relativity.238 general relativity general relativity and the equivalence principle main article: history of general relativity see also: theory of relativity and einstein field equations black circle covering the sun, rays visible around it, in a dark sky. eddingtons photo of a solar eclipse general relativity (gr) is a theory of gravitation that was developed by einstein between 1907 and 1915. according to it, the observed gravitational attraction between masses results from the warping of spacetime by those masses. general relativity has developed into an essential tool in modern astrophysics; it provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.239 as einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory.240 consequently, in 1907 he published an article on acceleration under special relativity. in that article titled on the relativity principle and the conclusions drawn from it, he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. this argument is called the equivalence principle. in the same article, einstein also predicted the phenomena of gravitational time dilation, gravitational redshift and gravitational lensing.241242 in 1911, einstein published another article on the influence of gravitation on the propagation of light expanding on the 1907 article, in which he estimated the amount of deflection of light by massive"
  },
  {
    "chunk_id": 29,
    "doc_id": "Albert_Einstein.txt",
    "text": "the same article, einstein also predicted the phenomena of gravitational time dilation, gravitational redshift and gravitational lensing.241242 in 1911, einstein published another article on the influence of gravitation on the propagation of light expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies. thus, the theoretical prediction of general relativity could for the first time be tested experimentally.243 gravitational waves in 1916, einstein predicted gravitational waves,244245 ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation. the existence of gravitational waves is possible under general relativity due to its lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it. by contrast, gravitational waves cannot exist in the newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed. the first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, psr b191316.246 the explanation for the decay in their orbital period was that they were emitting gravitational waves.246247 einsteins prediction was confirmed on 11 february 2016, when researchers at ligo published the first observation of gravitational waves,248 detected on earth on 14 september 2015, nearly one hundred years after the prediction.246249250251252 hole argument and entwurf theory while developing general relativity, einstein became confused about the gauge invariance in the theory. he formulated an argument that led him to conclude that a general relativistic field theory is impossible. he gave up looking for fully generally covariant tensor equations and searched for equations that would be invariant under general linear transformations only.253 in june 1913, the entwurf (draft) theory was the result of these investigations. as its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions. after more than two years of intensive work, einstein realized that the hole argument was mistaken254 and abandoned the theory in november 1915. physical cosmology main article: physical cosmology robert a. millikan, georges lematre and einstein at the california institute of technology in january 1933 in 1917, einstein applied the general theory of relativity to the structure of the universe as a whole.25512 he discovered that the general field equations predicted a universe that was"
  },
  {
    "chunk_id": 30,
    "doc_id": "Albert_Einstein.txt",
    "text": "main article: physical cosmology robert a. millikan, georges lematre and einstein at the california institute of technology in january 1933 in 1917, einstein applied the general theory of relativity to the structure of the universe as a whole.25512 he discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding. as observational evidence for a dynamic universe was lacking at the time, einstein introduced a new term, the cosmological constant, into the field equations, in order to allow the theory to predict a static universe. the modified field equations predicted a static universe of closed curvature, in accordance with einsteins understanding of machs principle in these years. this model became known as the einstein world or einsteins static universe.256257 this paper is widely regarded as marking the emergence of modern theoretical cosmology.258 following the discovery of the recession of the galaxies by edwin hubble in 1929, einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, the friedmanneinstein universe of 1931259260 and the einsteinde sitter universe of 1932.261262 in each of these models, einstein discarded the cosmological constant, claiming that it was in any case theoretically unsatisfactory.259260263 in many einstein biographies, it is claimed that einstein referred to the cosmological constant in later years as his biggest blunder, based on a letter george gamow claimed to have received from him. the astrophysicist mario livio has cast doubt on this claim.264 in late 2013, a team led by the irish physicist cormac oraifeartaigh discovered evidence that, shortly after learning of hubbles observations of the recession of the galaxies, einstein considered a steady-state model of the universe.265266 in a hitherto overlooked manuscript, apparently written in early 1931, einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process that he associated with the cosmological constant.267268 as he stated in the paper, in what follows, i would like to draw attention to a solution to equation (1) that can account for hubbels sic facts, and in which the density is constant over time ... if one considers a physically bounded volume, particles of matter will be continually leaving it. for the density to remain constant, new particles of matter must be continually formed in the volume from space. it thus appears that einstein considered a steady-state model of"
  },
  {
    "chunk_id": 31,
    "doc_id": "Albert_Einstein.txt",
    "text": "density is constant over time ... if one considers a physically bounded volume, particles of matter will be continually leaving it. for the density to remain constant, new particles of matter must be continually formed in the volume from space. it thus appears that einstein considered a steady-state model of the expanding universe many years before hoyle, bondi and gold.269270 however, einsteins steady-state model contained a fundamental flaw and he quickly abandoned the idea.267268271 energy momentum pseudotensor main article: stressenergymomentum pseudotensor general relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. noethers theorem allows these quantities to be determined from a lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. the energy and momentum derived within general relativity by noethers prescriptions do not make a real tensor for this reason.272 einstein argued that this is true for a fundamental reason: the gravitational field could be made to vanish by a choice of coordinates. he maintained that the non-covariant energy momentum pseudotensor was, in fact, the best description of the energy momentum distribution in a gravitational field. while the use of non-covariant objects like pseudotensors was criticized by erwin schrdinger and others, einsteins approach has been echoed by physicists including lev landau and evgeny lifshitz.273 wormholes in 1935, einstein collaborated with nathan rosen to produce a model of a wormhole, often called einsteinrosen bridges.274275 his motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper do gravitational fields play an important role in the constitution of the elementary particles?. these solutions cut and pasted schwarzschild black holes to make a bridge between two patches. because these solutions included spacetime curvature without the presence of a physical body, einstein and rosen suggested that they could provide the beginnings of a theory that avoided the notion of point particles. however, it was later found that einsteinrosen bridges are not stable.276 einsteincartan theory main article: einsteincartan theory einstein, sitting at a table, looks up from the papers he is reading and into the camera. einstein at his office, university of berlin, 1920 in order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. this modification was made by einstein"
  },
  {
    "chunk_id": 32,
    "doc_id": "Albert_Einstein.txt",
    "text": "up from the papers he is reading and into the camera. einstein at his office, university of berlin, 1920 in order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. this modification was made by einstein and cartan in the 1920s. equations of motion main article: einsteininfeldhoffmann equations in general relativity, gravitational force is reimagined as curvature of spacetime. a curved path like an orbit is not the result of a force deflecting a body from an ideal straight-line path, but rather the bodys attempt to fall freely through a background that is itself curved by the presence of other masses. a remark by john archibald wheeler that has become proverbial among physicists summarizes the theory: spacetime tells matter how to move; matter tells spacetime how to curve.277278 the einstein field equations cover the latter aspect of the theory, relating the curvature of spacetime to the distribution of matter and energy. the geodesic equation covers the former aspect, stating that freely falling bodies follow lines that are as straight as possible in a curved spacetime. einstein regarded this as an independent fundamental assumption that had to be postulated in addition to the field equations in order to complete the theory. believing this to be a shortcoming in how general relativity was originally presented, he wished to derive it from the field equations themselves. since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the einstein field equations themselves, not by a new law. accordingly, einstein proposed that the field equations would determine the path of a singular solution, like a black hole, to be a geodesic. both physicists and philosophers have often repeated the assertion that the geodesic equation can be obtained from applying the field equations to the motion of a gravitational singularity, but this claim remains disputed.279280 old quantum theory main article: old quantum theory photons and energy quanta the photoelectric effect. incoming photons on the left strike a metal plate (bottom), and eject electrons, depicted as flying off to the right. in a 1905 paper,227 einstein postulated that light itself consists of localized particles (quanta). einsteins light quanta were nearly universally rejected by all physicists, including max planck and niels bohr. this"
  },
  {
    "chunk_id": 33,
    "doc_id": "Albert_Einstein.txt",
    "text": "photons on the left strike a metal plate (bottom), and eject electrons, depicted as flying off to the right. in a 1905 paper,227 einstein postulated that light itself consists of localized particles (quanta). einsteins light quanta were nearly universally rejected by all physicists, including max planck and niels bohr. this idea only became universally accepted in 1919, with robert millikans detailed experiments on the photoelectric effect, and with the measurement of compton scattering. einstein concluded that each wave of frequency f is associated with a collection of photons with energy hf each, where h is the planck constant. he did not say much more, because he was not sure how the particles were related to the wave. but he did suggest that this idea would explain certain experimental results, notably the photoelectric effect.227 light quanta were dubbed photons by gilbert n. lewis in 1926.281 quantized atomic vibrations main article: einstein solid in 1907, einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. in the einstein model, each atom oscillates independentlya series of equally spaced quantized states for each oscillator. einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. peter debye refined this model.282 boseeinstein statistics main article: boseeinstein statistics in 1924, einstein received a description of a statistical model from indian physicist satyendra nath bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. einstein noted that boses statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of boses paper to the zeitschrift fr physik. einstein also published his own articles describing the model and its implications, among them the boseeinstein condensate phenomenon that some particulates should appear at very low temperatures.283 it was not until 1995 that the first such condensate was produced experimentally by eric allin cornell and carl wieman using ultra-cooling equipment built at the nistjila laboratory at the university of colorado at boulder.284 boseeinstein statistics are now used to describe the behaviors of any assembly of bosons. einsteins sketches for this project may be seen in the einstein archive in the library of the leiden university.223 waveparticle duality einstein in"
  },
  {
    "chunk_id": 34,
    "doc_id": "Albert_Einstein.txt",
    "text": "equipment built at the nistjila laboratory at the university of colorado at boulder.284 boseeinstein statistics are now used to describe the behaviors of any assembly of bosons. einsteins sketches for this project may be seen in the einstein archive in the library of the leiden university.223 waveparticle duality einstein in 1921, by harris ewing studio although the patent office promoted einstein to technical examiner second class in 1906, he had not given up on academia. in 1908, he became a privatdozent at the university of bern.285 in ber die entwicklung unserer anschauungen ber das wesen und die konstitution der strahlung (the development of our views on the composition and essence of radiation), on the quantization of light, and in an earlier 1909 paper, einstein showed that max plancks energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. this paper introduced the photon concept and inspired the notion of waveparticle duality in quantum mechanics. einstein saw this waveparticle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation. zero-point energy in a series of works completed from 1911 to 1913, planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his second quantum theory. soon, this idea attracted the attention of einstein and his assistant otto stern. assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data. the numbers matched nicely. however, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.286 stimulated emission in 1917, at the height of his work on relativity, einstein published an article in physikalische zeitschrift that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.287 this article showed that the statistics of absorption and emission of light would only be consistent with plancks distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. this paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.288 matter waves einstein discovered louis de broglies work and supported his ideas, which were"
  },
  {
    "chunk_id": 35,
    "doc_id": "Albert_Einstein.txt",
    "text": "emission of light into an empty mode. this paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.288 matter waves einstein discovered louis de broglies work and supported his ideas, which were received skeptically at first. in another major paper from this era, einstein observed that de broglie waves could explain the quantization rules of bohr and sommerfeld. this paper would inspire schrdingers work of 1926.289290 quantum mechanics einsteins objections to quantum mechanics newspaper headline on 4 may 1935 einstein played a major role in developing quantum theory, beginning with his 1905 paper on the photoelectric effect. however, he became displeased with modern quantum mechanics as it had evolved after 1925, despite its acceptance by other physicists. he was skeptical that the randomness of quantum mechanics was fundamental rather than the result of determinism, stating that god is not playing at dice.291 until the end of his life, he continued to maintain that quantum mechanics was incomplete.292 bohr versus einstein main article: bohreinstein debates two men sitting, looking relaxed. a dark-haired bohr is talking while einstein looks skeptical. einstein and niels bohr, 1925 the bohreinstein debates were a series of public disputes about quantum mechanics between einstein and niels bohr, who were two of its founders. their debates are remembered because of their importance to the philosophy of science.293294295 their debates would influence later interpretations of quantum mechanics. einsteinpodolskyrosen paradox main article: epr paradox einstein never fully accepted quantum mechanics. while he recognized that it made correct predictions, he believed a more fundamental description of nature must be possible. over the years he presented multiple arguments to this effect, but the one he preferred most dated to a debate with bohr in 1930. einstein suggested a thought experiment in which two objects are allowed to interact and then moved apart a great distance from each other. the quantum-mechanical description of the two objects is a mathematical entity known as a wavefunction. if the wavefunction that describes the two objects before their interaction is given, then the schrdinger equation provides the wavefunction that describes them after their interaction. but because of what would later be called quantum entanglement, measuring one object would lead to an instantaneous change of the wavefunction describing the other object, no matter how far away it is. moreover, the"
  },
  {
    "chunk_id": 36,
    "doc_id": "Albert_Einstein.txt",
    "text": "is given, then the schrdinger equation provides the wavefunction that describes them after their interaction. but because of what would later be called quantum entanglement, measuring one object would lead to an instantaneous change of the wavefunction describing the other object, no matter how far away it is. moreover, the choice of which measurement to perform upon the first object would affect what wavefunction could result for the second object. einstein reasoned that no influence could propagate from the first object to the second instantaneously fast. indeed, he argued, physics depends on being able to tell one thing apart from another, and such instantaneous influences would call that into question. because the true physical condition of the second object could not be immediately altered by an action done to the first, einstein concluded, the wavefunction could not be that true physical condition, only an incomplete description of it.296297 a more famous version of this argument came in 1935, when einstein published a paper with boris podolsky and nathan rosen that laid out what would become known as the epr paradox.298 in this thought experiment, two particles interact in such a way that the wavefunction describing them is entangled. then, no matter how far the two particles were separated, a precise position measurement on one particle would imply the ability to predict, perfectly, the result of measuring the position of the other particle. likewise, a precise momentum measurement of one particle would result in an equally precise prediction for of the momentum of the other particle, without needing to disturb the other particle in any way. they argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is forbidden by the theory of relativity. they invoked a principle, later known as the epr criterion of reality, positing that: if, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity. from this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. but quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. einstein, podolsky, and rosen therefore concluded that"
  },
  {
    "chunk_id": 37,
    "doc_id": "Albert_Einstein.txt",
    "text": "they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. but quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. einstein, podolsky, and rosen therefore concluded that quantum theory does not provide a complete description of reality.299 in 1964, john stewart bell carried the analysis of quantum entanglement much further. he deduced that if measurements are performed independently on the two separated particles of an entangled pair, then the assumption that the outcomes depend upon hidden variables within each half implies a mathematical constraint on how the outcomes on the two measurements are correlated. this constraint would later be called a bell inequality. bell then showed that quantum physics predicts correlations that violate this inequality. consequently, the only way that hidden variables could explain the predictions of quantum physics is if they are nonlocal, which is to say that somehow the two particles are able to interact instantaneously no matter how widely they ever become separated.300301 bell argued that because an explanation of quantum phenomena in terms of hidden variables would require nonlocality, the epr paradox is resolved in the way which einstein would have liked least.302 despite this, and although einstein personally found the argument in the epr paper overly complicated,296297 that paper became among the most influential papers published in physical review. it is considered a centerpiece of the development of quantum information theory.303 unified field theory main article: classical unified field theories encouraged by his success with general relativity, einstein sought an even more ambitious geometrical theory that would treat gravitation and electromagnetism as aspects of a single entity. in 1950, he described his unified field theory in a scientific american article titled on the generalized theory of gravitation.304 his attempt to find the most fundamental laws of nature won him praise but not success: a particularly conspicuous blemish of his model was that it did not accommodate the strong and weak nuclear forces, neither of which was well understood until many years after his death. although most researchers now believe that einsteins approach to unifying physics was mistaken, his goal of a theory of everything is one to which his successors still aspire.305 other investigations main article: einsteins unsuccessful investigations einstein conducted other investigations that were unsuccessful and abandoned. these pertain to force,"
  },
  {
    "chunk_id": 38,
    "doc_id": "Albert_Einstein.txt",
    "text": "his death. although most researchers now believe that einsteins approach to unifying physics was mistaken, his goal of a theory of everything is one to which his successors still aspire.305 other investigations main article: einsteins unsuccessful investigations einstein conducted other investigations that were unsuccessful and abandoned. these pertain to force, superconductivity, and other research. collaboration with other scientists the 1927 solvay conference in brussels, a gathering of the worlds top physicists. einstein is in the center. in addition to longtime collaborators leopold infeld, nathan rosen, peter bergmann and others, einstein also had some one-shot collaborations with various scientists. einsteinde haas experiment main article: einsteinde haas effect in 1908, owen willans richardson predicted that a change in the magnetic moment of a free body will cause this body to rotate. this effect is a consequence of the conservation of angular momentum and is strong enough to be observable in ferromagnetic materials.306 einstein and wander johannes de haas published two papers in 1915 claiming the first experimental observation of the effect.307308 measurements of this kind demonstrate that the phenomenon of magnetization is caused by the alignment (polarization) of the angular momenta of the electrons in the material along the axis of magnetization. these measurements also allow the separation of the two contributions to the magnetization: that which is associated with the spin and with the orbital motion of the electrons. the einstein-de haas experiment is the only experiment conceived, realized and published by albert einstein himself. a complete original version of the einstein-de haas experimental equipment was donated by geertruida de haas-lorentz, wife of de haas and daughter of lorentz, to the ampre museum in lyon france in 1961 where it is currently on display. it was lost among the museums holdings and was rediscovered in 2023.309310 einstein as an inventor in 1926, einstein and his former student le szilrd co-invented (and in 1930, patented) the einstein refrigerator. this absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input.311 on 11 november 1930, u.s. patent 1,781,541 was awarded to einstein and le szilrd for the refrigerator. their invention was not immediately put into commercial production, but the most promising of their patents were acquired by the swedish company electrolux.note 6 einstein also invented an electromagnetic pump,313 sound reproduction device,314 and several other household devices.315 legacy non-scientific left-right: heinrich goldschmidt, einstein, ole colbjrnsen, jrgen vogt, and"
  },
  {
    "chunk_id": 39,
    "doc_id": "Albert_Einstein.txt",
    "text": "refrigerator. their invention was not immediately put into commercial production, but the most promising of their patents were acquired by the swedish company electrolux.note 6 einstein also invented an electromagnetic pump,313 sound reproduction device,314 and several other household devices.315 legacy non-scientific left-right: heinrich goldschmidt, einstein, ole colbjrnsen, jrgen vogt, and ilse einstein at a picnic in oslo in 1920. while traveling, einstein wrote daily to his wife elsa and adopted stepdaughters margot and ilse. the letters were included in the papers bequeathed to the hebrew university of jerusalem. margot einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986316). barbara wolff, of the hebrew universitys albert einstein archives, told the bbc that there are about 3,500 pages of private correspondence written between 1912 and 1955.317 in his final four years, einstein was involved with the establishment of the albert einstein college of medicine in new york city.318 in 1979, the albert einstein memorial was unveiled outside the national academy of sciences building in washington, d.c. for the einstein centenary. it was sculpted by robert berks. einstein can be seen holding a paper with three of his most important equations: for the photoelectric effect, general relativity and mass-energy equivalence.319 einsteins right of publicity was litigated in 2015 in a federal district court in california. although the court initially held that the right had expired,320 that ruling was immediately appealed, and the decision was later vacated in its entirety. the underlying claims between the parties in that lawsuit were ultimately settled. the right is enforceable, and the hebrew university of jerusalem is the exclusive representative of that right.321 corbis, successor to the roger richman agency, licenses the use of his name and associated imagery, as agent for the university.322 mount einstein in the chugach mountains of alaska was named in 1955. mount einstein in new zealands paparoa range was named after him in 1970 by the department of scientific and industrial research.323 in 1999, einstein was named times person of the century.15 scientific recognition in 1999, a survey of the top 100 physicists voted for einstein as the greatest physicist ever, while a parallel survey of rank-and-file physicists gave the top spot to isaac newton, with einstein second.324325 the physicist lev landau ranked physicists from 0 to 5 on a logarithmic scale of"
  },
  {
    "chunk_id": 40,
    "doc_id": "Albert_Einstein.txt",
    "text": "recognition in 1999, a survey of the top 100 physicists voted for einstein as the greatest physicist ever, while a parallel survey of rank-and-file physicists gave the top spot to isaac newton, with einstein second.324325 the physicist lev landau ranked physicists from 0 to 5 on a logarithmic scale of productivity and genius, with newton receiving the highest ranking of 0, followed by einstein with 0.5, while fathers of quantum mechanics such as paul dirac, niels bohr, and werner heisenberg were ranked 1, with landau himself a 2.326327 science writer john g. simmons ranked einstein second after newton in the scientific 100, based on a qualitative assessment in which he ordered the scientists according to overall influence, and noted that the work of einstein forms the source of twentieth-century physics.328 physicist eugene wigner noted that while john von neumann had the quickest and most acute mind he ever knew, it was einstein who had the more penetrating and original mind of the two, stating that:329 but einsteins understanding was deeper than even jancsi von neumanns. his mind was both more penetrating and more original than von neumanns. and that is a very remarkable statement. einstein took an extraordinary pleasure in invention. two of his greatest inventions are the special and general theories of relativity; and for all of jancsis brilliance, he never produced anything so original. no modern physicist has. the international union of pure and applied physics declared 2005 the world year of physics, also known as einstein year, in recognition of einsteins miracle year in 1905.330 it was also declared the international year of physics by the united nations.331 in popular culture main article: albert einstein in popular culture the famous image of einstein taken by arthur sasse in 1951, sitting in a car on his 72nd birthday, having been asked to smile for the camera once again. einstein became one of the most famous scientific celebrities after the confirmation of his general theory of relativity in 1919.332333334 although most of the public had little understanding of his work, he was widely recognized and admired. in the period before world war ii, the new yorker published a vignette in their the talk of the town feature saying that einstein was so well known in america that he would be stopped on the street by people wanting him to explain that theory. eventually he came to cope with unwanted"
  },
  {
    "chunk_id": 41,
    "doc_id": "Albert_Einstein.txt",
    "text": "before world war ii, the new yorker published a vignette in their the talk of the town feature saying that einstein was so well known in america that he would be stopped on the street by people wanting him to explain that theory. eventually he came to cope with unwanted enquirers by pretending to be someone else: pardon me, sorry! always i am mistaken for professor einstein.335 einstein has been the subject of or inspiration for many novels, films, plays, and works of music.336 he is a favorite model for depictions of absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. time magazines frederic golden wrote that einstein was a cartoonists dream come true.337 his intellectual achievements and originality made einstein broadly synonymous with genius.338 many popular quotations are often misattributed to him.339340 awards and honors main article: list of awards and honors received by albert einstein einstein received numerous awards and honors, and in 1922, he was awarded the 1921 nobel prize in physics for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect. none of the nominations in 1921 met the criteria set by alfred nobel, so the 1921 prize was carried forward and awarded to einstein in 1922.7 einsteinium, a synthetic chemical element, was named in his honor in 1955, a few months after his death.341"
  },
  {
    "chunk_id": 42,
    "doc_id": "Transformer.txt",
    "text": "transformer (deep learning) in deep learning, the transformer is an artificial neural network architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.1 at each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished. transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (rnns) such as long shortterm memory (lstm).2 later variations have been widely adopted for training large language models (llms) on large (language) datasets. 3 the modern version of the transformer was proposed in the 2017 paper attention is all you need by researchers at google. 1 the predecessors of transformers were developed as an improvement over previous architectures for machine translation, 45 but have found many applications since. they are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning, 67 audio, 8 multimodal learning, robotics, 9 and even playing chess. 10 it has also led to the development of pre-trained systems, such as generative pre-trained transformers (gpts)11 and bert12 (bidirectional encoder representations from transformers). for many years, sequence modelling and generation was done by using plain recurrent neural networks (rnns). a well-cited early example was the elman network (1990). in theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the models state at the end of a long sentence without precise, extractable information about preceding tokens. a key breakthrough was lstm (1995),note 1 an rnn which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. one key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units. 13 neural networks using multiplicative units were later called sigma-pi networks14 or higher-order networks. 15 lstm became the standard history predecessors 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 130 architecture for long sequence modelling until the 2017 publication of transformers. however, lstm still used sequential processing, like most other rnns.note 2 specifically, rnns operate one token at a time from first to last; they cannot operate"
  },
  {
    "chunk_id": 43,
    "doc_id": "Transformer.txt",
    "text": "the standard history predecessors 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 130 architecture for long sequence modelling until the 2017 publication of transformers. however, lstm still used sequential processing, like most other rnns.note 2 specifically, rnns operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. modern transformers overcome this problem, but unlike rnns, they require computation time that is quadratic in the size of the context window. the linearly scaling fast weight controller (1992) learns to compute a weight matrix for further processing depending on the input.16 one of its two networks has fast weights or dynamic links (1981).171819 a slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.16 this was later shown to be equivalent to the unnormalized linear transformer.2021 the idea of encoderdecoder sequence transduction had been developed in the early 2010s; commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.2223 a 380m-parameter model for machine translation uses two long short-term memories (lstm).23 its architecture consists of two parts. the encoder is an lstm that takes in a sequence of tokens and turns it into a vector. the decoder is another lstm that converts the vector into a sequence of tokens. similarly, another 130m-parameter model used gated recurrent units (gru) instead of lstm.22 later research showed that grus are neither better nor worse than lstms for seq2seq.2425 these early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. this is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. if the input is long, then the output vector would not be able to contain all relevant information, degrading the output. as evidence, reversing the input sentence improved seq2seq translation.26 the rnn search model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process longdistance dependencies more easily. the name is because it emulates searching through"
  },
  {
    "chunk_id": 44,
    "doc_id": "Transformer.txt",
    "text": "output. as evidence, reversing the input sentence improved seq2seq translation.26 the rnn search model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process longdistance dependencies more easily. the name is because it emulates searching through a source sentence during decoding a translation.4 the relative performances were compared between global (that of rnn search) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.27 in 2016, google translate was revamped to google neural machine translation, which replaced the previous model based on statistical machine translation. the new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional lstm.28 it took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.29 attention with seq2seq 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 230 seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on gpus. in 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved sota result in textual entailment with an order of magnitude fewer parameters than lstms.30 one of its authors, jakob uszkoreit, suspected that attention without recurrence would be sufficient for language translation, thus the title attention is all you need.31 that hypothesis was against conventional wisdom at the time, and even his father hans uszkoreit, a well-known computational linguist, was skeptical.31 in the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for lstms.32 in 2017, the original (100m-sized) encoderdecoder transformer model was proposed in the attention is all you need paper. at the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.1 this led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks.33 as early as spring 2017, even before the attention is all you need preprint was published,"
  },
  {
    "chunk_id": 45,
    "doc_id": "Transformer.txt",
    "text": "multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. its parallelizability was an important factor to its widespread use in large neural networks.33 as early as spring 2017, even before the attention is all you need preprint was published, one of the co-authors applied the decoder-only variation of the architecture to generate fictitious wikipedia articles.34 transformer architecture is now used alongside many generative models that contribute to the ongoing ai boom. in language modelling, elmo (2018) was a bi-directional lstm that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. it was followed by bert (2018), an encoder-only transformer model.35 in 2019 october, google started using bert to process search queries.36 in 2020, google translate replaced the previous rnnencoderrnn-decoder model by a transformer-encoderrnn-decoder model.37 starting in 2018, the openai gpt series of decoder-only transformers became state of the art in natural language generation. in 2022, a chatbot based on gpt-3, chatgpt, became unexpectedly38 popular, triggering a boom around large language models. 3940 since 2020, transformers have been applied in modalities beyond text, including the vision transformer, 41 speech recognition,42 robotics,6 and multimodal. 43 the vision transformer, in turn, stimulated new developments in convolutional neural networks. 44 image and video generators like dall-e (2021), stable diffusion 3 (2024),45 and sora (2024), use transformers to analyse input data (like text prompts) by breaking it down into tokens and then calculating the relevance between each token using self-attention, which helps the model understand the context and relationships within the data. parallelizing attention ai boom era 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 330 the plain transformer architecture had difficulty in converging. in the original paper,1 the authors recommended using learning rate warmup. that is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2 of the total number of training steps), before decaying again. a 2020 paper found that using layer normalization before (instead of after) multihead attention and feedforward layers stabilizes training, not requiring learning rate warmup.46 transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine-tuning commonly include:"
  },
  {
    "chunk_id": 46,
    "doc_id": "Transformer.txt",
    "text": "layers stabilizes training, not requiring learning rate warmup.46 transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. the pretrain dataset is typically an unlabeled large corpus, such as the pile. tasks for pretraining and fine-tuning commonly include: language modeling12 next-sentence prediction12 question answering3 reading comprehension sentiment analysis1 paraphrasing1 the t5 transformer report47 documents a large number of natural language pretraining tasks. some examples are: restoring or repairing incomplete or corrupted text. for example, the input, thank you me to your party week, might generate the output, thank you for inviting me to your party last week. translation between natural languages (machine translation) judging the pragmatic acceptability of natural language. for example, the following sentence might be judged not acceptable,48 because even though it is syntactically well-formed, it is improbable in ordinary human usage: the course is jumping well. note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture. in general, there are 3 classes of language modelling tasks: masked,49 autoregressive,50 and prefixlm.51 these classes are independent of a specific modeling architecture such as transformer, but they are often discussed in the context of transformer. training methods for stabilizing training pretrain-finetune tasks 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 430 in a masked task,49 one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. the loss function for the task is typically sum of log-perplexities for the masked-out tokens: and the model is trained to minimize this loss function. the bert series of models are trained for masked token prediction and another task. in an autoregressive task,50 the entire sequence is masked at first, and the model produces a probability distribution for the first token. then the first token is revealed and the model predicts the second token, and so on. the loss function for the task is still typically the same. the gpt series of models are trained by autoregressive tasks. in a prefixlm task,51 the sequence is divided into two parts. the first part is presented as context, and the model predicts the first token of the second part. then that would be revealed, and the"
  },
  {
    "chunk_id": 47,
    "doc_id": "Transformer.txt",
    "text": "still typically the same. the gpt series of models are trained by autoregressive tasks. in a prefixlm task,51 the sequence is divided into two parts. the first part is presented as context, and the model predicts the first token of the second part. then that would be revealed, and the model predicts the second token, and so on. the loss function for the task is still typically the same. the t5 series of models are trained by prefixlm tasks. note that masked as in masked language modelling is not masked as in masked attention, and prefixlm as in prefix language modeling is not prefixlm as in prefix language model. all transformers have the same primary components: tokenizers, which convert text into tokens. embedding layer, which converts tokens and positions of the tokens into vector representations. transformer layers, which carry out repeated transformations on the vector representations, extracting more and more linguistic information. these consist of alternating attention and feedforward layers. there are two major types of transformer layers: encoder layers and decoder layers, with further variants. un-embedding layer, which converts the final vector representations back to a probability distribution over the tokens. the following description follows exactly the transformer as described in the original paper. there are variants, described in the following section. by convention, we write all vectors as row vectors. for example, pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as . as the transformer architecture natively consists of operations over numbers (matrix multiplications, dot products, activation functions) rather than over text, there must first be a mapping from any input text to some numerical representation. this happens in three steps. first, the input text is treated by a preprocessor, which performs both textual transformations and splits the text into coarse-grained segments called pretokens. the latter is referred to as pretokenization. second, each pretoken is segmented further into tokens by a tokenizer that architecture tokenization 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 530 expects to only see pretokens output by its preprocessor. each token it produces is a string of one or more characters belonging to a finite set of strings called the vocabulary . third, because the vocabulary is finite and known beforehand, each token can be assigned an integer identifier, and this mapping is applied to the sequence of tokens to represent any input text as"
  },
  {
    "chunk_id": 48,
    "doc_id": "Transformer.txt",
    "text": "string of one or more characters belonging to a finite set of strings called the vocabulary . third, because the vocabulary is finite and known beforehand, each token can be assigned an integer identifier, and this mapping is applied to the sequence of tokens to represent any input text as a numerical sequence. since this mapping is bijective, the output side can produce a sequence of integer identifiers which can then be turned back into tokens. after undoing some of the preprocessing, the result is again legible text. training a tokenizer (sometimes referred to as vocabularization) means finding a suitable vocabulary , but also learning how to use it, since any given string of length has hypothetical segmentations, some of which containing segments that are not in the vocabulary. the most important hyperparameter during vocabularization is the vocabulary size : when it is small, the learned vocabulary generally consists of characters and smaller strings, and words will be segmented into many tokens. at larger sizes, it becomes affordable to dedicate tokens to full words, although depending on the preprocessor and tokenizer, it is not necessarily the case that large vocabularies will always use the largest token(s) available to segment a word. because tokens are not always full words, they may also be referred to as subwords and tokenization algorithms may be referred to as subword tokenizers. this is also to differentiate these systems from traditional terminology used in older information retrieval and natural language processing systems, where tokenization was used to denote what is today called pretokenization (very crudely: splitting into words). in tokenizers that produce tokens that are not part of the vocabulary, a special token that does belong to the vocabulary is used as a generic stand-in, written as unk for unknown. in principle, any string could be hidden by such an unk. indeed, in information retrieval, pretokenizers were themselves used as tokenizers (and also called tokenizers) with a word-level vocabulary that contained an unk. commonly used subword tokenization algorithms are byte pair encoding (bpe) and the unigram language model (ulm), which each include a vocabularization algorithm and a dedicated segmentation algorithm. there also exist several segmentation algorithms that require no learning and can be applied given a vocabulary (produced by bpe or ulm, for example), like greedily recognising tokens in a pretoken by moving through it left-to-right. well-known software implementations of subword tokenizers are hugging faces tokenizers"
  },
  {
    "chunk_id": 49,
    "doc_id": "Transformer.txt",
    "text": "a dedicated segmentation algorithm. there also exist several segmentation algorithms that require no learning and can be applied given a vocabulary (produced by bpe or ulm, for example), like greedily recognising tokens in a pretoken by moving through it left-to-right. well-known software implementations of subword tokenizers are hugging faces tokenizers python package implemented in rust, and the sentencepiece python package implemented in c. the latter package is named as such because one of its configuration options allows disabling the built-in pretokenizer, hence effectively making entire sentences a pretoken and thus having the tokenizer see entire sentences, rather than individual words. each integer token identifier is converted into an embedding vector via a lookup table. equivalently stated, it multiplies a one-hot representation of the token identifier by an embedding matrix . for example, if the input tokens identifier is , then the one-hot representation is , and its embedding vector is the token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. embedding 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 630 illustration of (absolute) positional encoding with parameters the dimension of an embedding vector is called hidden size or embedding size and written as . 35 this size is written as in the original transformer paper.1 an un-embedding layer is almost the reverse of an embedding layer. whereas an embedding layer converts a token identifier into a vector, an un-embedding layer converts a vector into a probability distribution over tokens. the un-embedding layer is a linear-softmax layer: the matrix has shape . some architectures use the transpose of the embedding matrix as the un-embedding matrix in order to avoid needing double the amount of embeddingrelated parameters and to avoid divergence during training. this practice is called weight tying. 52 a positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. this induces a bias towards the order of the input sequence, so that, for example, the input sequence man bites dog is processed differently from dog bites man. the positional encoding is defined as a function of type , where is a positive even integer. the full positional encoding defined in the original paper1 is: where . here, is a free parameter that should be significantly larger than the"
  },
  {
    "chunk_id": 50,
    "doc_id": "Transformer.txt",
    "text": "bites dog is processed differently from dog bites man. the positional encoding is defined as a function of type , where is a positive even integer. the full positional encoding defined in the original paper1 is: where . here, is a free parameter that should be significantly larger than the biggest that would be input into the positional encoding function. the original paper uses . the function is in a simpler form when written as a complex function of type where . the main reason for using this positional encoding function is that using it, shifts are linear transformations: where is the distance one wishes to shift. this allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication. un-embedding positional encoding 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 730 one encoderdecoder block a transformer is composed of stacked encoder layers and decoder layers. by taking a linear sum, any convolution can also be implemented as linear transformations: for any constants . this allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. this sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. in the authors words, we hypothesized it would allow the model to easily learn to attend by relative position. in typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference. like earlier seq2seq models, the original transformer model used an encoderdecoder architecture. the encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoders output and the decoders output tokens so far. the purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that mixes information from other input tokens via self-attention mechanism. each decoder layer contains two attention sublayers: (1) crossattention for incorporating the output of encoder (contextualized input token representations), and (2) selfattention for mixing information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).5354 both the encoder"
  },
  {
    "chunk_id": 51,
    "doc_id": "Transformer.txt",
    "text": "other input tokens via self-attention mechanism. each decoder layer contains two attention sublayers: (1) crossattention for incorporating the output of encoder (contextualized input token representations), and (2) selfattention for mixing information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).5354 both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.54 these feed-forward layers contain most of the parameters in a transformer model. the feedforward network (ffn) modules in a transformer are 2-layered multilayer perceptrons: where and are weight matrices and and are bias vectors, and is its activation function. the original transformer used relu activation. encoderdecoder (overview) feedforward network 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 830 the feedforward network module. it is a two-layered network that maps - dimensional vectors into - dimensional vectors. the number of neurons in the middle layer is called intermediate size (gpt),55 filter size (bert),35 or feedforward size (bert).35 it is typically larger than the embedding size. for example, in both gpt-2 series and bert series, the intermediate size of a model is 4 times its embedding size: . the attention mechanism used in the transformer architecture are scaled dot-product attention units. for each unit, the transformer model learns three weight matrices: the query weights , the key weights , and the value weights . the module takes three sequences, a query sequence, a key sequence, and a value sequence. the query sequence is a sequence of length , and each entry is a vector of dimension . similarly for the key and value sequences. for each vector in the query sequence, it is multiplied by a matrix to produce a query vector . the matrix of all query vectors is the query matrix: similarly, we construct the key matrix and the value matrix . it is usually the case that all are square matrices, meaning , etc. attention weights are calculated using the query and key vectors: the attention weight from token to token is the dot product between and . the attention weights are divided by the square root of the dimension of the key vectors, , which stabilizes gradients during training, and passed through a softmax which normalizes the weights. the fact that and are different matrices allows attention to be non-symmetric: if token attends to token (i.e."
  },
  {
    "chunk_id": 52,
    "doc_id": "Transformer.txt",
    "text": ". the attention weights are divided by the square root of the dimension of the key vectors, , which stabilizes gradients during training, and passed through a softmax which normalizes the weights. the fact that and are different matrices allows attention to be non-symmetric: if token attends to token (i.e. is large), this does not necessarily mean that token will attend to token (i.e. could be small). the output of the attention unit for token is the weighted sum of the value vectors of all tokens, weighted by , the attention from token to each token. the attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. the matrices , and are defined as the matrices where the th rows are vectors , , and respectively. then we can represent the attention as where the softmax is applied over each of the rows of the matrix. scaled dot-product attention attention head 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 930 scaled dot-product attention, block diagram exact dimension counts within an attention head module the number of dimensions in a query vector is query size and similarly for the key size and value size . the output dimension of an attention head is its head dimension . the attention mechanism requires the following three equalities to hold: but is otherwise unconstrained. if the attention head is used in a self-attention fashion, then . if the attention head is used in a cross-attention fashion, then usually . it is theoretically possible for all three to be different, but that is rarely the case in practice. one set of matrices is called an attention head, and each layer in a transformer model has multiple attention heads. while each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of relevance. specifically, the query and key projection matrices, and , which are involved in the attention score computation, defines the relevance. meanwhile, the value projection matrix , in combination with the part of the output projection matrix , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. in addition, the scope of attention, or the range of token"
  },
  {
    "chunk_id": 53,
    "doc_id": "Transformer.txt",
    "text": "computation, defines the relevance. meanwhile, the value projection matrix , in combination with the part of the output projection matrix , determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. in addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. this allows the model to capture more complex and longrange dependencies in deeper layers. many transformer attention heads encode relevance relations that are meaningful to humans. for example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.56 the computations for each attention head can be performed in parallel, which allows for fast processing. the outputs for the attention layer are concatenated to pass into the feedforward neural network layers. concretely, let the multiple attention heads be indexed by , then we have multihead attention 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1030 multihead attention, block diagram exact dimension counts within a multihead attention module where the matrix is the concatenation of word embeddings, and the matrices are projection matrices owned by individual attention head , and is a final projection matrix owned by the whole multihead attention head. it is theoretically possible for each attention head to have a different head dimension , but that is rarely the case in practice. as an example, in the smallest gpt-2 model, there are only self-attention mechanisms. it has the following dimensions: since , its output projection matrix is a square matrix. the transformer architecture is constructed to calculate output tokens iteratively. assuming refers to the calculation of the first output token , for step , the output token shall remain constant. this ensures properties of the model similar to autoregressive models. 1 therefore, at every time step , the calculation for all outputs should not have access to tokens at position for (as it naturally is the case for time step , when tokens are not yet calculated). this behavior may be accomplished before the softmax stage by adding a mask matrix that is at entries where the attention link must be cut, and at other places: the following matrix is commonly used in decoder self-attention modules, called causal masking: in words, it means that each token can pay attention to itself, and every token before"
  },
  {
    "chunk_id": 54,
    "doc_id": "Transformer.txt",
    "text": "stage by adding a mask matrix that is at entries where the attention link must be cut, and at other places: the following matrix is commonly used in decoder self-attention modules, called causal masking: in words, it means that each token can pay attention to itself, and every token before it, but not any after it. a non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. as an example of an uncommon use of mask matrix, the xlnet considers all masks of the form , where is a random permutation matrix. 57 an encoder consists of an embedding layer, followed by multiple encoder layers. masked attention encoder 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1130 one encoder layer one decoder layer each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. it takes an input as a sequence of input vectors, applies the selfattention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. schematically, we have: where stands for feed-forward network. we can more succinctly write it as with the implicit convention that the is applied to each row of the matrix individually. the encoder layers are stacked. the first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. this sequence of vectors is processed by the second encoder, and so on. the output from the final encoder layer is then used by the decoder. as the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking. a decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer. each decoder consists of three major components: a causally masked self-attention mechanism, a crossattention mechanism, and a feed-forward neural network. the decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. this mechanism can also be called the encoderdecoder attention. 154 like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. the transformer must not use the current or future output to predict an output, so the output"
  },
  {
    "chunk_id": 55,
    "doc_id": "Transformer.txt",
    "text": "encoders. this mechanism can also be called the encoderdecoder attention. 154 like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. the transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.1 this allows for autoregressive text generation. for decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. thus, the self-attention module in the decoder is causally masked. decoder 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1230 (a) one encoder layer and one decoder layer. (b) two encoder layers and two decoder layers. the sublayers are labelled as well. in contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. consequently, there is no need for masking in the cross-attention mechanism. schematically, we have: where is the matrix with rows being the output vectors from the encoder. the last decoder is followed by a final un-embedding layer to produce the output probabilities over the vocabulary. then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc., autoregressively generating output text. each encoder layer contains 2 sublayers: the self-attention and the feedforward network. each decoder layer contains 3 sublayers: the causally masked self-attention, the crossattention, and the feedforward network. the final points of detail are the residual connections and layer normalization, (denoted as layernorm, or ln in the following), which while conceptually unnecessary, are necessary for numerical stability and convergence. the residual connection, which is introduced to avoid vanishing gradient issues and stabilize the training process, can be expressed as follows: y f(x) x. the expression indicates that an output y is the sum of the transformation of input x (f(x)) and the input itself (x). adding the input x can preserve the input information and avoid issues when the gradient of f(x) is close to zero. similarly to how the feedforward network modules are applied individually to each vector, the layernorm is also applied individually to each vector. there are two common conventions in use: the post-ln and the pre-ln convention. in the postln convention, the output of each sublayer is where is the function implemented by the sublayer itself. in"
  },
  {
    "chunk_id": 56,
    "doc_id": "Transformer.txt",
    "text": "network modules are applied individually to each vector, the layernorm is also applied individually to each vector. there are two common conventions in use: the post-ln and the pre-ln convention. in the postln convention, the output of each sublayer is where is the function implemented by the sublayer itself. in the pre-ln convention, the output of each sublayer is the original 2017 transformer used the post-ln convention. it was difficult to train and required full transformer architecture sublayers 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1330 transformer encoder with norm-first and norm-last transformer decoder with norm-first and norm-last block diagram for the full transformer architecture careful hyperparameter tuning and a warm-up in learning rate, where it starts small and gradually increases. the pre-ln convention, proposed several times in 2018,58 was found to be easier to train, requiring no warm-up, leading to faster convergence.46 the following is the pseudocode for a standard pre-ln encoderdecoder transformer, adapted from formal algorithms for transformers59 input: encoder input te decoder input td output: array of probability distributions, with shape (decoder vocabulary size x length(decoder output sequence)) encoder ze encoder.tokenizer(te) for each t in 1:length(ze) do zet encoder.embedding(zet) encoder.positionalembedding(t) for each l in 1:length(encoder.layers) do layer encoder.layersl first sublayer zecopy copy(ze) for each t in 1:length(ze) do zet layer.layernorm(zet) ze layer.multiheadattention(ze, ze, ze) for each t in 1:length(ze) do zet zet zecopyt second sublayer zecopy copy(ze) for each t in 1:length(ze) do zet layer.layernorm(zet) ze layer.feedforward(ze) for each t in 1:length(ze) do zet zet zecopyt for each t in 1:length(ze) do zet encoder.finallayernorm(zet) decoder zd decoder.tokenizer(td) for each t in 1:length(zd) do zdt decoder.embedding(zdt) decoder.positionalembedding(t) for each l in 1:length(decoder.layers) do layer decoder.layersl first sublayer zdcopy copy(zd) for each t in 1:length(zd) do zdt layer.layernorm(zdt) zd layer.maskedmultiheadattention(zd, zd, zd) for each t in 1:length(zd) do zdt zdt zdcopyt second sublayer zdcopy copy(zd) for each t in 1:length(zd) do zdt layer.layernorm(zdt) zd layer.multiheadattention(zd, ze, ze) for each i in 1:length(zd) do zdt zdt zdcopyt pseudocode 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1430 schematic object hierarchy for the full transformer architecture, in objectoriented programming style third sublayer zdcopy copy(zd) for each t in 1:length(zd) do zdt layer.layernorm(zdt) zd layer.feedforward(zd) for each t in 1:length(zd) do zdt zdt zdcopyt zd decoder.finallayernorm(zd) outputdistributions for each t in 1:length(zd) do outputdistributions.append(decoder.unembed(zdt)) return outputdistributions the transformer architecture, being modular, allows variations. several common variations are described here.60 an encoder-only"
  },
  {
    "chunk_id": 57,
    "doc_id": "Transformer.txt",
    "text": "third sublayer zdcopy copy(zd) for each t in 1:length(zd) do zdt layer.layernorm(zdt) zd layer.feedforward(zd) for each t in 1:length(zd) do zdt zdt zdcopyt zd decoder.finallayernorm(zd) outputdistributions for each t in 1:length(zd) do outputdistributions.append(decoder.unembed(zdt)) return outputdistributions the transformer architecture, being modular, allows variations. several common variations are described here.60 an encoder-only transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. this is usually used for text embedding and representation learning for downstream applications. bert is encoder-only. they are less often used currently, as they were found to be not significantly better than training an encoderdecoder transformer, then taking just the encoder.51 they are also referred to as all-to-all or bert-like. a decoder-only transformer is not literally decoder-only, since without an encoder, the crossattention mechanism has nothing to attend to. thus, the decoder layers in a decoder-only transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. this is usually used for text generation and instruction following. the models in the gpt series and chinchilla series are decoder-only. they are also referred to as autoregressive or causal. an encoderdecoder transformer is generally the same as the original transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. they might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. this is also usually used for text generation and instruction following. the models in the t5 series are encoderdecoder.60 a prefixlm (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. specifically, it has mask of the form60:figure 3 where the first columns correspond to the prefix, and the subsequent columns correspond to the autoregressively generated text based on the prefix. they resemble encoderdecoder models, but has less sparsity. such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.51 there are also mixed seq2seq models. for example, in 2020, google translate replaced the previous rnn-encoderrnn-decoder model with a transformer-encoderrnn-decoder model, as transformer-based decoders did not appear to significantly increase quality unlike the encoder, while the rnn decoder was much faster.37 terminology 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1530 the original transformer uses relu activation function. other activation functions were developed. the llama series and palm used swiglu;61 both gpt-1 and bert35 used gelu.62 alternative activation"
  },
  {
    "chunk_id": 58,
    "doc_id": "Transformer.txt",
    "text": "significantly increase quality unlike the encoder, while the rnn decoder was much faster.37 terminology 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1530 the original transformer uses relu activation function. other activation functions were developed. the llama series and palm used swiglu;61 both gpt-1 and bert35 used gelu.62 alternative activation functions are often used in combination with gated linear units in the feedforward module.61 the normalization used in the transformer can be different from layernorm. one example is rmsnorm63 which is used in the llama series. other examples include capsulenorm64 scalenorm,65 or fixnorm.65 transformers may use other positional encoding methods than sinusoidal.66 the original transformer paper reported using a learned positional encoding,67 but finding it not superior to the sinusoidal one.1 later,68 found that causal masking itself provides enough signal to a transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module. rope (rotary positional embedding),69 is best explained by considering a list of 2-dimensional vectors . now pick some angle . then rope encoding is equivalently, if we write the 2-dimensional vectors as complex numbers , then rope encoding is just multiplication by an angle: for a list of -dimensional vectors, a rope encoder is defined by a sequence of angles . then the rope encoding is applied to each pair of coordinates. the benefit of rope is that the dot-product between two vectors depends on their relative location only: for any integer . subsequent work alternative activation functions alternative normalizations alternative positional encodings rope 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1630 alibi (attention with linear biases)70 is not a replacement for the positional encoder on the original transformer. instead, it is an additional positional encoder that is directly plugged into the attention mechanism. specifically, the alibi attention mechanism is here, is a real number (scalar), and is the linear bias matrix defined by in other words, . the idea being that the linear bias matrix is a softened mask. just as represent full attention paid, and represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction. alibi allows pretraining on short context windows, then fine-tuning on longer context windows. since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the bottom of the entire network (which"
  },
  {
    "chunk_id": 59,
    "doc_id": "Transformer.txt",
    "text": "direction and decreases attention paid in the other direction. alibi allows pretraining on short context windows, then fine-tuning on longer context windows. since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the bottom of the entire network (which is where the sinusoidal encoder on the original transformer, as well as rope and many others, are located). relative position encodings71 is similar to alibi, but more generic: where is a toeplitz matrix, that is, whenever . this is contrasted with the original sinusoidal positional encoding, which is an absolute positional encoding.72 the transformer model has been implemented in standard deep learning frameworks such as tensorflow and pytorch. transformers is a library produced by hugging face that supplies transformer-based architectures and pretrained models.11 when an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. the kv caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. pagedattention applies memory paging to kv caching.737475 alibi relative position encodings efficient implementation kv caching 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1730 comparison between several different forms of attention mechanism and the amount of kv caching necessary for each if a transformer is used with a baked-in prompt, such as you are a customer support agent..., then the key and value vectors can be computed for the prompt, and saved on disk. the saving in compute is significant when the model is used for many short real-time interactions, such as in online chatbots. flashattention76 is an algorithm that implements the transformer attention mechanism efficiently on a gpu. it is a communication-avoiding algorithm that performs matrix multiplications in blocks, such that each block fits within the cache of a gpu, and by careful management of the blocks it minimizes data copying between gpu caches (as data movement is slow). see the page on softmax for details. an improved version, flashattention-2,777879 was developed to cater to the rising demand for language models capable of handling longer context lengths. it offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 tflopss on a100 gpus (fp16bf16), a 2x speed increase over the original flashattention. key advancements in flashattention-2 include"
  },
  {
    "chunk_id": 60,
    "doc_id": "Transformer.txt",
    "text": "was developed to cater to the rising demand for language models capable of handling longer context lengths. it offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 tflopss on a100 gpus (fp16bf16), a 2x speed increase over the original flashattention. key advancements in flashattention-2 include the reduction of non-matmul flops, improved parallelism over the sequence length dimension, better work partitioning between gpu warps, and added support for head dimensions up to 256 and multi-query attention (mqa) and groupedquery attention (gqa).80 benchmarks revealed flashattention-2 to be up to 2x faster than flashattention and up to 9x faster than a standard attention implementation in pytorch. future developments include optimization for new hardware like h100 gpus and new data types like fp8. flashattention-4 focuses on pipelining to increase instruction throughput, and was developed to perform particularly well on blackwell gpus. 81 multi-query attention changes the multihead attention mechanism.82 whereas normally, with multi-query attention, there is just one , thus: this has a neutral effect on model quality and training speed, but increases inference speed. flashattention multi-query attention 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1830 the architecture of v2, showing both mla and a variant of mixture of experts 84:figure 2 multi-token prediction more generally, grouped-query attention (gqa) partitions attention heads into groups, each of which shares the key-value pair. mqa is gqa with one group, while standard multihead attention is gqa with the maximal number of groups.83 multihead latent attention (mla) is a low-rank approximation to standard mha. specifically, each hidden vector, before entering the attention mechanism, is first projected to two low-dimensional spaces (latent space), one for query and one for key-value (kv vector). this design minimizes the kv cache, as only the lowdimensional kv vector needs to be cached.84 speculative decoding8586 is a method to accelerate token decoding. similarly to speculative execution in cpus, future tokens are computed quickly, then verified. if the quickly computed tokens are incorrect, they are discarded and computed slowly. the key factor in speculative decoding is that a transformer decoder can verify faster than it can decode, in the following sense. suppose we have two transformer models like gpt-3 and gpt-3-small, both with a context window size of 512. to generate an entire context window autoregressively with greedy decoding with gpt-3, it must be run for 512 times, each time generating a token , taking time . however, if"
  },
  {
    "chunk_id": 61,
    "doc_id": "Transformer.txt",
    "text": "following sense. suppose we have two transformer models like gpt-3 and gpt-3-small, both with a context window size of 512. to generate an entire context window autoregressively with greedy decoding with gpt-3, it must be run for 512 times, each time generating a token , taking time . however, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each is indeed the token with the largest log-likelihood in the -th output. in speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. for example, suppose we use gpt-3-small to generate four speculative tokens: . this only takes . these tokens are then run through the larger gpt-3 in one go. suppose that and are verified by gpt-3 as what it would have picked, then those are kept, but is not, so are discarded, and gpt-3 is run on those. this would take , which might be shorter than . for non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.8587 in multi-token prediction, a single forward pass creates a final embedding vector, which then is un-embedded into a token probability. however, that vector can then be further processed by another transformer block to predict the next token, and so on for arbitrarily many steps into the future. this trades off accuracy for speed, since each new token costs just one more transformer block, rather than the entire stack.8889 speculative decoding 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 1930 training transformer-based architectures can be expensive, especially for long inputs.90 many methods have been developed to attempt to address the issue. in the image domain, swin transformer is an efficient architecture that performs attention inside shifting windows.91 in the audio domain, septr decouples the attention in time and frequency domains.92 long range arena (2020)93 is a standard benchmark for comparing the behavior of transformer architectures over long inputs. the standard attention graph is either all-to-all or causal, both of which scales as where is the number of tokens in a sequence. reformer (2020)9094 reduces the computational load from to by using locality-sensitive"
  },
  {
    "chunk_id": 62,
    "doc_id": "Transformer.txt",
    "text": "arena (2020)93 is a standard benchmark for comparing the behavior of transformer architectures over long inputs. the standard attention graph is either all-to-all or causal, both of which scales as where is the number of tokens in a sequence. reformer (2020)9094 reduces the computational load from to by using locality-sensitive hashing and reversible layers.95 sparse attention96 uses attention graphs that grows slower than . for example, bigbird (2020)97 uses random small-world networks which grows as . ordinary transformers require a memory size that is quadratic in the size of the context window. attention-free transformers98 reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value. random feature attention (2021)99 uses fourier random features: where are independent samples from the normal distribution . this choice of parameters satisfy , or consequently, the one-headed attention, with one query, can be written as where . similarly for multiple queries, and for multihead attention. this approximation can be computed in linear time, as we can compute the matrix first, then multiply it with the query. in essence, we have managed to obtain a more precise version of sub-quadratic transformers alternative attention graphs random feature attention 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 2030 performer (2022)100 uses the same random feature attention, but are first independently sampled from the normal distribution , then they are gram-schmidt processed. transformers can also be usedadapted for modalities (input or output) beyond just text, usually by finding a way to tokenize the modality. multimodal models can either be trained from scratch, or by finetuning. a 2022 study found that transformers pretrained only on natural language can be finetuned on only 0.03 of parameters and become competitive with lstms on a variety of logical and visual tasks, demonstrating transfer learning. 101 the llava was a vision-language model composed of a language model (vicuna-13b)102 and a vision model (vit-l14), connected by a linear layer. only the linear layer is finetuned.103 vision transformers41 adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like embedding vector of tokens in a standard transformer. conformer42 and later whisper104 follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into"
  },
  {
    "chunk_id": 63,
    "doc_id": "Transformer.txt",
    "text": "into vectors, and treating them like embedding vector of tokens in a standard transformer. conformer42 and later whisper104 follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like embedding vector of tokens in a standard transformer. perceivers105106 are a variant of transformers designed for multimodality. for image generation, notable architectures are dall-e 1 (2021), parti (2022),107 phenaki (2023),108 and muse (2023).109 unlike later models, dall-e is not a diffusion model. instead, it uses a decoder-only transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.110 parti is an encoderdecoder transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.111 muse is an encoder-only transformer that is trained to predict masked image tokens from unmasked image tokens. during generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.109 phenaki is a text-to-video model. it is a bidirectional masked transformer conditioned on pre-computed text tokens. the generated tokens are then decoded to a video.108 the transformer has had great success in natural language processing (nlp). many large language models such as gpt-2, gpt-3, gpt-4, gemini, albertagpt, claude, bert, grok, xlnet, roberta and chatgpt demonstrate the ability of transformers to perform a wide variety of nlprelated subtasks and their related real-world applications, including: machine translation time series prediction document summarization multimodality applications 28122025 22:52 transformer (deep learning) - wikipedia https:en.wikipedia.orgwikitransformer(deeplearning) 2130 document generation named entity recognition (ner)112 writing computer code based on requirements expressed in natural language. speech-to-text beyond traditional nlp, the transformer architecture has had success in other applications, such as: biological sequence analysis video understanding protein folding (such as alphafold) evaluating chess board positions. using static evaluation alone (that is, with no minimax search) transformer achieved an elo of 2895, putting it at grandmaster level.10 seq2seq family of machine learning approaches perceiver variant of transformer designed for multimodal data vision transformer machine learning model for vision processing large language model type of machine learning model bert (language model) series of language models developed by google ai generative pre-trained transformer type of large language model t5 (language model) series of large language"
  },
  {
    "chunk_id": 64,
    "doc_id": "Transformer.txt",
    "text": "perceiver variant of transformer designed for multimodal data vision transformer machine learning model for vision processing large language model type of machine learning model bert (language model) series of language models developed by google ai generative pre-trained transformer type of large language model t5 (language model) series of large language models developed by google ai"
  },
  {
    "chunk_id": 65,
    "doc_id": "Turing_test.txt",
    "text": "the standard interpretation of the turing test, in which player c, the interrogator, is given the task of trying to determine which player a or b is a computer and which is a human. the interrogator is limited to using the responses to written questions to make the determination. 1 turing test the turing test, originally called the imitation game by alan turing in 1949,2 is a test of a machines ability to exhibit intelligent behaviour equivalent to that of a human. in the test, a human evaluator judges a text transcript of a natural-language conversation between a human and a machine. the evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. the results would not depend on the machines ability to answer questions correctly, only on how closely its answers resembled those of a human. since the turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).3 the test was introduced by turing in his 1950 paper computing machinery and intelligence while working at the university of manchester. 4 it opens with the words: i propose to consider the question, can machines think? because thinking is difficult to define, turing chooses to replace the question by another, which is closely related to it and is expressed in relatively unambiguous words.5 turing describes the new form of the problem in terms of a three-person party game called the imitation game, in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. turings new question is: are there imaginable digital computers which would do well in the imitation game?2 this question, turing believed, was one that could actually be answered. in the remainder of the paper, he argued against the major objections to the proposition that machines can think.6 since turing introduced his test, it has been highly influential in the philosophy of artificial intelligence, resulting in substantial discussion and controversy, as well as criticism from philosophers like john searle, who argue against the tests ability to detect consciousness. 78 the question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind."
  },
  {
    "chunk_id": 66,
    "doc_id": "Turing_test.txt",
    "text": "controversy, as well as criticism from philosophers like john searle, who argue against the tests ability to detect consciousness. 78 the question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. ren descartes prefigures aspects of the turing test in his 1637 discourse on the method when he writes: history philosophical background 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 130 how many different automata or moving machines could be made by the industry of man ... for we can easily understand a machines being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. but it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.9 here descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. descartes therefore prefigures the turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the turing test as such, even if he prefigures its conceptual framework and criterion. denis diderot formulates in his 1746 book penses philosophiques a turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts: if they find a parrot who could answer to everything, i would claim it to be an intelligent being without hesitation. this does not mean he agrees with this, but that it was already a common argument of materialists at that time. according to dualism, the mind is non-physical (or, at the very least, has non-physical properties) 10 and, therefore, cannot be explained in purely physical terms. according to materialism, the mind can be explained physically, which leaves"
  },
  {
    "chunk_id": 67,
    "doc_id": "Turing_test.txt",
    "text": "this, but that it was already a common argument of materialists at that time. according to dualism, the mind is non-physical (or, at the very least, has non-physical properties) 10 and, therefore, cannot be explained in purely physical terms. according to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.11 in 1936, philosopher alfred ayer considered the standard philosophical question of other minds: how do we know that other people have the same conscious experiences that we do? in his book, language, truth and logic, ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: the only ground i can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined.12 (this suggestion is very similar to the turing test, but it is not certain that ayers popular philosophical classic was familiar to turing.) in other words, a thing is not conscious if it fails the consciousness test. a rudimentary idea of the turing test appears in the 1726 novel gullivers travels by jonathan swift. 1314 when gulliver is brought before the king of brobdingnag, the king thinks at first that gulliver might be a a piece of clock-work (which is in that country arrived to a very great perfection) contrived by some ingenious artist. even when he hears gulliver speaking, the king cultural background 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 230 still doubts whether gulliver was taught a set of words to make him sell at a better price. gulliver tells that only after he put several other questions to me, and still received rational answers the king became satisfied that gulliver was not a machine.15 tests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that turing would have been aware of these.16 stanley g. weinbaums a martian odyssey (1934) provides an example of how nuanced such tests could be.16 earlier examples of machines or automatons attempting to pass as human include the ancient greek myth of pygmalion who creates a sculpture of a woman that is animated by aphrodite, carlo collodis novel"
  },
  {
    "chunk_id": 68,
    "doc_id": "Turing_test.txt",
    "text": "stanley g. weinbaums a martian odyssey (1934) provides an example of how nuanced such tests could be.16 earlier examples of machines or automatons attempting to pass as human include the ancient greek myth of pygmalion who creates a sculpture of a woman that is animated by aphrodite, carlo collodis novel the adventures of pinocchio, about a puppet who wants to become a real boy, and e. t. a. hoffmanns 1816 story the sandman, where the protagonist falls in love with an automaton. in all these examples, people are fooled by artificial beings that - up to a point - pass as human.17 researchers in the united kingdom had been exploring machine intelligence for up to ten years prior to the founding of the field of artificial intelligence (ai) research in 1956.18 it was a common topic among the members of the ratio club, an informal group of british cybernetics and electronics researchers that included alan turing.19 turing, in particular, had been running the notion of machine intelligence since at least 194120 and one of the earliest-known mentions of computer intelligence was made by him in 1947.21 in turings report, intelligent machinery,22 he investigated the question of whether or not it is possible for machinery to show intelligent behaviour23 and, as part of that investigation, proposed what may be considered the forerunner to his later tests: it is not difficult to devise a paper machine which will play a not very bad game of chess.24 now get three men a, b and c as subjects for the experiment. a and c are to be rather poor chess players, b is the operator who works the paper machine. ... two rooms are used with some arrangement for communicating moves, and a game is played between c and either a or the paper machine. c may find it quite difficult to tell which he is playing.25 computing machinery and intelligence (1950) was the first published paper by turing to focus exclusively on machine intelligence. turing begins the 1950 paper with the claim, i propose to consider the question can machines think? 5 as he highlights, the traditional approach to such a question is to start with definitions, defining both the terms machine and think. turing chooses not to do so; instead, he replaces the question with a new one, which is closely related to it and is expressed in relatively unambiguous words.5 in"
  },
  {
    "chunk_id": 69,
    "doc_id": "Turing_test.txt",
    "text": "he highlights, the traditional approach to such a question is to start with definitions, defining both the terms machine and think. turing chooses not to do so; instead, he replaces the question with a new one, which is closely related to it and is expressed in relatively unambiguous words.5 in essence he proposes to change the question from can machines think? to can machines do what we (as thinking entities) can do?26 the advantage of the new question, turing argues, is that it draws a fairly sharp line between the physical and intellectual capacities of a man.27 to demonstrate this approach turing proposes a test inspired by a party game, known as the imitation game, in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. in this game, alan turing and the imitation game 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 330 both the man and the woman aim to convince the guests that they are the other. (huma shah argues that this two-human version of the game was presented by turing only to introduce the reader to the machine-human question-answer test.28 ) turing described his new version of the game as follows: we now ask the question, what will happen when a machine takes the part of a in this game? will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? these questions replace our original, can machines think?27 later in the paper, turing suggests an equivalent alternative formulation involving a judge conversing only with a computer and a man.29 while neither of these formulations precisely matches the version of the turing test that is more generally known today, he proposed a third in 1952. in this version, which turing discussed in a bbc radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.30 turings paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see computing machinery and intelligence).6 john searles 1980 paper minds, brains, and programs proposed the chinese room"
  },
  {
    "chunk_id": 70,
    "doc_id": "Turing_test.txt",
    "text": "it is really a man.30 turings paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see computing machinery and intelligence).6 john searles 1980 paper minds, brains, and programs proposed the chinese room thought experiment and argued that the turing test could not be used to determine if a machine could think. searle noted that software (such as eliza) could pass the turing test simply by manipulating symbols of which they had no understanding. without understanding, they could not be described as thinking in the same sense people did. therefore, searle concluded, the turing test could not prove that machines could think.31 much like the turing test itself, searles argument has been both widely criticised32 and endorsed.33 arguments such as searles and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the turing test that continued through the 1980s and 1990s.34 the loebner prize, now reported as defunct,35 provided an annual platform for practical turing tests with the first competition held in november 1991.36 it was underwritten by hugh loebner. the cambridge center for behavioral studies in massachusetts, united states, organised the prizes up to and including the 2003 contest. as loebner described it, one reason the competition was created is to advance the state of ai research, at least in part, because no one had taken steps to implement the turing test despite 40 years of discussing it.37 the first loebner prize competition in 1991 led to a renewed discussion of the viability of the turing test and the value of pursuing it, in both the popular press38 and academia.39 the first contest was won by a mindless program with no identifiable intelligence that managed to fool nave interrogators into making the wrong identification. this highlighted several of the shortcomings of the turing test (discussed below): the winner won, at least in part, because it was able to imitate the chinese room loebner prize 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 430 human typing errors;38 the unsophisticated interrogators were easily fooled;39 and some researchers in ai have been led to feel that the test is merely a distraction from more fruitful research.40 the silver (text only) and gold (audio and visual)"
  },
  {
    "chunk_id": 71,
    "doc_id": "Turing_test.txt",
    "text": "room loebner prize 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 430 human typing errors;38 the unsophisticated interrogators were easily fooled;39 and some researchers in ai have been led to feel that the test is merely a distraction from more fruitful research.40 the silver (text only) and gold (audio and visual) prizes have never been won. however, the competition has awarded the bronze medal every year for the computer system that, in the judges opinions, demonstrates the most human conversational behaviour among that years entries. artificial linguistic internet computer entity (a.l.i.c.e.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). learning ai jabberwacky won in 2005 and 2006. the loebner prize tested conversational intelligence; winners were typically chatterbot programs, or artificial conversational entities (ace)s. early loebner prize rules restricted conversations: each entry and hidden-human conversed on a single topic,41 thus the interrogators were restricted to one line of questioning per entity interaction. the restricted conversation rule was lifted for the 1995 loebner prize. interaction duration between judge and entity has varied in loebner prizes. in loebner 2003, at the university of surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. between 2004 and 2007, the interaction time allowed in loebner prizes was more than twenty minutes. the final competition was in 2019, due to a lack of funding for the prize following loebners death in 2016.42 captcha (completely automated public turing test to tell computers and humans apart) is one of the oldest concepts for artificial intelligence. the captcha system is commonly used online to tell humans and bots apart on the internet. it is based on the turing test. displaying distorted letters and numbers, it asks the user to identify the letters and numbers and type them into a field, which bots struggle to do.4344 the recaptcha is a captcha system owned by google. the recaptcha v1 and v2 both used to operate by asking the user to match distorted pictures or identify distorted letters and numbers. the recaptcha v3 is designed to not interrupt users and run automatically when pages are loaded or buttons are clicked. this invisible captcha verification happens in the background and no challenges appear, which filters out most basic bots.4546 several early symbolic ai programs were controversially claimed to pass the turing test, either by limiting themselves to scripted situations or by presenting excuses"
  },
  {
    "chunk_id": 72,
    "doc_id": "Turing_test.txt",
    "text": "when pages are loaded or buttons are clicked. this invisible captcha verification happens in the background and no challenges appear, which filters out most basic bots.4546 several early symbolic ai programs were controversially claimed to pass the turing test, either by limiting themselves to scripted situations or by presenting excuses for poor reasoning and conversational abilities, such as mental illness or a poor grasp of english.474849 in 1966, joseph weizenbaum created a program called eliza, which mimicked a rogerian psychotherapist. 50 the program would search the users sentence for keywords before repeating them back to the user, providing the impression of a program listening and paying attention.51 weizenbaum thus succeeded by designing a context where a chatbot could mimic a person despite knowing almost nothing of the real world.48 weizenbaums program was able to fool some people into believing that they were talking to a real person.48 captcha attempts 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 530 kenneth colby created parry in 1972, a program modeled after the behaviour of paranoid schizophrenics. 5250 psychiatrists asked to compare transcripts of conversations generated by the program to those of conversations by actual schizophrenics could only identify about 52 percent of cases correctly (a figure consistent with random guessing).53 in 2001, three programmers developed eugene goostman, a chatbot portraying itself as a 13-yearold boy from odesa who spoke english as a second language. this background was intentionally chosen so judges would forgive mistakes by the program. in a competition, 33 of judges thought goostman was human.435455 in june 2022, googles lamda model received widespread coverage after claims about it having achieved sentience. initially in an article in the economist google research fellow blaise agera y arcas said the chatbot had demonstrated a degree of understanding of social relationships.56 several days later, google engineer blake lemoine claimed in an interview with the washington post that lamda had achieved sentience. lemoine had been placed on leave by google for internal assertions to this effect. google had investigated the claims but dismissed them.5758 openais chatbot, chatgpt, released in november 2022, is based on gpt-3.5 and gpt-4 large language models. celeste biever wrote in a nature article that chatgpt broke the turing test.59 stanford researchers reported that chatgpt passes the test; they found that chatgpt-4 passes a rigorous turing test, diverging from average human behavior chiefly to be more cooperative,60 making it the first computer program to"
  },
  {
    "chunk_id": 73,
    "doc_id": "Turing_test.txt",
    "text": "large language models. celeste biever wrote in a nature article that chatgpt broke the turing test.59 stanford researchers reported that chatgpt passes the test; they found that chatgpt-4 passes a rigorous turing test, diverging from average human behavior chiefly to be more cooperative,60 making it the first computer program to successfully do so.61 in late march 2025, a study evaluated four systems (eliza, gpt-4o, llama-3.1-405b, and gpt4.5) in two randomized, controlled, and pre-registered turing tests with independent participant groups. participants engaged in simultaneous 5-minute conversations with another human participant and one of these systems, then judged which conversational partner they believed to be human. when instructed to adopt a humanlike persona, gpt-4.5 was identified as the human 73 of the timesignificantly more often than the actual human participants. llama-3.1, under the same conditions, was judged to be human 56 of the time, not significantly more or less often than the humans they were compared to. baseline models (eliza and gpt-4o) achieved win rates significantly below chance (23 and 21, respectively). these results provide the first empirical evidence that any artificial system passes a standard three-party turing test. the findings have implications for debates about the nature of intelligence exhibited by large language models (llms) and the social and economic impacts these systems are likely to have.62 saul traiger argues that there are at least three primary versions of the turing test, two of which are offered in computing machinery and intelligence and one that he describes as the standard interpretation.63 while there is some debate regarding whether the standard interpretation is large language models google lamda chatgpt versions 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 630 the imitation game, as described by alan turing in computing machinery and intelligence. player c, through a series of written questions, attempts to determine which of the other two players is a man, and which of the two is the woman. player a, the man, tries to trick player c into making the wrong decision, while player b tries to help player c. figure adapted from saygin, 2000. 7 that described by turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,63 and their strengths and weaknesses are distinct.64 turings original article describes a simple party game involving three players. player a is a man, player b is a woman and player c (who plays the"
  },
  {
    "chunk_id": 74,
    "doc_id": "Turing_test.txt",
    "text": "instead, based on a misreading of his paper, these three versions are not regarded as equivalent,63 and their strengths and weaknesses are distinct.64 turings original article describes a simple party game involving three players. player a is a man, player b is a woman and player c (who plays the role of the interrogator) is of either sex. in the imitation game, player c is unable to see either player a or player b, and can communicate with them only through written notes. by asking questions of player a and player b, player c tries to determine which of the two is the man and which is the woman. player as role is to trick the interrogator into making the wrong decision, while player b attempts to assist the interrogator in making the right one.7 turing then asks: what will happen when a machine takes the part of a in this game? will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? these questions replace our original, can machines think?27 the second version appeared later in turings 1950 paper. similar to the original imitation game test, the role of player a is performed by a computer. however, the role of player b is performed by a man rather than a woman. let us fix our attention on one particular digital computer c. is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, c can be made to play satisfactorily the part of a in the imitation game, the part of b being taken by a man?27 in this version, both player a (the computer) and player b are trying to trick the interrogator into making an incorrect decision. the standard interpretation is not included in the original paper, but is both accepted and debated. common understanding has it that the purpose of the turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human.7 while there is some dispute whether this interpretation was intended by turing, sterrett believes that it was65 and thus conflates the second version with this one, while others, such as traiger,"
  },
  {
    "chunk_id": 75,
    "doc_id": "Turing_test.txt",
    "text": "fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human.7 while there is some dispute whether this interpretation was intended by turing, sterrett believes that it was65 and thus conflates the second version with this one, while others, such as traiger, do not63 this has nevertheless led to what can be viewed as the standard interpretation. in this version, player a is a computer and player b a person of either sex. the role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.66 the fundamental issue with 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 730 the original imitation game test, in which the player a is replaced with a computer. the computer is now charged with the role of the man, while player b continues to attempt to assist the interrogator. figure adapted from saygin, 2000. 7 the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. there are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable. controversy has arisen over which of the alternative formulations of the test turing intended.65 sterrett argues that two distinct tests can be extracted from his 1950 paper and that, despite turings remark, they are not equivalent. the test that employs the party game and compares frequencies of success is referred to as the original imitation game test, whereas the test consisting of a human judge conversing with a human and a machine is referred to as the standard turing test, noting that sterrett equates this with the standard interpretation rather than the second version of the imitation game. sterrett agrees that the standard turing test (stt) has the problems that its critics cite but feels that, in contrast, the original imitation game test (oig test) so defined is immune to many of them, due to a crucial difference: unlike the stt, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. a man can fail the oig test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: the oig test requires the resourcefulness associated with intelligence and not"
  },
  {
    "chunk_id": 76,
    "doc_id": "Turing_test.txt",
    "text": "it employs human performance in setting a criterion for machine intelligence. a man can fail the oig test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: the oig test requires the resourcefulness associated with intelligence and not merely simulation of human conversational behaviour. the general structure of the oig test could even be used with nonverbal versions of imitation games.67 according to huma shah, turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions.68 shah argues the imitation game which turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator.28 still other writers69 have interpreted turing as proposing that the imitation game itself is the test, without specifying how to take into account turings statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game. some writers argue that the imitation game is best understood by its social aspects. in his 1948 paper, turing refers to intelligence as an emotional concept, and notes that the extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. if we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine interpretations 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 830 intelligence. with the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour.70 following this remark and similar ones scattered throughout turings publications, diane proudfoot71 claims that turing held a response-dependence approach to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. shlomo danziger72 promotes a socio-technological interpretation, according to which turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve"
  },
  {
    "chunk_id": 77,
    "doc_id": "Turing_test.txt",
    "text": "to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. shlomo danziger72 promotes a socio-technological interpretation, according to which turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in societys attitude toward machines. according to this reading, turings celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. the first is a technological prediction: i believe that in about fifty years time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70 chance of making the right identification after five minutes of questioning.73 the second prediction turing makes is a sociological one: i believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.73 danziger claims further that for turing, alteration of societys attitude towards machinery is a prerequisite for the existence of intelligent machines: only when the term intelligent machine is no longer seen as an oxymoron the existence of intelligent machines would become logically possible. saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.74 the imitation game also includes a social hack not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.75 a crucial piece of any laboratory test is that there should be a control. turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. he states only that player a is to be replaced with a machine, not that player c is to be made aware of this replacement.27 when colby, fd hilf, s weber and ad kramer tested parry, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.76 as ayse saygin, peter swirski,77 and others have highlighted, this makes a"
  },
  {
    "chunk_id": 78,
    "doc_id": "Turing_test.txt",
    "text": "when colby, fd hilf, s weber and ad kramer tested parry, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.76 as ayse saygin, peter swirski,77 and others have highlighted, this makes a big difference to the implementation and outcome of the test.7 an experimental study looking at gricean maxim violations using transcripts of loebners one-to-one should the interrogator know about the computer? 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 930 (interrogator-hidden interlocutor) prize for ai contests between 1994 and 1999, ayse saygin found significant differences between the responses of participants who knew and did not know about computers being involved.78 the power and appeal of the turing test derives from its simplicity. the philosophy of mind, psychology, and modern neuroscience have been unable to provide definitions of intelligence and thinking that are sufficiently precise and general to be applied to machines. without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. the turing test, even if imperfect, at least provides something that can actually be measured. as such, it is a pragmatic attempt to answer a difficult philosophical question. the format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. turing wrote that the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include.79 john haugeland adds that understanding the words is not enough; you have to understand the topic as well.80 to pass a well-designed turing test, the machine must use natural language, reason, have knowledge and learn. the test can be extended to include video input, as well as a hatch through which objects can be passed: this would force the machine to demonstrate skilled use of well designed vision and robotics as well. together, these represent almost all of the major problems that artificial intelligence research would like to solve.81 the feigenbaum test is designed to take advantage of the broad range of topics available to a turing test. it is a limited form of turings question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry. as a cambridge honours graduate in mathematics, turing might have been expected to propose a test of computer"
  },
  {
    "chunk_id": 79,
    "doc_id": "Turing_test.txt",
    "text": "topics available to a turing test. it is a limited form of turings question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry. as a cambridge honours graduate in mathematics, turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject. instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant. given the status of human sexual dimorphism as one of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. the challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibilityboth of which qualities are on display in this snippet of dialogue which turing has imagined: strengths tractability and simplicity breadth of subject matter emphasis on emotional and aesthetic intelligence 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1030 interrogator: will x please tell me the length of his or her hair? contestant: my hair is shingled, and the longest strands are about nine inches long. when turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry: interrogator: in the first line of your sonnet which reads, shall i compare thee to a summers day, would not a spring day do as well or better? witness: it wouldnt scan. interrogator: how about a winters day. that would scan all right. witness: yes, but nobody wants to be compared to a winters day. turing thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an ai run amok,82 it has been suggested83 that this focus perhaps represents a critical intuition on turings part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a friendly ai. it is further noted, however,"
  },
  {
    "chunk_id": 80,
    "doc_id": "Turing_test.txt",
    "text": "of an increasing awareness of the threat from an ai run amok,82 it has been suggested83 that this focus perhaps represents a critical intuition on turings part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a friendly ai. it is further noted, however, that whatever inspiration turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a standard interpretation of the turing testi.e., one which focuses on a discursive intelligence onlymust be regarded with some caution. turing did not explicitly state that the turing test could be used as a measure of intelligence, or any other human quality. he wanted to provide a clear and understandable alternative to the word think, which he could then use to reply to criticisms of the possibility of thinking machines and to suggest ways that research might move forward. nevertheless, the turing test has been proposed as a measure of a machines ability to think or its intelligence. this proposal has received criticism from both philosophers and computer scientists. the interpretation makes the assumption that an interrogator can determine if a machine is thinking by comparing its behaviour with human behaviour. every element of this assumption has been questioned: the reliability of the interrogators judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. because of these and other considerations, some ai researchers have questioned the relevance of the test to their field. in practice, the tests results can easily be dominated not by the computers intelligence, but by the attitudes, skill, or navet of the questioner. numerous experts in the field, including cognitive scientist gary marcus, insist that the turing test only shows how easy it is to fool humans and is not an indication of machine intelligence.84 turing doesnt specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term average interrogator: the average interrogator would not have more than 70 per cent chance of making the right identification after five minutes weaknesses navet of interrogators 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1130 of questioning.73 chatterbot programs such as eliza have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. in these cases, the interrogators are"
  },
  {
    "chunk_id": 81,
    "doc_id": "Turing_test.txt",
    "text": "per cent chance of making the right identification after five minutes weaknesses navet of interrogators 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1130 of questioning.73 chatterbot programs such as eliza have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. in these cases, the interrogators are not even aware of the possibility that they are interacting with computers. to successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.50 early loebner prize competitions used unsophisticated interrogators who were easily fooled by the machines.39 since 2004, the loebner prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. nonetheless, some of these experts have been deceived by the machines.85 one interesting feature of the turing test is the frequency of the confederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. it has been suggested that what interrogators expect as human responses is not necessarily typical of humans. as a result, some individuals can be categorised as machines. this can therefore work in favour of a competing machine. the humans are instructed to act themselves, but sometimes their answers are more like what the interrogator expects a machine to say.86 this raises the question of how to ensure that the humans are motivated to act human. the turing test does not directly test whether the computer behaves intelligently. it tests only whether the computer behaves like a human being. since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways: some human behaviour is unintelligent the turing test requires that the machine be able to execute all human behaviours, regardless of whether they are intelligent. it even tests for behaviours that may not be considered intelligent at all, such as the susceptibility to insults,87 the temptation to lie or, simply, a high frequency of typing mistakes. if a machine cannot imitate these unintelligent behaviours in detail it fails the test. this objection was raised by the economist, in an article entitled artificial stupidity published shortly after the first loebner prize competition in 1992. the article noted that the first loebner winners victory was due, at least in part, to its ability to imitate human typing errors.38 turing himself had suggested that programs add errors"
  },
  {
    "chunk_id": 82,
    "doc_id": "Turing_test.txt",
    "text": "by the economist, in an article entitled artificial stupidity published shortly after the first loebner prize competition in 1992. the article noted that the first loebner winners victory was due, at least in part, to its ability to imitate human typing errors.38 turing himself had suggested that programs add errors into their output, so as to be better players of the game.88 some intelligent behaviour is inhuman the turing test does not test for highly intelligent behaviours, such as the ability to solve difficult problems or come up with original insights. in fact, it specifically requires deception on the part of the machine: if the machine is more intelligent than a human being it must deliberately avoid appearing too intelligent. if it were to solve a computational problem that is practically impossible for a human to solve, then the interrogator would know the program is not human, and the machine would fail the test. human intelligence vs. intelligence in general 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1230 because it cannot measure intelligence that is beyond the ability of humans, the test cannot be used to build or evaluate systems that are more intelligent than humans. because of this, several test alternatives that would be able to evaluate super-intelligent systems have been proposed.89 the turing test is concerned strictly with how the subject acts the external behaviour of the machine. in this regard, it takes a behaviourist or functionalist approach to the study of the mind. the example of eliza suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.50 john searle has argued that external behaviour cannot be used to determine if a machine is actually thinking or merely simulating thinking.31 his chinese room argument is intended to show that, even if the turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind, consciousness, or intentionality. (intentionality is a philosophical term for the power of thoughts to be about something.) turing anticipated this line of criticism in his original paper,90 writing: i do not wish to give the impression that i think there is no mystery about consciousness. there is, for instance, something of a paradox connected with any attempt to localise it. but i do not"
  },
  {
    "chunk_id": 83,
    "doc_id": "Turing_test.txt",
    "text": "be about something.) turing anticipated this line of criticism in his original paper,90 writing: i do not wish to give the impression that i think there is no mystery about consciousness. there is, for instance, something of a paradox connected with any attempt to localise it. but i do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.91 mainstream ai researchers argue that trying to pass the turing test is merely a distraction from more fruitful research.40 indeed, the turing test is not an active focus of much academic or commercial effortas stuart russell and peter norvig write: ai researchers have devoted little attention to passing the turing test.92 there are several reasons. first, there are easier ways to test their programs. most current research in ai-related fields is aimed at modest and specific goals, such as object recognition or logistics. to test the intelligence of the programs that solve these problems, ai researchers simply give them the task directly. stuart russell and peter norvig suggest an analogy with the history of flight: planes are tested by how well they fly, not by comparing them to birds. aeronautical engineering texts, they write, do not define the goal of their field as making machines that fly so exactly like pigeons that they can fool other pigeons. 92 second, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of ai research. believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence. turing did not intend for his idea to be used to test the intelligence of programshe wanted to provide a clear and understandable example to aid in the discussion of the philosophy of artificial intelligence. 93 john mccarthy argues that we should not be surprised that a philosophical idea consciousness vs. the simulation of consciousness impracticality and irrelevance: the turing test and ai research 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1330 turns out to be useless for practical applications. he observes that the philosophy of ai is unlikely to have any more effect on the practice of ai research than philosophy of"
  },
  {
    "chunk_id": 84,
    "doc_id": "Turing_test.txt",
    "text": "of consciousness impracticality and irrelevance: the turing test and ai research 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1330 turns out to be useless for practical applications. he observes that the philosophy of ai is unlikely to have any more effect on the practice of ai research than philosophy of science generally has on the practice of science.9495 another well known objection raised towards the turing test concerns its exclusive focus on linguistic behaviour (i.e. it is only a language-based experiment, while all the other cognitive faculties are not tested). this drawback downsizes the role of other modality-specific intelligent abilities concerning human beings that the psychologist howard gardner, in his multiple intelligence theory, proposes to consider (verbal-linguistic abilities are only one of those).96 a critical aspect of the turing test is that a machine must give itself away as being a machine by its utterances. an interrogator must then make the right identification by correctly identifying the machine as being just that. if, however, a machine remains silent during a conversation, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.97 even taking into account a parallelhidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.98 by focusing on imitating humans, rather than augmenting or extending human capabilities, the turing test risks directing research and implementation toward technologies that substitute for humans and thereby drive down wages and income for workers. as they lose economic power, these workers may also lose political power, making it more difficult for them to change the allocation of wealth and income. this can trap them in a bad equilibrium. erik brynjolfsson has called this the turing trap99 and argued that there are currently excess incentives for creating machines that imitate rather than augment humans. numerous other versions of the turing test, including those expounded above, have been raised through the years. a modification of the turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse turing test. an example is implied in the work of psychoanalyst wilfred bion, 100 who was particularly fascinated by the storm that resulted from the encounter of one mind by another. in his 2000 book,77 among several other original points with regard to"
  },
  {
    "chunk_id": 85,
    "doc_id": "Turing_test.txt",
    "text": "machines and humans is termed a reverse turing test. an example is implied in the work of psychoanalyst wilfred bion, 100 who was particularly fascinated by the storm that resulted from the encounter of one mind by another. in his 2000 book,77 among several other original points with regard to the turing test, literary scholar peter swirski discussed in detail the idea of what he termed the swirski testessentially the reverse turing test. he pointed out that it overcomes most if not all standard objections levelled at the standard version. the language-centric objection silence the turing trap variations reverse turing test and captcha 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1430 carrying this idea forward, r. d. hinshelwood101 described the mind as a mind recognizing apparatus. the challenge would be for the computer to be able to determine if it were interacting with a human or another computer. this is an extension of the original question that turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could think in a way that we typically define as characteristically human. captcha is a form of reverse turing test. before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. this is intended to prevent automated systems from being used to abuse the site. the rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human. software that could reverse captcha with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of captcha.102 in 2013, researchers at vicarious announced that they had developed a system to solve captcha challenges from google, yahoo!, and paypal up to 90 of the time.103 in 2014, google engineers demonstrated a system that could defeat captcha challenges with 99.8 accuracy.104 in 2015, shuman ghosemajumder, former click fraud czar of google, stated that there were cybercriminal sites that would defeat captcha challenges for a fee, to enable various forms of fraud.105 a further variation is motivated by the concern that modern natural language processing prove to be highly successful in generating text on the basis of a huge text corpus"
  },
  {
    "chunk_id": 86,
    "doc_id": "Turing_test.txt",
    "text": "google, stated that there were cybercriminal sites that would defeat captcha challenges for a fee, to enable various forms of fraud.105 a further variation is motivated by the concern that modern natural language processing prove to be highly successful in generating text on the basis of a huge text corpus and could eventually pass the turing test simply by manipulating words and sentences that have been used in the initial training of the model. since the interrogator has no precise understanding of the training data, the model might simply be returning sentences that exist in similar fashion in the enormous amount of training data. for this reason, arthur schwaninger proposes a variation of the turing test that can distinguish between systems that are only capable of using language and systems that understand language. he proposes a test in which the machine is confronted with philosophical questions that do not depend on any prior knowledge and yet require self-reflection to be answered appropriately.106 another variation is described as the subject-matter expert turing test, where a machines response cannot be distinguished from an expert in a given field. this is also known as a feigenbaum test and was proposed by edward feigenbaum in a 2003 paper.107 robert french (1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied by cognitive science. such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.108 distinguishing accurate use of language from actual understanding subject matter expert turing test low-level cognition test 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1530 the total turing test3 variation of the turing test, proposed by cognitive scientist stevan harnad, 109 adds two further requirements to the traditional turing test. the interrogator can also test the perceptual abilities of the subject (requiring computer vision) and the subjects ability to manipulate objects (requiring robotics).110 paul schweizer argues that harnads work is too weak, and extended it further with the truly total turing test:111 it is essential to note that the tttt is not a test of individual cognitive systems. instead, it is meant to test the overall capacities of the type of cognitive architecture of which particular individuals are tokens. a letter published in communications of the acm112 describes the"
  },
  {
    "chunk_id": 87,
    "doc_id": "Turing_test.txt",
    "text": "total turing test:111 it is essential to note that the tttt is not a test of individual cognitive systems. instead, it is meant to test the overall capacities of the type of cognitive architecture of which particular individuals are tokens. a letter published in communications of the acm112 describes the concept of generating a synthetic patient population and proposes a variation of turing test to assess the difference between synthetic and real patients. the letter states: in the ehr context, though a human physician can readily distinguish between synthetically generated and real live human patients, could a machine be given the intelligence to make such a determination on its own? and further the letter states: before synthetic patient identities become a public health problem, the legitimate ehr market might benefit from applying turing test-like techniques to ensure greater data reliability and diagnostic value. any new techniques must thus consider patients heterogeneity and are likely to have greater complexity than the allen eighth-grade-science-test is able to grade. the minimum intelligent signal test was proposed by chris mckinstry as the maximum abstraction of the turing test,113 in which only binary responses (truefalse or yesno) are permitted, to focus only on the capacity for thought. it eliminates text chat problems like anthropomorphism bias, and does not require emulation of unintelligent human behaviour, allowing for systems that exceed human intelligence. the questions must each stand on their own, however, making it more like an iq test than an interrogation. it is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.114 the organisers of the hutter prize believe that compressing natural language text is a hard ai problem, equivalent to passing the turing test. the data compression test has some advantages over most versions and variations of a turing test, including: it gives a single number that can be directly used to compare which of two machines is more intelligent. it does not require the computer to lie to the judge the main disadvantages of using data compression as a test are: it is not possible to test humans this way. total turing test electronic health records minimum intelligent signal test hutter prize 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1630 it is unknown what particular score on this testif anyis equivalent to passing a humanlevel turing test. a related approach to hutters prize which appeared much"
  },
  {
    "chunk_id": 88,
    "doc_id": "Turing_test.txt",
    "text": "test humans this way. total turing test electronic health records minimum intelligent signal test hutter prize 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1630 it is unknown what particular score on this testif anyis equivalent to passing a humanlevel turing test. a related approach to hutters prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended turing test.115 or by tests which are completely derived from kolmogorov complexity. 116 other related tests in this line are presented by hernandezorallo and dowe.117 algorithmic iq, or aiq for short, is an attempt to convert the theoretical universal intelligence measure from legg and hutter (based on solomonoffs inductive inference) into a working practical test of machine intelligence.118 two major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers. the turing test inspired the ebert test proposed in 2011 by film critic roger ebert which is a test whether a computer-based synthesised voice has sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.119 taking advantage of large language models, in 2023 the research company ai21 labs created an online social experiment titled human or not?120121 it was played more than 10 million times by more than 2 million people.122 it is the biggest turing-style experiment to that date. the results showed that 32 of people could not distinguish between humans and machines.123124 the lovelace test is named for ada lovelace, who suggested only when computers originate things should they be believed to have minds.125 in 2023, david eagleman proposed that a meaningfully intelligent system should be able to do scientific discovery.126 in eaglemans framework, level 1 discovery means the ai is piecing together facts that already exist scattered in the literature (useful but not yet meaningfully intelligent). level 2 discovery, in contrast, describes scientific progress that requires fresh conceptualization, simulation, and verification to arrive at genuinely new frameworks. other tests based on compression or kolmogorov complexity ebert test social turing game alternative tests for machine intelligence lovelace test scientific discovery 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1730 other tests of ai intelligence include the winograd schema challenge, which tests a machines ability to understand natural language.127 . there is also the allen ai science challenge, which tests a machines ability to answer 8th grade science questions.128 another"
  },
  {
    "chunk_id": 89,
    "doc_id": "Turing_test.txt",
    "text": "scientific discovery 28122025 22:58 turing test - wikipedia https:en.wikipedia.orgwikituringtest 1730 other tests of ai intelligence include the winograd schema challenge, which tests a machines ability to understand natural language.127 . there is also the allen ai science challenge, which tests a machines ability to answer 8th grade science questions.128 another test is the artificial general intelligence (agi) test, which asks whether a machine can perform any intellectual task that a human can.129 1990 marked the fortieth anniversary of the first publication of turings computing machinery and intelligence paper, and saw renewed interest in the test. two significant events occurred in that year: the first was the turing colloquium, which was held at the university of sussex in april, and brought together academics and researchers from a wide variety of disciplines to discuss the turing test in terms of its past, present, and future; the second was the formation of the annual loebner prize competition. blay whitby lists four major turning points in the history of the turing test the publication of computing machinery and intelligence in 1950, the announcement of joseph weizenbaums eliza in 1966, kenneth colbys creation of parry, which was first described in 1972, and the turing colloquium in 1990.130 in parallel to the 2008 loebner prize held at the university of reading, 131 the society for the study of artificial intelligence and the simulation of behaviour (aisb) hosted a one-day symposium to discuss the turing test, organised by john barnden, mark bishop, huma shah and kevin warwick. 132 the speakers included the royal institutions director baroness susan greenfield, selmer bringsjord, turings biographer andrew hodges, and consciousness scientist owen holland. no agreement emerged for a canonical turing test, though bringsjord expressed that a sizeable prize would result in the turing test being passed sooner."
  },
  {
    "chunk_id": 90,
    "doc_id": "Overfitting.txt",
    "text": "figure 1. the green line represents an overfitted model and the black line represents a regularized model. while the green line best follows the training data, it is too dependent on that data and is likely to have a higher error rate on new unseen data, illustrated by black-outlined dots, compared to the black line. figure 2. noisy (roughly linear) data is fitted to a linear function and a polynomial function. although the polynomial function is a perfect fit, the linear function can be expected to generalize better: if the two functions were used to extrapolate beyond the fitted data, the linear function should make better predictions. overfitting in mathematical modeling, overfitting is the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably.1 an overfitted model is a mathematical model that contains more parameters than can be justified by the data.2 in the special case of a model that consists of a polynomial function, these parameters represent the degree of a polynomial. the essence of overfitting is to unknowingly extract some of the residual variation (i.e., noise) as if that variation represents the underlying model structure.3:45 underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. an under-fitted model is a model that is missing some parameters or terms that would appear in a correctly specified model.2 underfitting would occur, for example, when fitting a linear model to nonlinear data. such a model will tend to have poor predictive performance. the possibility of over-fitting exists when the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. for example, a model might be selected by maximizing its performance on some set of training data, yet its suitability might be determined by its ability to perform well on unseen data; overfitting occurs when a model begins to memorize training data rather than learning to generalize from a trend. as an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (for an illustration, see figure 2.) such a model will typically fail severely when making predictions. 28122025 22:55 overfitting"
  },
  {
    "chunk_id": 91,
    "doc_id": "Overfitting.txt",
    "text": "number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (for an illustration, see figure 2.) such a model will typically fail severely when making predictions. 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 18 figure 3. the blue dashed line represents an underfitted model. a straight line can never fit a parabola. this model is too simple. overfitting is directly related to approximation error of the selected function class and the optimization error of the optimization procedure. a function class that is too large, in a suitable sense, relative to the dataset size is likely to overfit.4 even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new dataset than on the dataset used for fitting (a phenomenon sometimes known as shrinkage).2 in particular, the value of the coefficient of determination will shrink relative to the original data. to lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, bayesian priors, or dropout). the basis of some techniques is to either (1) explicitly penalize overly complex models or (2) test the models ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter. in statistics, an inference is drawn from a statistical model, which has been selected via some procedure. burnham anderson, in their much-cited text on model selection, argue that to avoid overfitting, we should adhere to the principle of parsimony.3 the authors also state the following.3:3233 overfitted models ... are often free of bias in the parameter estimators, but have estimated (and actual) sampling variances that are needlessly large (the precision of the estimators is poor, relative to what could have been accomplished with a more parsimonious model). false treatment effects tend to be identified, and false variables are included with overfitted models. ... a best approximating model is achieved by properly balancing the errors of underfitting and overfitting. overfitting is more likely to be a serious concern when there is little theory available to guide the analysis, in part because then there tend to be a large number of"
  },
  {
    "chunk_id": 92,
    "doc_id": "Overfitting.txt",
    "text": "with overfitted models. ... a best approximating model is achieved by properly balancing the errors of underfitting and overfitting. overfitting is more likely to be a serious concern when there is little theory available to guide the analysis, in part because then there tend to be a large number of models to select from. the book model selection and model averaging (2008) puts it this way.5 given a data set, you can fit thousands of models at the push of a button, but how do you choose the best? with so many candidate models, overfitting is a real danger. is the monkey who typed hamlet actually a good writer? statistical inference 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 28 figure 4. overfittingovertraining in supervised learning (e.g., a neural network). training error is shown in blue, and validation error in red, both as a function of the number of training cycles. if the validation error increases (positive slope) while the training error steadily decreases (negative slope), then a situation of overfitting may have occurred. the best predictive and fitted model would be where the validation error has its global minimum. in regression analysis, overfitting occurs frequently.6 as an extreme example, if there are p variables in a linear regression with p data points, the fitted line can go exactly through every point.7 for logistic regression or cox proportional hazards models, there are a variety of rules of thumb (e.g. 59,8 109 and 101510 the guideline of 10 observations per independent variable is known as the one in ten rule). in the process of regression model selection, the mean squared error of the random regression function can be split into random noise, approximation bias, and variance in the estimate of the regression function. the biasvariance tradeoff is often used to overcome overfit models. with a large set of explanatory variables that actually have no relation to the dependent variable being predicted, some variables will in general be falsely found to be statistically significant and the researcher may thus retain them in the model, thereby overfitting the model. this is known as freedmans paradox. usually, a learning algorithm is trained using some set of training data: exemplary situations for which the desired output is known. the goal is that the algorithm will also perform well on predicting the output when fed validation data that was not encountered during its training. overfitting is the use"
  },
  {
    "chunk_id": 93,
    "doc_id": "Overfitting.txt",
    "text": "usually, a learning algorithm is trained using some set of training data: exemplary situations for which the desired output is known. the goal is that the algorithm will also perform well on predicting the output when fed validation data that was not encountered during its training. overfitting is the use of models or procedures that violate occams razor, for example by including more adjustable parameters than are ultimately optimal, or by using a more complicated approach than is ultimately optimal. for an example where there are too many adjustable parameters, consider a dataset where training data for y can be adequately predicted by a linear function of two independent variables. such a function requires only three parameters (the intercept and two slopes). replacing this simple function with a new, more complex quadratic function, or with a new, more complex linear function on more than two independent variables, carries a risk: occams razor implies that any given complex function is a priori less probable than any given simple function. if the new, more complicated function is selected instead of the simple function, and if there was not a large enough gain in training data fit to offset the complexity increase, then the new complex function overfits the data and the complex overfitted function will likely perform worse than the simpler function on validation data outside the training dataset, even though the complex function performed as well, or perhaps even better, on the training dataset.11 regression machine learning 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 38 a photograph of anne graham lotz included in the training set of stable diffusion, a text-to-image model an image generated by stable diffusion using the prompt ann sic graham lotz overfitted generative models may produce outputs that are virtually identical to instances from their training set. 12 when comparing different types of models, complexity cannot be measured solely by counting how many parameters exist in each model; the expressivity of each parameter must be considered as well. for example, it is nontrivial to directly compare the complexity of a neural net (which can track curvilinear relationships) with m parameters to a regression model with n parameters.11 overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function."
  },
  {
    "chunk_id": 94,
    "doc_id": "Overfitting.txt",
    "text": "m parameters to a regression model with n parameters.11 overfitting is especially likely in cases where learning was performed too long or where training examples are rare, causing the learner to adjust to very specific random features of the training data that have no causal relation to the target function. in this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse. as a simple example, consider a database of retail purchases that includes the item bought, the purchaser, and the date and time of purchase. its easy to construct a model that will fit the training set perfectly by using the date and time of purchase to predict the other attributes, but this model will not generalize at all to new data because those past times will never occur again. generally, a learning algorithm is said to overfit relative to a simpler one if it is more accurate in fitting known data (hindsight) but less accurate in predicting new data (foresight). one can intuitively understand overfitting from the fact that information from all past experience can be divided into two groups: information that is relevant for the future, and irrelevant information (noise). everything else being equal, the more difficult a criterion is to predict (i.e., the higher its uncertainty), the more noise exists in past information that needs to be ignored. the problem is determining which part to ignore. a learning algorithm that can reduce the risk of fitting noise is called robust. the most obvious consequence of overfitting is poor performance on the validation dataset. other negative consequences include: consequences 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 48 a function that is overfitted is likely to request more information about each item in the validation dataset than does the optimal function; gathering this additional unneeded data can be expensive or error-prone, especially if each individual piece of information must be gathered by human observation and manual data entry. 11 a more complex, overfitted function is likely to be less portable than a simple one. at one extreme, a one-variable linear regression is so portable that, if necessary, it could even be done by hand. at the other extreme are models that can be reproduced only by exactly duplicating the original modelers entire setup, making reuse or scientific reproduction difficult.11 it may be possible to reconstruct details of individual training instances"
  },
  {
    "chunk_id": 95,
    "doc_id": "Overfitting.txt",
    "text": "regression is so portable that, if necessary, it could even be done by hand. at the other extreme are models that can be reproduced only by exactly duplicating the original modelers entire setup, making reuse or scientific reproduction difficult.11 it may be possible to reconstruct details of individual training instances from an overfitted machine learning models training set. this may be undesirable if, for example, the training data includes sensitive personally identifiable information (pii). this phenomenon also presents problems in the area of artificial intelligence and copyright, with the developers of some generative deep learning models such as stable diffusion and github copilot being sued for copyright infringement because these models have been found to be capable of reproducing certain copyrighted items from their training data.1213 the optimal function usually needs verification on bigger or completely new datasets. there are, however, methods like minimum spanning tree or life-time of correlation that applies the dependence between correlation coefficients and time-series (window width). whenever the window width is big enough, the correlation coefficients are stable and dont depend on the window width size anymore. therefore, a correlation matrix can be created by calculating a coefficient of correlation between investigated variables. this matrix can be represented topologically as a complex network where direct and indirect influences between variables are visualized. dropout regularisation (random removal of training set data) can also improve robustness and therefore reduce over-fitting by probabilistically removing inputs to a layer. pruning is another technique that mitigates overfitting and enhances generalization by identifying a sparse, optimal neural network structure,14 while simultaneously reducing the computational cost of both training and inference. underfitting is the inverse of overfitting, meaning that the statistical model or machine learning algorithm is too simplistic to accurately capture the patterns in the data. a sign of underfitting is that there is a high bias and low variance detected in the current model or algorithm used (the inverse of overfitting: low bias and high variance). this can be gathered from the bias-variance tradeoff, which is the method of analyzing a model or algorithm for bias error, variance error, and irreducible error. with a high bias and low variance, the result of the model is that it will inaccurately represent the data points and thus insufficiently be able to predict future data results (see generalization error). as shown in figure 5, the linear line could not represent all the given"
  },
  {
    "chunk_id": 96,
    "doc_id": "Overfitting.txt",
    "text": "irreducible error. with a high bias and low variance, the result of the model is that it will inaccurately represent the data points and thus insufficiently be able to predict future data results (see generalization error). as shown in figure 5, the linear line could not represent all the given data points due to the line not resembling the curvature of the points. we would expect to see a parabola-shaped line as shown in figure 6 and figure 1. if we were to use figure 5 for analysis, we would get false predictive results contrary to the results if we analyzed figure 6. burnham anderson state the following.3:32 remedy underfitting 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 58 figure 5. the red line represents an underfitted model of the data points represented in blue. we would expect to see a parabola shaped line to represent the curvature of the data points. figure 6. the blue line represents a fitted model of the data points represented in green. ... an underfitted model would ignore some important replicable (i.e., conceptually replicable in most other samples) structure in the data and thus fail to identify effects that were actually supported by the data. in this case, bias in the parameter estimators is often substantial, and the sampling variance is underestimated, both factors resulting in poor confidence interval coverage. underfitted models tend to miss important treatment effects in experimental settings. there are multiple ways to deal with underfitting: 1. increase the complexity of the model: if the model is too simple, it may be necessary to increase its complexity by adding more features, increasing the number of parameters, or using a more flexible model. however, this should be done carefully to avoid overfitting.15 2. use a different algorithm: if the current algorithm is not able to capture the patterns in the data, it may be necessary to try a different one. for example, a neural network may be more effective than a linear regression model for some types of data.15 3. increase the amount of training data: if the model is underfitting due to a lack of data, increasing the amount of training data may help. this will allow the model to better capture the underlying patterns in the data.15 4. regularization: regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. it"
  },
  {
    "chunk_id": 97,
    "doc_id": "Overfitting.txt",
    "text": "lack of data, increasing the amount of training data may help. this will allow the model to better capture the underlying patterns in the data.15 4. regularization: regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. it can also be used to prevent underfitting by controlling the complexity of the model.16 5. ensemble methods: ensemble methods combine multiple models to create a more accurate prediction. this can help reduce underfitting by allowing multiple models to work together to capture the underlying patterns in the data. 6. feature engineering: feature engineering involves creating new model features from the existing ones that may be more relevant to the problem at hand. this can help improve the accuracy of the model and prevent underfitting.15 benign overfitting describes the phenomenon of a statistical model that seems to generalize well to unseen data, even when it has been fit perfectly on noisy training data (i.e., obtains perfect predictive accuracy on the training set). the phenomenon is of particular interest in deep neural networks, but is studied from a theoretical perspective in the context of much simpler models, such resolving underfitting benign overfitting 28122025 22:55 overfitting - wikipedia https:en.wikipedia.orgwikioverfitting 68 as linear regression. in particular, it has been shown that overparameterization is essential for benign overfitting in this setting. in other words, the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size.17 biasvariance tradeoff curve fitting data dredging double descent feature selection feature engineering freedmans paradox generalization error goodness of fit grokking (machine learning) life-time of correlation model selection researcher degrees of freedom occams razor primary model vapnikchervonenkis dimension larger vc dimension implies larger risk of overfitting"
  },
  {
    "chunk_id": 98,
    "doc_id": "Black_hole.txt",
    "text": "for other uses, see black hole (disambiguation). a black spot, center, surrounded by a doughnut-shaped orange-yellow ring with the blackness of empty space in the background an image of the core region of messier 87, a supermassive black hole, processed from a sparse array of radio telescopes known as the eht with colors indicating brightness temperature.12 a black circle, center, surrounded by three bright white blobs of light and the orange-colored milky way distorted into an ark around it. the background is a starry sky with a second image of the milky way, not warped, visible in the top left corner. simulated view of a schwarzschild black hole in front of the large magellanic cloud. the gravitational lensing effect produces two enlarged but distorted views of the cloud. across the top, the milky way disk appears distorted into an arc.3 a black hole is an astronomical body so compact that its gravity prevents anything, including light, from escaping. albert einsteins theory of general relativity predicts that a sufficiently compact mass will form a black hole.4 the boundary of no escape is called the event horizon. in general relativity, a black holes event horizon seals an objects fate but produces no locally detectable change when crossed.5 general relativity also predicts that every black hole should have a central singularity, where the curvature of spacetime is infinite. in many ways, a black hole acts like an ideal black body, as it reflects no light.67 quantum field theory in curved spacetime predicts that event horizons emit hawking radiation, with the same spectrum as a black body of a temperature inversely proportional to its mass. this temperature is of the order of billionths of a kelvin for stellar black holes, making it essentially impossible to observe directly. objects whose gravitational fields are too strong for light to escape were first considered in the 18th century by john michell and pierre-simon laplace. in 1916, karl schwarzschild found the first modern solution of general relativity that would characterise a black hole. due to his influential research, the schwarzschild metric is named after him. david finkelstein, in 1958, first published the interpretation of black hole as a region of space from which nothing can escape. black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. the first black hole known was"
  },
  {
    "chunk_id": 99,
    "doc_id": "Black_hole.txt",
    "text": "1958, first published the interpretation of black hole as a region of space from which nothing can escape. black holes were long considered a mathematical curiosity; it was not until the 1960s that theoretical work showed they were a generic prediction of general relativity. the first black hole known was cygnus x-1, identified by several researchers independently in 1971.89 black holes typically form when massive stars collapse at the end of their life cycle. after a black hole has formed, it can grow by absorbing mass from its surroundings. supermassive black holes of millions of solar masses may form by absorbing other stars and merging with other black holes, or via direct collapse of gas clouds. there is consensus that supermassive black holes exist in the centres of most galaxies. the presence of a black hole can be inferred through its interaction with other matter and with electromagnetic radiation such as visible light. matter falling toward a black hole can form an accretion disk of infalling plasma, heated by friction and emitting light. in extreme cases, this creates a quasar, some of the brightest objects in the universe. merging black holes can also be detected by observation of the gravitational waves they emit. if other stars are orbiting a black hole, their orbits can be used to determine the black holes mass and location. such observations can be used to exclude possible alternatives such as neutron stars. in this way, astronomers have identified numerous stellar black hole candidates in binary systems and established that the radio source known as sagittarius a, at the core of the milky way galaxy, contains a supermassive black hole of about 4.3 million solar masses. history the idea of a body so massive that even light could not escape was briefly proposed by english astronomical pioneer and clergyman john michell and independently by french scientist pierre-simon laplace. both scholars proposed very large stars in contrast to the modern concept of an extremely dense object.10 michells idea, in a short part of a letter published in 1784,11 calculated that a star with the same density but 500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed the speed of light.12: 122 michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies.10 in 1796, laplace mentioned that"
  },
  {
    "chunk_id": 100,
    "doc_id": "Black_hole.txt",
    "text": "500 times the radius of the sun would not let any emitted light escape; the surface escape velocity would exceed the speed of light.12: 122 michell correctly noted that such supermassive but non-radiating bodies might be detectable through their gravitational effects on nearby visible bodies.10 in 1796, laplace mentioned that a star could be invisible if it were sufficiently large while speculating on the origin of the solar system in his book exposition du systme du monde. franz xaver von zach asked laplace for a mathematical analysis, which laplace provided and published in a journal edited by von zach.10 laplace omitted his comment about invisible stars in later editions of his book, perhaps because thomas youngs wave theory of light had cast doubt on the validity of the corpuscles of light used in laplaces mathematical analysis.12: 123 general relativity see also: history of general relativity general relativity spacetime curvature schematic displaystyle gmu nu lambda gmu nu kappa tmu nu introduction historytimelinetests mathematical formulation fundamental concepts equivalence principlespecial relativityworld linepseudo-riemannian manifold phenomena kepler problemgravitational lensinggravitational redshiftgravitational time dilationgravitational wavesframe-dragginggeodetic effectevent horizonsingularityblack hole spacetime spacetime diagramsminkowski spacetimeeinsteinrosen bridge equationsformalisms equations linearized gravityeinstein field equationsfriedmanngeodesicsmathissonpapapetroudixonhamiltonjacobieinsteinraychaudhuri formalisms admnpbssnpost-newtonian advanced theory kaluzaklein theoryquantum gravity solutions schwarzschild (interior)reissnernordstrmeinsteinrosen waveswormholegdelkerrkerrnewmankerrnewmande sitterkasnerkantowski-sachslematretolmanwahlquisttaubnutmilnerobertsonwalkeroppenheimersnyderpp-wavevan stockum dusthartlethornevaidyaperesde sitter-schwarzschildmcvittieweyl scientists einsteinlorentzhilbertpoincarschwarzschildde sitterreissnernordstrmweyleddingtonfriedmannmilnezwickylematreoppenheimergdelwheelerrobertsonbardeenwalkerkerrchandrasekharehlerspenrosehawkingraychaudhuritaylorhulsevan stockumtaubnewmanyauthorneothers icon physics portal category vte in 1905 albert einstein showed that the laws of electromagnetism would be invariant under a lorentz transformation: they would be identical for observers travelling at different velocities relative to each other. this discovery became known as the principle of special relativity. although the laws of mechanics had already been shown to be invariant, gravity remained yet to be included.13: 19 to add gravity to the his theory of relativity, einstein was guided by observations by galileo galilei, isaac newton and others which showed inertial mass equalled gravitational mass.13: 11 in 1907, einstein published a paper proposing his equivalence principle, the hypothesis that this equality means the two forms of mass have a common cause. using the principle, einstein predicted the redshift effect of gravity on light.13: 19 in 1911, einstein predicted14 the deflection of light by massive bodies, but his analysis was premature and off by a factor of two.13: 19 by 1917, einstein refined these ideas into his general theory of relativity, which explained how matter affects spacetime, which in turn affects the motion of other matter.151617 this theory formed the basis for"
  },
  {
    "chunk_id": 101,
    "doc_id": "Black_hole.txt",
    "text": "light by massive bodies, but his analysis was premature and off by a factor of two.13: 19 by 1917, einstein refined these ideas into his general theory of relativity, which explained how matter affects spacetime, which in turn affects the motion of other matter.151617 this theory formed the basis for black hole physics.18 singular solutions in general relativity only a few months after einstein published the field equations describing general relativity, astrophysicist karl schwarzschild set out to apply the idea to stars. he assumed spherical symmetry with no spin and found a solution to einsteins equations.12: 124 19 a few months after schwarzschild, johannes droste, a student of hendrik lorentz, independently gave the same solution for the point mass using a different set of coordinates.2021 at a certain radius from the center of the mass, the schwarzschild solution became singular, meaning that some of the terms in the einstein equations became infinite. the nature of this radius, which later became known as the schwarzschild radius, was not understood at the time.22 many physicists of the early 20th century were skeptical of the existence of black holes. in a 1926 popular science book, arthur eddington discussed the idea of a star with mass compressed to its schwarzschild radius, but his analysis was meant to illustrate issues in the then-poorly-understood theory of general relativity rather than to seriously analyze the problem: eddington did not believe black holes existed.2312: 134 in 1939, einstein himself used his theory of general relativity in an attempt to prove that black holes were impossible.2425 his work relied on increasing pressure or increasing centrifugal force balancing the force of gravity so that the object would not collapse beyond its schwarzschild radius. he missed the possibility that implosion would drive the system below this critical value.12: 135 gravity vs degeneracy pressure by the 1920s, astronomers had classified a number of white dwarf stars as too cool and dense to be explained by the gradual cooling of ordinary stars. in 1926, ralph fowler showed that quantum-mechanical degeneracy pressure was larger than thermal pressure at these densities.12: 145 in 1931, using a combination of special relativity and quantum mechanics, subrahmanyan chandrasekhar calculated that a non-rotating body of electron-degenerate matter below a certain limiting mass (now called the chandrasekhar limit at 1.4 m) is stable, and by 1934 he showed that this explained the catalog of white dwarf stars.12: 151 at the"
  },
  {
    "chunk_id": 102,
    "doc_id": "Black_hole.txt",
    "text": "a combination of special relativity and quantum mechanics, subrahmanyan chandrasekhar calculated that a non-rotating body of electron-degenerate matter below a certain limiting mass (now called the chandrasekhar limit at 1.4 m) is stable, and by 1934 he showed that this explained the catalog of white dwarf stars.12: 151 at the same meeting where chandrasekhar announced his results, eddington pointed out that stars above this limit would radiate until they were sufficiently dense to prevent light from exiting, a conclusion he considered absurd. eddington and, later, lev landau argued that some yet unknown mechanism would stop the collapse.26 they were partially correct: a white dwarf slightly more massive than the chandrasekhar limit will collapse into a neutron star, which is itself stable.27 these arguments from senior scientists delayed acceptance of chandrasekhars model.12: 159 in the 1930s, fritz zwicky and walter baade studied stellar novae, focusing on exceptionally bright ones they called supernovae. zwicky promoted the idea that supernovae produced stars with the density of atomic nucleineutron starsbut this idea was largely ignored.12: 171 in 1937, lev landau published a detailed model of a nuclear core model for stellar cores, which caught the attention of robert oppenheimer. in 1939, based on chandrasekhars reasoning, oppenheimer and george volkoff predicted that neutron stars below a certain mass limitnow known as the tolmanoppenheimervolkoff limitwould be stable due to neutron degeneracy pressure. above that limit, they reasoned that either their model would not apply or that gravitational contraction would not stop.28: 380381 john archibald wheeler and two of his students resolved questions about the model behind the tolmanoppenheimervolkoff (tov) limit. harrison and wheeler developed the equations of state relating density to pressure for cold matter all the way from atoms through electron degeneracy to neutron degeneracy. masami wakano and wheeler then used the equations to compute the equilibrium curve for stars, relating mass to circumference. they found no additional features that would invalidate the tov limit. this meant that the only thing that could prevent black holes from forming was a dynamic process ejecting sufficient mass from a star as it cooled.12: 205 wheeler held the view that the neutrons in an imploding star would convert to electromagnetic radiation fast enough that the resulting light would not be trapped in a black hole.12: 210 birth of modern model the modern concept of black holes was formulated by robert oppenheimer and his student hartland snyder in 1939.2429:"
  },
  {
    "chunk_id": 103,
    "doc_id": "Black_hole.txt",
    "text": "view that the neutrons in an imploding star would convert to electromagnetic radiation fast enough that the resulting light would not be trapped in a black hole.12: 210 birth of modern model the modern concept of black holes was formulated by robert oppenheimer and his student hartland snyder in 1939.2429: 80 in the paper,30 oppenheimer and snyder solved einsteins equations of general relativity for an idealized imploding star, in a model later called the oppenheimersnyder model, then described the results from far outside the star. the implosion starts as one might expect: the star material rapidly collapses inward. but as density of the star increases, gravitational time dilation increases and the collapse, viewed from afar, seems to slow down. once the star reached a critical radiusits schwarzschild radiusfaraway viewers would no long see the implosion. the light from the implosion would be infinitely redshifted and time dilation would be so extreme that it would appear frozen in time.12: 217 in 1958, david finkelstein identified the schwarzschild surface as an event horizon, calling it a perfect unidirectional membrane: causal influences can cross it in only one direction. in this sense, events that occur inside of the black hole cannot affect events that occur outside of the black hole.31 finkelstein created a new reference frame to include the point of view of infalling observers.29: 103 finkelsteins solution extended the schwarzschild solution for the future of observers falling into a black hole. a similar concept had already been found by martin kruskal, but its significance had not been fully understood at the time.29: 103 finkelsteins new frame of reference allowed events at the event horizon of an imploding star to be related to events far away. by 1962 the two points of view were reconciled, convincing many skeptics that implosion into a black hole made physical sense.12: 226 golden age a black and white image of a black hole with an accretion disk on punch cards. the black hole is visible as a black semicircle in the center with a white ring overlaid. around it, a bright white accretion disk wraps around the top and bottom of the black hole and to its sides, appearing brightest on the left side of the black hole. the first simulated image of a black hole, created by jean-pierre luminet in 1978 and featuring the characteristic shadow, photon sphere, and lensed accretion disk. the disk is brighter"
  },
  {
    "chunk_id": 104,
    "doc_id": "Black_hole.txt",
    "text": "top and bottom of the black hole and to its sides, appearing brightest on the left side of the black hole. the first simulated image of a black hole, created by jean-pierre luminet in 1978 and featuring the characteristic shadow, photon sphere, and lensed accretion disk. the disk is brighter on one side due to the doppler beaming.3233 the era from the mid-1960s to the mid-1970s was the golden age of black hole research, when general relativity and black holes became mainstream subjects of research.3412: 258 in this period, more general black hole solutions were found. in 1963, roy kerr found the exact solution for a rotating black hole.3536 two years later, ezra newman found the cylindrically symmetric solution for a black hole that is both rotating and electrically charged.37 in 1967, werner israel found that the schwarzschild solution was the only possible solution for a nonspinning, uncharged black hole, and couldnt have any additional parameters. in that sense, a schwarzschild black hole would be defined by its mass alone, and any two schwarzschild black holes with the same mass would be identical.38 israel later found that reissner-nordstrom black holes were only defined by their mass and electric charge, while brandon carter discovered that kerr black holes only had two degrees of freedom, mass and spin.3940 together, these findings became known as the no-hair theorem, which states that a stationary black hole is completely described by the three parameters of the kerrnewman metric: mass, angular momentum, and electric charge.41 at first, it was suspected that the strange mathematical singularities found in each of the black hole solutions only appeared due to the assumption that a black hole would be perfectly spherically symmetric, and therefore the singularities would not appear in generic situations where black holes would not necessarily be symmetric. this view was held in particular by vladimir belinski, isaak khalatnikov, and evgeny lifshitz, who tried to prove that no singularities appear in generic solutions, although they would later reverse their positions.42 however, in 1965, roger penrose proved that general relativity without quantum mechanics requires that singularities appear in all black holes.4344 shortly afterwards, hawking generalized penroses solution to find that in all but a few physically infeasible scenarios, a cosmological big bang singularity is inevitable unless quantum gravity intervenes.45 astronomical observations also made great strides during this era. in 1967, antony hewish and jocelyn bell burnell discovered pulsars4647 and"
  },
  {
    "chunk_id": 105,
    "doc_id": "Black_hole.txt",
    "text": "black holes.4344 shortly afterwards, hawking generalized penroses solution to find that in all but a few physically infeasible scenarios, a cosmological big bang singularity is inevitable unless quantum gravity intervenes.45 astronomical observations also made great strides during this era. in 1967, antony hewish and jocelyn bell burnell discovered pulsars4647 and by 1969, these were shown to be rapidly rotating neutron stars.48 until that time, neutron stars, like black holes, were regarded as just theoretical curiosities, but the discovery of pulsars showed their physical relevance and spurred a further interest in all types of compact objects that might be formed by gravitational collapse.49 based on observations in greenwich and toronto in the early 1970s, cygnus x-1, a galactic x-ray source discovered in 1964, became the first astronomical object commonly accepted to be a black hole.5051 work by james bardeen, jacob bekenstein, carter, and hawking in the early 1970s led to the formulation of black hole thermodynamics.52 these laws describe the behaviour of a black hole in close analogy to the laws of thermodynamics by relating mass to energy, area to entropy, and surface gravity to temperature. the analogy was completed12: 442 when hawking, in 1974, showed that quantum field theory implies that black holes should radiate like a black body with a temperature proportional to the surface gravity of the black hole, predicting the effect now known as hawking radiation.53 modern research and observation the first strong evidence for black holes came from combined x-ray and optical observations of cygnus x-1 in 1972.54 the x-ray source, located in the cygnus constellation, was discovered through a survey by two suborbital rockets, as the blocking of x-rays by earths atmosphere makes it difficult to detect them from the ground.555657 unlike stars or pulsars, cygnus x-1 was not associated with any prominent radio or optical source.5758 in 1972, louise webster, paul murdin, and, independently, charles thomas bolton, found that cygnus x-1 was actually in a binary system with the supergiant star hde 226868. using the emission patterns of the visible star, both research teams found that the mass of cygnus x-1 was likely too large to be a white dwarf or neutron star, indicating that it was probably a black hole.5960 further research strengthened their hypothesis.6162 while cygnus x-1, a stellar-mass black hole, was generally accepted by the scientific community as a black hole by the end of 1973,61 it would be decades before"
  },
  {
    "chunk_id": 106,
    "doc_id": "Black_hole.txt",
    "text": "large to be a white dwarf or neutron star, indicating that it was probably a black hole.5960 further research strengthened their hypothesis.6162 while cygnus x-1, a stellar-mass black hole, was generally accepted by the scientific community as a black hole by the end of 1973,61 it would be decades before a supermassive black hole would gain the same broad recognition. although, as early as the 1960s, physicists such as donald lynden-bell and martin rees had suggested that powerful quasars in the center of galaxies were powered by accreting supermassive black holes, little observational proof existed at the time.6364 however, the hubble space telescope, launched decades later, found that supermassive black holes were not only present in these active galactic nuclei, but that supermassive black holes in the center of galaxies were ubiquitous: almost every galaxy had a supermassive black hole at its center, many of which were quiescent.6566 in 1999, david merritt proposed the msigma relation, which related the dispersion of the velocity of matter in the center bulge of a galaxy to the mass of the supermassive black hole at its core.67 subsequent studies confirmed this correlation.686970 around the same time, based on telescope observations of the velocities of stars at the center of the milky way galaxy, independent work groups led by andrea ghez and reinhard genzel concluded that the compact radio source in the center of the galaxy, sagittarius a, was likely a supermassive black hole.7172 graphs of the first detections of gravitational waves at the hanford and livingston ligo sections, along with comparisons to theoretical predictions, noise, and visual renderings. the readings appear as periodic waves that increase in magnitude over time before suddenly dropping back down. the first detection of gravitational waves, imaged by ligo observatories in hanford site, washington and livingston, louisiana on 11 february 2016, the ligo scientific collaboration and virgo collaboration announced the first direct detection of gravitational waves, named gw150914, representing the first observation of a black hole merger.73 at the time of the merger, the black holes were approximately 1.4 billion light-years away from earth and had masses of 30 and 35 solar masses.74: 6 the mass of the resulting black hole was approximately 62 solar masses, with an additional three solar masses radiated away as gravitational waves.7475 the laser interferometer gravitational-wave observatory (ligo) detected the gravitational waves by using two mirrors spaced four kilometers apart to measure microscopic changes"
  },
  {
    "chunk_id": 107,
    "doc_id": "Black_hole.txt",
    "text": "and 35 solar masses.74: 6 the mass of the resulting black hole was approximately 62 solar masses, with an additional three solar masses radiated away as gravitational waves.7475 the laser interferometer gravitational-wave observatory (ligo) detected the gravitational waves by using two mirrors spaced four kilometers apart to measure microscopic changes in length.76 in 2017, rainer weiss, kip thorne, and barry barish, who had spearheaded the project, were awarded the nobel prize in physics for their work.77 since the initial discovery in 2015, hundreds more gravitational waves have been observed by ligo and another interferometer, virgo.78 over a black background of empty space, an orange-red donut of gas lies in the center of the image, with a black circle--the black holes shadow--in the middle of the donut. image by the event horizon telescope of the supermassive black hole in the center of messier 87 on 10 april 2019, the first direct image of a black hole and its vicinity was published, following observations made by the event horizon telescope (eht) in 2017 of the supermassive black hole in messier 87s galactic centre.798081 the observations were carried out by eight observatories in six geographical locations across four days and totaled five petabytes of data.828384 in 2022, the event horizon telescope collaboration released an image of the black hole in the center of the milky way galaxy, sagittarius a; the data had been collected in 2017.85 detailed analysis of the motion of stars recorded by the gaia mission produced evidence in 202286 and 202387 of a black hole named gaia bh1 in a binary with a sun-like star about 1,560 light-years (480 parsecs) away. gaia bh1 is currently the closest known black hole to earth.8889 two more black holes have since been found from gaia data, one in a binary with a red giant90 and the other in a binary with a g-type star.91 in 2020, the nobel prize in physics was awarded for work on black holes. andrea ghez and reinhard genzel shared one-half for their discovery that sagittarius a is a supermassive black hole.92 penrose received the other half for his work showing that the mathematics of general relativity requires the formation of black holes.939495 cosmologists lamented that hawkings extensive theoretical work on black holes would not be honored since he died in 2018.96 etymology in december 1967, a student reportedly suggested the phrase black hole at a lecture by john"
  },
  {
    "chunk_id": 108,
    "doc_id": "Black_hole.txt",
    "text": "work showing that the mathematics of general relativity requires the formation of black holes.939495 cosmologists lamented that hawkings extensive theoretical work on black holes would not be honored since he died in 2018.96 etymology in december 1967, a student reportedly suggested the phrase black hole at a lecture by john wheeler; wheeler adopted the term for its brevity and advertising value, and wheelers stature in the field ensured it quickly caught on,2997 leading some to credit wheeler with coining the phrase.98 however, the term was used by others around that time. science writer marcia bartusiak traces the term black hole to physicist robert h. dicke, who in the early 1960s reportedly compared the phenomenon to the black hole of calcutta, notorious as a prison where people entered but never left alive. the term black hole was used in print by life and science news magazines in 1963, and by science journalist ann ewing in her article black holes in space, dated 18 january 1964, which was a report on a meeting of the american association for the advancement of science held in cleveland, ohio.29 definition a black hole is generally defined as a region of spacetime from which no information-carrying signals or objects can escape.99 however, verifying an object as a black hole by this definition would require waiting for an infinite time and at an infinite distance from the black hole to verify that indeed, nothing has escaped, and thus cannot be used to identify a physical black hole.100 broadly, physicists do not have a precisely-agreed-upon definition of a black hole. among astrophysicists, a black hole is a compact object with a mass larger than four solar masses.101 a black hole may also be defined as a reservoir of information102: 142 or a region where space is falling inwards faster than the speed of light.103104 properties the no-hair theorem postulates that, once it achieves a stable condition after formation, a black hole has only three independent physical properties: mass, electric charge, and angular momentum; the black hole is otherwise featureless. if the conjecture is true, any two black holes that share the same values for these properties, or parameters, are indistinguishable from one another. the degree to which the conjecture is true for real black holes is currently an unsolved problem.41 the simplest static black holes have mass but neither electric charge nor angular momentum. these black holes are"
  },
  {
    "chunk_id": 109,
    "doc_id": "Black_hole.txt",
    "text": "that share the same values for these properties, or parameters, are indistinguishable from one another. the degree to which the conjecture is true for real black holes is currently an unsolved problem.41 the simplest static black holes have mass but neither electric charge nor angular momentum. these black holes are often referred to as schwarzschild black holes after karl schwarzschild, who discovered the solution in 1916.19 according to birkhoffs theorem, it is the only vacuum solution that is spherically symmetric.105 solutions describing more general black holes also exist. non-rotating charged black holes are described by the reissnernordstrm metric, while the kerr metric describes a non-charged rotating black hole. the most general stationary black hole solution known is the kerrnewman metric, which describes a black hole with both charge and angular momentum.106 mass radii for shadow and photon sphere relative to the event horizon the simplest static black holes have mass but neither electric charge nor angular momentum. contrary to the popular notion of a black hole sucking in everything in its surroundings, from far away, the external gravitational field of a black hole is identical to that of any other body of the same mass.107 while a black hole can theoretically have any positive mass, the charge and angular momentum are constrained by the mass. the total electric charge q and the total angular momentum j are expected to satisfy the inequality displaystyle frac q24pi epsilon 0frac c2j2gm2leq gm2 for a black hole of mass m. black holes with the maximum possible charge or spin satisfying this inequality are called extremal black holes. solutions of einsteins equations that violate this inequality exist, but they do not possess an event horizon. these are so-called naked singularities that can be observed from the outside.108 because these singularities make the universe inherently unpredictable, many physicists believe they could not exist.109 the weak cosmic censorship hypothesis, proposed by sir roger penrose, rules out the formation of such singularities, when they are created through the gravitational collapse of realistic matter. however, this theory has not yet been proven, and some physicists believe that naked singularities could exist.110 it is also unknown whether black holes could even become extremal, forming naked singularities, since natural processes counteract increasing spin and charge when a black hole becomes near-extremal.110111112 the total mass of a black hole can be estimated by analyzing the motion of objects near the black hole,"
  },
  {
    "chunk_id": 110,
    "doc_id": "Black_hole.txt",
    "text": "could exist.110 it is also unknown whether black holes could even become extremal, forming naked singularities, since natural processes counteract increasing spin and charge when a black hole becomes near-extremal.110111112 the total mass of a black hole can be estimated by analyzing the motion of objects near the black hole, such as stars or gas.66 spin and angular momentum all black holes spin, often fastone supermassive black hole, grs 1915105 has been estimated to spin at over 1,000 revolutions per second.113114 the milky ways central black hole sagittarius a rotates at about 90 of the maximum rate.115116 the spin rate can be inferred from measurements of atomic spectral lines in the x-ray range. as gas near the black hole plunges inward, high energy x-ray emission from electron-positron pairs illuminates the gas further out, appearing red-shifted due to relativistic effects. depending on the spin of the black hole, this plunge happens at different radii from the hole, with different degrees of redshift. astronomers can use the gap between the x-ray emission of the outer disk and the redshifted emission from plunging material to determine the spin of the black hole.117 a newer way to estimate spin is based on the temperature of gasses accreting onto the black hole. the method requires an independent measurement of the black hole mass and inclination angle of the accretion disk followed by computer modeling. gravitational waves from coalescing binary black holes can also provide the spin of both progenitor black holes and the merged hole, but such events are rare.117 a spinning black hole has angular momentum. the supermassive black hole in the center of the messier 87 (m87) galaxy appears to have an angular momentum very close to the maximum theoretical value.115118119 that uncharged limit is 120 displaystyle jleq frac gm2c, allowing definition of a dimensionless spin magnitude such that120121 displaystyle 0leq frac cjgm2leq 1. charge most black holes are believed to have an approximately neutral charge. for example, michal zajaek, arman tursunov, andreas eckart, and silke britzen found the electric charge of sagittarius a to be at least ten orders of magnitude below the theoretical maximum.122 a charged black hole repels other like charges just like any other charged object.123 if a black hole were to become charged, particles with an opposite sign of charge would be pulled in by the extra electromagnetic force, while particles with the same sign of charge would"
  },
  {
    "chunk_id": 111,
    "doc_id": "Black_hole.txt",
    "text": "the theoretical maximum.122 a charged black hole repels other like charges just like any other charged object.123 if a black hole were to become charged, particles with an opposite sign of charge would be pulled in by the extra electromagnetic force, while particles with the same sign of charge would be repelled, neutralizing the black hole. this effect may not be as strong if the black hole is also spinning.124 the presence of charge can reduce the diameter of the black hole by up to 38.122125 the charge q for a nonspinning black hole is bounded by displaystyle qleq sqrt gm, where g is the gravitational constant and m is the black holes mass.126 classification black hole classifications class approx. mass approx. radius ultramassive black hole 1091011 m 1,000 au supermassive black hole 106109 m 0.001400 au intermediate-mass black hole 102105 m 103 km rearth stellar black hole 2150 m 30 km micro black hole up to mmoon up to 0.1 mm black holes can have a wide range of masses. the minimum mass of a black hole formed by stellar gravitational collapse is governed by the maximum mass of a neutron star and is believed to be approximately two-to-four solar masses.127128129 however, theoretical primordial black holes, believed to have formed soon after the big bang, could be far smaller, with masses as little as 105 grams at formation.130 these very small black holes are sometimes called micro black holes.131132 black holes formed by stellar collapse are called stellar black holes. estimates of their maximum mass at formation vary, but generally range from 10 to 100 solar masses, with higher estimates for black holes progenated by low-metallicity stars.133134135136137 the mass of a black hole formed via a supernova has a lower bound: if the progenitor star is too small, the collapse may be stopped by the degeneracy pressure of the stars constituents, allowing the condensation of matter into an exotic denser state. degeneracy pressure occurs from the pauli exclusion principleparticles will resist being in the same place as each other. smaller progenitor stars, with masses less than about 8 m, will be held together by the degeneracy pressure of electrons and will become a white dwarf. for more massive progenitor stars, electron degeneracy pressure is no longer strong enough to resist the force of gravity and the star will be held together by neutron degeneracy pressure, which can occur at"
  },
  {
    "chunk_id": 112,
    "doc_id": "Black_hole.txt",
    "text": "will be held together by the degeneracy pressure of electrons and will become a white dwarf. for more massive progenitor stars, electron degeneracy pressure is no longer strong enough to resist the force of gravity and the star will be held together by neutron degeneracy pressure, which can occur at much higher densities, forming a neutron star. if the star is still too massive, even neutron degeneracy pressure will not be able to resist the force of gravity and the star will collapse into a black hole.138139: 5.8 stellar black holes can also gain mass via accretion of nearby matter, often from a companion object such as a star.140141142 black holes that are larger than stellar black holes but smaller than supermassive black holes are called intermediate-mass black holes, with masses of approximately 102 to 105 solar masses. these black holes seem to be rarer than their stellar and supermassive counterparts, with relatively few candidates having been observed.143137144 physicists have speculated that such black holes may form from collisions in globular and star clusters or at the center of low-mass galaxies.145146147148149 they may also form as the result of mergers of smaller black holes, with several ligo observations finding merged black holes within the 110-350 solar mass range.150151 the black holes with the largest masses are called supermassive black holes, with masses more than 106 times that of the sun.143152153 these black holes are believed to exist at the centers of almost every large galaxy, including the milky way.6566154155 some scholars have theorized that the collapse of very massive population iii stars in the early universe could have produced black holes of up to 103 m. these black holes could be the seeds of the supermassive black holes found in the centres of most galaxies.156 some scientists have proposed a subcategory of even larger black holes, called ultramassive black holes, with masses greater than 109-1010 solar masses.157158159 theoretical models predict that the accretion disc that feeds black holes will be unstable once a black hole reaches 50-100 billion times the mass of the sun, setting an upper limit to black hole mass.160161 radius for a nonspinning, uncharged black hole, the radius of the event horizon, or schwarzschild radius, is proportional to the mass, m, through displaystyle rmathrm s frac 2gmc2approx 2.95,frac mmodot mathrm km, where rs is the schwarzschild radius and m is the mass of the sun.162: 124 for"
  },
  {
    "chunk_id": 113,
    "doc_id": "Black_hole.txt",
    "text": "hole mass.160161 radius for a nonspinning, uncharged black hole, the radius of the event horizon, or schwarzschild radius, is proportional to the mass, m, through displaystyle rmathrm s frac 2gmc2approx 2.95,frac mmodot mathrm km, where rs is the schwarzschild radius and m is the mass of the sun.162: 124 for a black hole with nonzero spin or electric charge, the radius is smaller,note 1 until an extremal black hole could have an event horizon close to displaystyle rmathrm frac gmc2, half the radius of a nonspinning, uncharged black hole of the same mass.163 structure a circular black hole (centered) with its anatomy labelled: a singularity in the center of the black hole, a photon sphere at the edge of the black hole, an orange-yellow-blue accretion disk circling the black hole, an innermost stable circular orbit in the inner part of the accretion disk, and a relativistic jet coming out from the black holes poles perpendicularly to the disk. an artistic depiction of a black hole and its features while black holes are conceptually invisible sinks of all matter and light, in astronomical settings, their enormous gravity alters the motion of surrounding objects and pulls nearby gas inwards at near-light speed, making the area around black holes the brightest objects in the universe.164 external geometry relativistic jets see also: astrophysical jet see caption. relativistic jets from the supermassive black hole in centaurus a extend perpendicularly from the galaxy. some black holes have relativistic jetsthin streams of plasma travelling away from the black hole at more than one-tenth of the speed of light.165 a small faction of the matter falling towards the black hole gets accelerated away along the hole rotation axis.166 these jets can extend as far as millions of parsecs from the black hole itself.167 black holes of any mass can have jets.168 however, they are typically observed around spinning black holes with strongly-magnetized accretion disks.169170 relativistic jets were more common in the early universe, when galaxies and their corresponding supermassive black holes were rapidly gaining mass.169171 all black holes with jets also have an accretion disk, but the jets are usually brighter than the disk.165172 supermassive black holes with jets are often called quasars or even macroquasars and are observed in other galaxies. stellar-mass objects with jets, typically observed in the milky way, are referred to as microquasars.173174 the mechanism of formation of jets is not yet known,168 but"
  },
  {
    "chunk_id": 114,
    "doc_id": "Black_hole.txt",
    "text": "are usually brighter than the disk.165172 supermassive black holes with jets are often called quasars or even macroquasars and are observed in other galaxies. stellar-mass objects with jets, typically observed in the milky way, are referred to as microquasars.173174 the mechanism of formation of jets is not yet known,168 but several options have been proposed. one method proposed to fuel these jets is the blandford-znajek process, which suggests that the dragging of magnetic field lines by a black holes rotation could launch jets of matter into space.175176 the penrose process, which involves extraction of a black holes rotational energy, has also been proposed as a potential mechanism of jet propulsion.177178 accretion disk main article: accretion disk see caption. visualization of a black hole with an orange accretion disk. the parts of the disk circling over and under the hole are actually gravitationally lensed from the back side of the black hole.179180 due to conservation of angular momentum, gas falling into the gravitational well created by a massive object will typically form a disk-like structure around the object.181: 242 as the disks angular momentum is transferred outward due to internal processes, its matter falls farther inward, converting its gravitational energy into heat and releasing a large flux of x-rays.182183184185 the temperature of these disks can range from thousands to millions of kelvin, and temperatures can differ throughout a single accretion disk.186187 accretion disks can also emit in other parts of the electromagnetic spectrum, depending on the disks turbulence and magnetization and the black holes mass and angular momentum.185188189 accretion disks can be defined as geometrically thin or geometrically thick. geometrically thin disks are mostly confined to the black holes equatorial plane and have a well-defined edge at the innermost stable circular orbit (isco), while geometrically thick disks are supported by internal pressure and temperature and can extend inside the isco. disks with high rates of electron scattering and absorption, appearing bright and opaque, are called optically thick; optically thin disks are more translucent and produce fainter images when viewed from afar.190 accretion disks of black holes accreting beyond the eddington limit are often referred to as polish donuts due to their thick, toroidal shape that resembles that of a donut.191192193 quasar accretion disks are expected to usually appear blue in color.194 the disk for a stellar black hole, on the other hand, would likely look orange, yellow, or red, with its"
  },
  {
    "chunk_id": 115,
    "doc_id": "Black_hole.txt",
    "text": "are often referred to as polish donuts due to their thick, toroidal shape that resembles that of a donut.191192193 quasar accretion disks are expected to usually appear blue in color.194 the disk for a stellar black hole, on the other hand, would likely look orange, yellow, or red, with its inner regions being the brightest.195 theoretical research suggests that the hotter a disk is, the bluer it should be, although this is not always supported by observations of real astronomical objects.196 accretion disk colors may also be altered by the doppler effect, with the part of the disk travelling towards an observer appearing bluer and brighter and the part of the disk travelling away from the observer appearing redder and dimmer.197198199 innermost stable circular orbit (isco) main article: innermost stable circular orbit artistic depiction of three black holes, showing that the accretion disk is closer to the black hole if it is orbiting in the same direction that the black hole is rotating. graphs next to these images show changes in the x-ray spectra released by the disks. since particles in a black holes accretion disk must orbit at or outside the isco, astronomers can observe the properties of accretion disks to determine black hole spins.200 in newtonian gravity, test particles can stably orbit at arbitrary distances from a central object. in general relativity, however, there exists a smallest possible radius for which a massive particle can orbit stably. any infinitesimal inward perturbations to this orbit will lead to the particle spiraling into the black hole, and any outward perturbations will, depending on the energy, cause the particle to spiral in, move to a stable orbit further from the black hole, or escape to infinity. this orbit is called the innermost stable circular orbit, or isco.201202 the location of the isco depends on the spin of the black hole and the spin of the particle itself. in the case of a schwarzschild black hole (spin zero) and a particle without spin, the location of the isco is: displaystyle rrm isco3,rtextsfrac 6,gmc2, where displaystyle rrm isco is the radius of the isco, displaystyle rtexts is the schwarzschild radius of the black hole, displaystyle g is the gravitational constant, and displaystyle c is the speed of light.203 the radius of this orbit changes slightly based on particle spin.204205 for charged black holes, the isco moves inwards.204 for spinning black holes, the isco"
  },
  {
    "chunk_id": 116,
    "doc_id": "Black_hole.txt",
    "text": "isco, displaystyle rtexts is the schwarzschild radius of the black hole, displaystyle g is the gravitational constant, and displaystyle c is the speed of light.203 the radius of this orbit changes slightly based on particle spin.204205 for charged black holes, the isco moves inwards.204 for spinning black holes, the isco is moved inwards for particles orbiting in the same direction that the black hole is spinning (prograde) and outwards for particles orbiting in the opposite direction (retrograde).202 for example, the isco for a particle orbiting retrograde can be as far out as about displaystyle 9rtexts, while the isco for a particle orbiting prograde can be as close as at the event horizon itself.202206 photon sphere and shadow main article: photon sphere video of a photon being captured by a schwarzschild black hole the photon sphere is a spherical boundary where photons that move on tangents to that sphere are bent completely around the black hole, possibly orbiting multiple times.207 light rays with impact parameters less than the radius of the photon sphere enter the black hole.208 for schwarzschild black holes, the photon sphere has a radius 1.5 times the schwarzschild radius; the radius for non-schwarzschild black holes is at least 1.5 times the radius of the event horizon.209210 the photon sphere viewed from a great distance creates an observable black hole shadow.209 since no light emerges from with the black hole, this shadow is the limit for possible observations.211: 152 the shadow of colliding black holes should have characteristic shapes allowing their possible identification.212 while light can still escape from the photon sphere, any light that crosses the photon sphere on an inbound trajectory will be captured by the black hole. therefore, any light that reaches an outside observer from the photon sphere must have been emitted by objects between the photon sphere and the event horizon.212 light emitted towards the photon sphere would curve around the black hole and return to the observer.213 for a rotating, uncharged black hole, the radius of the photon sphere depends on the spin parameter and whether the photon is orbiting prograde or retrograde.203 for a photon orbiting prograde, the photon sphere will be 1-3 schwarzschild radii from the center of the black hole, while for a photon orbiting retrograde, the photon sphere will be between 3-5 schwarzschild radii from the center of the black hole. the exact location of the photon sphere depends"
  },
  {
    "chunk_id": 117,
    "doc_id": "Black_hole.txt",
    "text": "for a photon orbiting prograde, the photon sphere will be 1-3 schwarzschild radii from the center of the black hole, while for a photon orbiting retrograde, the photon sphere will be between 3-5 schwarzschild radii from the center of the black hole. the exact location of the photon sphere depends on the magnitude of the black holes rotation.214 for a charged, nonrotating black hole, there will only be one photon sphere, and the radius of the photon sphere will decrease for increasing black hole charge.215 for non-extremal, charged, rotating black holes, there will always be two photon spheres, with the exact radii depending on the parameters of the black hole.216 ergosphere main article: ergosphere the ergosphere as a peanut-shaped region that touches the event horizon in the middle and then bulges outwards at the poles. the black hole depicted has a spin 99 of the maximum. the ergosphere is a region outside of the event horizon, where objects cannot remain in place.217 near a rotating black hole, spacetime rotates similar to a vortex. the rotating spacetime will drag any matter and light into rotation around the spinning black hole. this effect of general relativity, called frame dragging, gets stronger closer to the spinning mass. the region of spacetime in which it is impossible to stay still is called the ergosphere.218 this is because frame dragging is so strong near the event horizon that the space around the black hole is moving faster than the speed of light.139: ch. 6.6 the ergosphere of a black hole is a volume bounded by the black holes event horizon and the ergosurface, which coincides with the event horizon at the poles but bulges out from it around the equator.217 matter and radiation can escape from the ergosphere. through the penrose process, objects can emerge from the ergosphere with more energy than they entered with. the extra energy is taken from the rotational energy of the black hole, slowing down the rotation of the black hole.139: ch. 6.7 a variation of the penrose process in the presence of strong magnetic fields, the blandfordznajek process, is considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.175219 plunging region main article: plunging region the observable region of spacetime around a black hole closest to its event horizon is called the plunging region. in this area it is no longer"
  },
  {
    "chunk_id": 118,
    "doc_id": "Black_hole.txt",
    "text": "considered a likely mechanism for the enormous luminosity and relativistic jets of quasars and other active galactic nuclei.175219 plunging region main article: plunging region the observable region of spacetime around a black hole closest to its event horizon is called the plunging region. in this area it is no longer possible for free falling matter to follow circular orbits or stop a final descent into the black hole. instead, it will rapidly plunge toward the black hole at close to the speed of light, growing increasingly hot and producing a characteristic, detectable thermal emission.220221222 however, light and radiation emitted from this region can still escape from the black holes gravitational pull.223 event horizon main article: event horizon a light cone near the event horizon of a black hole with arrows coming out of it to show the directions that a particle there can move. it is far enough away from the event horizon that that it is not warped towards the event horizon and a particle there can move in any direction. far away from the black hole, a particle can move in any direction, as illustrated by the set of arrows. it is restricted only by the speed of light. a light cone very close to the event horizon of a black hole, with arrows coming out of it to show the directions that a particle there can move. it is bent towards the event horizon, with more arrows going towards the event horizon than away, but a particle can still travel away from the event horizon. closer to the black hole, spacetime starts to deform. there are more paths going towards the black hole than paths moving away.note 2 a light cone inside of the event horizon of a black hole, with arrows coming out of it to show the directions that a particle there can move. all of the arrows lead further inside the event horizon, with none leading outwards. inside of the event horizon, all paths bring the particle closer to the centre of the black hole. it is no longer possible for the particle to escape. the defining feature of a black hole is the existence of an event horizon, a boundary in spacetime through which matter and light can pass only inward towards the center of the black hole. nothing, not even light, can escape from inside the event horizon.224225 the event horizon is"
  },
  {
    "chunk_id": 119,
    "doc_id": "Black_hole.txt",
    "text": "escape. the defining feature of a black hole is the existence of an event horizon, a boundary in spacetime through which matter and light can pass only inward towards the center of the black hole. nothing, not even light, can escape from inside the event horizon.224225 the event horizon is referred to as such because if an event occurs within the boundary, information from that event cannot reach or affect an outside observer, making it impossible to determine whether such an event occurred.226: 179 as predicted by general relativity, the presence of a mass deforms spacetime in such a way that the paths taken by particles bend towards the mass.139: ch. 5.4 and 7.3 at the event horizon of a black hole, this deformation becomes so strong that there are no paths that lead away from the black hole.227 to a distant observer, a clock near a black hole would appear to tick more slowly than one further from the black hole.139: 217 this effect, known as gravitational time dilation, would also cause an object falling into a black hole to appear to slow as it approached the event horizon, never quite reaching the horizon from the perspective of an outside observer.139: 218 all processes on this object would appear to slow down, and any light emitted by the object to appear redder and dimmer, an effect known as gravitational redshift.228 an object falling from 12 of a schwarzschild radius above the event horizon would fade away until it could no longer be seen, disappearing from view within one hundredth of a second.229 it would also appear to flatten onto the black hole, joining all other material that had ever fallen into the hole.230 on the other hand, an observer falling into a black hole would not notice any of these effects as they cross the event horizon. their own clocks appear to them to tick normally, and they cross the event horizon after a finite time without noting any singular behaviour. in general relativity, it is impossible to determine the location of the event horizon from local observations, due to einsteins equivalence principle.139: 222 231 for non-rotating black holes, the geometry of the event horizon is precisely spherical, while for rotating black holes, the event horizon is oblate.232233234 internal geometry cauchy horizon main article: cauchy horizon black holes that are rotating andor charged have an inner horizon, often called"
  },
  {
    "chunk_id": 120,
    "doc_id": "Black_hole.txt",
    "text": "to einsteins equivalence principle.139: 222 231 for non-rotating black holes, the geometry of the event horizon is precisely spherical, while for rotating black holes, the event horizon is oblate.232233234 internal geometry cauchy horizon main article: cauchy horizon black holes that are rotating andor charged have an inner horizon, often called the cauchy horizon, inside of the black hole.235236 the inner horizon is divided up into two segments: an ingoing section and an outgoing section.237 at the ingoing section of the cauchy horizon, radiation and matter that fall into the black hole would build up at the horizon, causing the curvature of spacetime to go to infinity. this would cause an observer falling in to experience tidal forces.235236237 this phenomenon is often called mass inflation, since it is associated with a parameter dictating the black holes internal mass growing exponentially,236238 and the buildup of tidal forces is called the mass-inflation singularity239237 or cauchy horizon singularity.240241 some physicists have argued that in realistic black holes, accretion and hawking radiation would stop mass inflation from occurring.242243 at the outgoing section of the inner horizon, infalling radiation would backscatter off of the black holes spacetime curvature and travel outward, building up at the outgoing cauchy horizon. this would cause an infalling observer to experience a gravitational shock wave and tidal forces as the spacetime curvature at the horizon grew to infinity. this buildup of tidal forces is called the shock singularity.238237 both of these singularities are weak, meaning that an object crossing them would only be deformed a finite amount by tidal forces, even though the spacetime curvature would still be infinite at the singularity. this is as opposed to a strong singularity, where an object hitting the singularity would be stretched and squeezed by an infinite amount.235239238 they are also null singularities, meaning that a photon could travel parallel to the them without ever being intercepted.237 singularity main article: gravitational singularity mathematical models of black holes based on general relativities have singularities at their centerspoints where the curvature of spacetime becomes infinite, and geodesics terminate within a finite proper time. however, it is unknown whether these singularities truly exist in real black holes.244 some physicists believe that singularities do not exist, and that their existence, which would make spacetime unpredictable, signals a breakdown of general relativity and a need for a more complete understanding of quantum gravity.245246247 others believe that such singularities could"
  },
  {
    "chunk_id": 121,
    "doc_id": "Black_hole.txt",
    "text": "unknown whether these singularities truly exist in real black holes.244 some physicists believe that singularities do not exist, and that their existence, which would make spacetime unpredictable, signals a breakdown of general relativity and a need for a more complete understanding of quantum gravity.245246247 others believe that such singularities could be resolved within the current framework of physics, without having to introduce quantum gravity.244 there are also physicists, including kip thorne197 and charles misner,248 who believe that not all singularities can be resolved, and that some likely still exist in the real universe despite the effects of quantum gravity.244249 finally, still others believe that singularities do not exist, and that their existence in general relativity does not matter, since general relativity is already believed to be an incomplete theory.244 according to general relativity, every black hole has a singularity inside.139: 205 250 for a non-rotating black hole, this region takes the shape of a single point; for a rotating black hole it is smeared out to form a ring singularity that lies in the plane of rotation.139: 264 in both cases, the singular region has zero volume. all of the mass of the black hole ends up in the singularity.139: 252 since the singularity has nonzero mass in an infinitely small space, it can be thought of as having infinite density.251 chaotic oscillations of spacetime experienced by an object approaching a gravitational singularity observers falling into a schwarzschild black hole (i.e., non-rotating and not charged) cannot avoid being carried into the singularity once they cross the event horizon.252253 as they fall further into the black hole, they will be torn apart by the growing tidal forces in a process sometimes referred to as spaghettification or the noodle effect. eventually, they will reach the singularity and be crushed into an infinitely small point.226: 182 before the 1970s, most physicists believed that the interior of a schwarzschild black hole curved inwards towards a sharp point at the singularity. however, in the late 1960s, soviet physicists vladimir belinskii, isaak khalatnikov, and evgeny lifshitz discovered that this model was only true when the spacetime inside the black hole had not been perturbed. any perturbations, such as those caused by matter or radiation falling in, would cause space to oscillate chaotically near the singularity. any matter falling in would experience intense tidal forces rapidly changing in direction, all while being compressed into an increasingly small"
  },
  {
    "chunk_id": 122,
    "doc_id": "Black_hole.txt",
    "text": "inside the black hole had not been perturbed. any perturbations, such as those caused by matter or radiation falling in, would cause space to oscillate chaotically near the singularity. any matter falling in would experience intense tidal forces rapidly changing in direction, all while being compressed into an increasingly small volume. physicists termed these oscillations mixmaster dynamics, after a brand of mixer that was popular at the time that belinskii, khalatnikov, and lifshitz made their discovery, because they have a similar effect on matter near a singularity as an electric mixer would have on dough.254197255 in the case of a charged (reissnernordstrm) or rotating (kerr) black hole, it is possible to avoid the singularity. extending these solutions as far as possible reveals the hypothetical possibility of exiting the black hole into a different spacetime with the black hole acting as a wormhole.139: 257 the possibility of travelling to another universe is, however, only theoretical, since any perturbation would destroy this possibility.256 it also appears to be possible to follow closed timelike curves (returning to ones own past) around the kerr singularity, which leads to problems with causality like the grandfather paradox.139: 266 257 however, processes inside the black hole, such as quantum gravity effects or mass inflation, might prevent closed timelike curves from arising.257 to solve technical issues with general relativity, some models of gravity do not include black hole singularities. these theoretical black holes without singularities are called regular, or nonsingular, black holes.258259 for example, the fuzzball model, based on string theory, states that black holes are actually made up of quantum microstates and need not have a singularity or an event horizon.260261 the theory of loop quantum gravity proposes that the curvature and density at the center of a black hole is large, but not infinite.262 formation black holes are formed by gravitational collapse of massive stars, either by direct collapse or during a supernova explosion in a process called fallback.263 black holes can result from the merger of two neutron stars or a neutron star and a black hole.264 other more speculative mechanisms include primordial black holes created from density fluctuations in the early universe, the collapse of dark stars, a hypothetical object powered by annihilation of dark matter, or from hypothetical self-interacting dark matter.265 supernova a blurry image of black space illuminated by white blobby stars. three different observations of a gas cloud from different years"
  },
  {
    "chunk_id": 123,
    "doc_id": "Black_hole.txt",
    "text": "created from density fluctuations in the early universe, the collapse of dark stars, a hypothetical object powered by annihilation of dark matter, or from hypothetical self-interacting dark matter.265 supernova a blurry image of black space illuminated by white blobby stars. three different observations of a gas cloud from different years can be seen, showing the cloud moving and deforming in shape. gas cloud being ripped apart by black hole at the centre of the milky way (observations from 2006, 2010 and 2013 are shown in blue, green and red, respectively)266 gravitational collapse occurs when an objects internal pressure is insufficient to resist the objects own gravity. at the end of a stars life, it will run out of hydrogen to fuse, and will start fusing more and more massive elements, until it gets to iron. since the fusion of elements heavier than iron would require more energy than it would release, nuclear fusion ceases. if the iron core of the star is too massive, the star will no longer be able to support itself and will undergo gravitational collapse.267268 while most of the energy released during gravitational collapse is emitted very quickly, an outside observer does not actually see the end of this process. even though the collapse takes a finite amount of time from the reference frame of infalling matter, a distant observer would see the infalling material slow and halt just above the event horizon, due to gravitational time dilation. light from the collapsing material takes longer and longer to reach the observer, with the delay growing to infinity as the emitting material reaches the event horizon. thus the external observer never sees the formation of the event horizon; instead, the collapsing material seems to become dimmer and increasingly red-shifted, eventually fading away.269 other mechanisms observations of quasars at redshift displaystyle zsim 7, less than a billion years after the big bang,270271 has led to investigations of other ways to form black holes. the accretion process to build supermassive black holes has a limiting rate of mass accumulation and a billion years is not enough time to reach quasar status. one suggestion is direct collapse of nearly pure hydrogen gas (low metalicity) clouds characteristic of the young universe, forming a supermassive star which collapses into a black hole. it has been suggested that seed black holes with typical masses of 105 m could have formed in this way"
  },
  {
    "chunk_id": 124,
    "doc_id": "Black_hole.txt",
    "text": "quasar status. one suggestion is direct collapse of nearly pure hydrogen gas (low metalicity) clouds characteristic of the young universe, forming a supermassive star which collapses into a black hole. it has been suggested that seed black holes with typical masses of 105 m could have formed in this way which then could grow to 109 m. however, the very large amount of gas required for direct collapse is not typically stable to fragmentation to form multiple stars. thus another approach suggests massive star formation followed by collisions that seed massive black holes which ultimately merge to create a quasar.272: 85 a neutron star in a common envelope with a regular star can accrete sufficient material to collapse to a black hole or two neutron stars can merge. these avenues for the formation ob black holes are considered relatively rare.273 primordial black holes and the big bang in the current epoch of the universe, conditions needed to form black holes are rare and are mostly only found in stars. however, in the early universe, conditions may have allowed for black hole formations via other means. fluctuations of spacetime soon after the big bang may have formed areas that were denser then their surroundings. initially, these regions would not have been compact enough to form a black hole, but eventually, the curvature of spacetime in the regions become large enough to cause them to collapse into a black hole.274275 different models for the early universe vary widely in their predictions of the scale of these fluctuations. various models predict the creation of primordial black holes ranging from a planck mass (2.2108 kg) to hundreds of thousands of solar masses.276277 primordial black holes with masses less than 1015 g would have evaporated by now due to hawking radiation.130 despite the early universe being extremely dense, it did not re-collapse into a black hole during the big bang, since the universe was expanding rapidly and did not have the gravitational differential necessary for black hole formation. models for the gravitational collapse of objects of relatively constant size, such as stars, do not necessarily apply in the same way to rapidly expanding space such as the big bang.278279280 high-energy collisions in principle, black holes could be formed in high-energy particle collisions that achieve sufficient density, although no such events have been detected.281282 these hypothetical micro black holes, which could form from the collision of"
  },
  {
    "chunk_id": 125,
    "doc_id": "Black_hole.txt",
    "text": "apply in the same way to rapidly expanding space such as the big bang.278279280 high-energy collisions in principle, black holes could be formed in high-energy particle collisions that achieve sufficient density, although no such events have been detected.281282 these hypothetical micro black holes, which could form from the collision of cosmic rays and earths atmosphere or in particle accelerators like the large hadron collider, would not be able to aggregate additional mass.283 instead, they would evaporate in about 1025 seconds, posing no threat to the earth.284 evolution growth simulation of two black holes colliding once a black hole has formed, it can continue to grow by absorbing additional matter. any black hole will continually absorb gas and interstellar dust from its surroundings. this growth process is one possible way through which some supermassive black holes may have been formed, although the formation of supermassive black holes is still an open field of research.156 a similar process has been suggested for the formation of intermediate-mass black holes found in globular clusters.285 black holes can also merge with other objects such as stars or even other black holes. this is thought to have been important, especially in the early growth of supermassive black holes, which could have formed from the aggregation of many smaller objects.156 the process has also been proposed as the origin of some intermediate-mass black holes.286287 restrictions have been proposed to limit the growth rate of black holes. in theory, at a certain rate of accretion, the outward radiation pressure will become as strong as the inward gravitational force, and the black hole will be unable to accrete any faster. this limit is called the eddington limit. however, in practicality, many black holes accrete beyond this rate due to their non-spherical geometry or instabilities in the accretion disk. accretion beyond the limit is called super-eddington accretion and may have been commonplace in the early universe.288289290 additionally, mergers of supermassive black holes may take a long time: as a binary of supermassive black holes approach each other, most nearby stars are ejected, leaving little for the remaining black holes to gravitationally interact with that would allow them to get closer to each other. this phenomenon has been called the final parsec problem, as the distance at which this happens is usually around one parsec.291292 accretion of matter see also: accretion disk an image, all blue, of centaurus as active galactic"
  },
  {
    "chunk_id": 126,
    "doc_id": "Black_hole.txt",
    "text": "gravitationally interact with that would allow them to get closer to each other. this phenomenon has been called the final parsec problem, as the distance at which this happens is usually around one parsec.291292 accretion of matter see also: accretion disk an image, all blue, of centaurus as active galactic nucleus as a bright spot in the center with a bright relativistic jet going away from it. the active galactic nucleus of galaxy centaurus a in x-ray light, believed to be powered by a supermassive black hole (centre) and surrounded by x-ray binaries (blue dots). a star becomes torn apart as it approaches a black hole. below, images of blue and orange dots show observations from the chandra x-ray telescope and the european southern observatory respectively. an artists impression (top) of a supermassive black hole tidally deforming a star based on observations from the chandra x-ray observatory and the european southern observatory. when a black hole accretes matter, the gas in the inner accretion disk orbits at very high speeds because of its proximity to the black hole. the resulting friction is so significant that it heats the inner disk to temperatures at which it emits vast amounts of electromagnetic radiation (mainly x-rays). these bright x-ray sources may be detected by telescopes. by the time the matter of the disk reaches the isco, it will have given off a significant amount of energy: between 5.7 and 42 of its mass will have been converted to energy, depending on the black holes spin. most of this energy (about 90) is released in a relatively small area, within about 20 black hole radii.182 in many cases, accretion disks are accompanied by relativistic jets that are emitted along the black holes poles, which carry away much of the energy. the mechanism for the creation of these jets is currently not well understood, in part due to insufficient data.293 as such, many of the universes most energetic phenomena have been attributed to the accretion of matter on black holes. in particular, active galactic nuclei and quasars are believed to be the accretion disks of supermassive black holes.294 similarly, x-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion.294 it has also been suggested that some ultraluminous x-ray sources may be the accretion disks of intermediate-mass black holes.295 stars"
  },
  {
    "chunk_id": 127,
    "doc_id": "Black_hole.txt",
    "text": "supermassive black holes.294 similarly, x-ray binaries are generally accepted to be binary star systems in which one of the two stars is a compact object accreting matter from its companion.294 it has also been suggested that some ultraluminous x-ray sources may be the accretion disks of intermediate-mass black holes.295 stars have been observed to get torn apart by tidal forces in the immediate vicinity of supermassive black holes in galaxy nuclei, in what is known as a tidal disruption event (tde). some of the material from the disrupted star forms an accretion disk around the black hole, which emits observable electromagnetic radiation.296297298 evaporation main article: hawking radiation in 1974, stephen hawking predicted that black holes emit small amounts of thermal radiation at a temperature of displaystyle frac hbar c38pi gmkb, where displaystyle hbar is the reduced planck constant, displaystyle c is the speed of light, displaystyle g is the gravitational constant, displaystyle m is the mass of the black hole and displaystyle kb is the boltzmann constant.53 this effect has become known as hawking radiation. by applying quantum field theory to black holes, hawking determined that a black hole should continuously emit thermal blackbody radiation. this theory was supported by previous work by jacob bekenstein, who theorized that black holes should have a finite entropy proportional to their surface area, and therefore should also have a temperature.299 it is also analogous to a special relativistic effect called the unruh effect, which predicts that an accelerating observer should detect a higher temperature of their surroundings than a nonaccelerating observer. the temperature experienced by an accelerating observer is the same as an observer near the horizon of a black hole with an equivalent surface gravity. this result is in accordance with the equivalence principle, which states that the effects of acceleration in flat spacetime should be the same as the effects of the equivalent acceleration due to gravity in curved spacetime.300 since hawkings publication, many others have mathematically verified the result through different approaches.299 if hawkings theory of black hole radiation is correct, then black holes are expected to shrink and evaporate over time as they lose mass by the emission of photons and other particles.53 the temperature of this thermal spectrum (hawking temperature) is proportional to the surface gravity of the black hole, which is inversely proportional to the mass. hence, large black holes emit less radiation than small black holes.139:"
  },
  {
    "chunk_id": 128,
    "doc_id": "Black_hole.txt",
    "text": "over time as they lose mass by the emission of photons and other particles.53 the temperature of this thermal spectrum (hawking temperature) is proportional to the surface gravity of the black hole, which is inversely proportional to the mass. hence, large black holes emit less radiation than small black holes.139: ch. 9.6 301 a stellar black hole of 1 m has a hawking temperature of 62 nanokelvins.302 this is far less than the 2.7 k temperature of the cosmic microwave background radiation. stellar-mass or larger black holes receive more mass from the cosmic microwave background than they emit through hawking radiation and thus will grow instead of shrinking.303 to have a hawking temperature larger than 2.7 k (and be able to evaporate), a black hole would need a mass less than the moon. such a black hole would have a diameter of less than a tenth of a millimetre.304 if a black hole is very small, the radiation effects are expected to become very strong. a black hole with the mass of a car would have a diameter of about 1024 m and take a nanosecond to evaporate, during which time it would briefly have a luminosity of more than 200 times that of the sun. lower-mass black holes are expected to evaporate even faster. for a very small black hole, quantum gravity effects are expected to play an important role and could hypothetically make the black hole stable, although current developments in quantum gravity do not indicate this is the case.305306 the hawking radiation for an astrophysical black hole is predicted to be very weak and would thus be exceedingly difficult to detect from earth. a possible exception is the burst of gamma rays emitted in the last stage of the evaporation of primordial black holes. searches for such flashes have proven unsuccessful and provide stringent limits on the possibility of existence of low mass primordial black holes, with modern research predicting that primordial black holes must make up less than a fraction of 107 of the universes total mass.307130 nasas fermi gamma-ray space telescope, launched in 2008, has searched for these flashes, but has not yet found any.308309 if black holes evaporate via hawking radiation, a non-accreting solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064 years.310 a supermassive black"
  },
  {
    "chunk_id": 129,
    "doc_id": "Black_hole.txt",
    "text": "searched for these flashes, but has not yet found any.308309 if black holes evaporate via hawking radiation, a non-accreting solar mass black hole will evaporate (beginning once the temperature of the cosmic microwave background drops below that of the black hole) over a period of 1064 years.310 a supermassive black hole with a mass of 1011 m will evaporate in around 210100 years.311 during the collapse of a supercluster of galaxies, supermassive black holes are predicted to grow to up to 1014 m. even these would evaporate over a timescale of up to 10106 years.310 it is unknown exactly what would happen at the end of a black holes evaporation. some physicists theorize that it would leave behind a remnant, such as a naked singularity.312313314 observational evidence millions of black holes with around 30 solar masses derived from stellar collapse are expected to exist in the milky way. even a dwarf galaxy like draco should have hundreds.315 only a few of these have been detected. by nature, black holes do not themselves emit any electromagnetic radiation other than the hypothetical hawking radiation, so astrophysicists searching for black holes must generally rely on indirect observations. the defining characteristic of a black hole is its event horizon. the horizon itself cannot be imaged316 so all other possible explanations for these indirect observations must be considered and eliminated before concluding that a black hole has been observed.317: 11 direct interferometry an orange donut of gases surrounding a black hole, with lines superimposed over the gases an m87 image with superimposed lines representing the magnitude and direction of polarization. m87 with a long relativistic jet extending from it. the event horizon telescope image of the black hole is inset. the m87 relativistic jet; inset is the black hole shadow. the event horizon telescope (eht) is a global system of radio telescopes capable of directly observing a black hole shadow.79 in april 2017, eht began observing the black hole at the centre of messier 87.318319 using petabytes of data from eight different radio observatories over a ten-day observation period, the eht team created a composite image of the black hole, which they debuted in april 2019.320321322323 the black holes shadow appears as a dark circle in the centre of the image, bordered by the orange-red ring of its accretion disk.324 the bottom half of the disk is brighter than the top due to doppler beaming:"
  },
  {
    "chunk_id": 130,
    "doc_id": "Black_hole.txt",
    "text": "image of the black hole, which they debuted in april 2019.320321322323 the black holes shadow appears as a dark circle in the centre of the image, bordered by the orange-red ring of its accretion disk.324 the bottom half of the disk is brighter than the top due to doppler beaming: material at the bottom of the disk, which is travelling towards the viewer at relativistic speeds, appears brighter than the material at the top of the disk, which is travelling away from the viewer.325324 in april 2023, the eht team presented an image of the shadow of the messier 87 black hole and its high-energy jet, viewed together for the first time.326327 an orange disc of gases surrounding a black hole, with bright spots visible within the disk sagittarius a, the supermassive black hole in the center of the milky way on 12 may 2022, the eht released the first image of sagittarius a, the supermassive black hole at the centre of the milky way galaxy. the eht team had previously detected magnetic field lines around the black hole, confirming theoretical predictions of magnetic fields around black holes.328329 the imaging of sagittarius a was done concurrently with the imaging of the messier 87 (m87) black hole. like m87, sagittarius as shadow and accretion disk can be seen in the eht image, with the size of the shadow matching theoretical projections.321330 although the image of sagittarius a was created through the same process as for m87, it was significantly more complex to image sagittarius a because of the instability of its surroundings. because sagittarius a is one thousand times less massive as m87, its accretion disk has a much shorter orbital period, so the environment around sagittarius a was rapidly changing as the eht team was trying to image it.331 additionally, turbulent plasma lies between sagittarius a and earth, preventing resolution of the image at longer wavelengths.332 detection of gravitational waves from merging black holes see caption ligo measurement of the gravitational waves at the livingston (right) and hanford (left) detectors, compared with the theoretical predicted values on 14 september 2015, the ligo gravitational wave observatory made the first-ever successful direct observation of gravitational waves.73333 the signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses.73334 because the two objects"
  },
  {
    "chunk_id": 131,
    "doc_id": "Black_hole.txt",
    "text": "the ligo gravitational wave observatory made the first-ever successful direct observation of gravitational waves.73333 the signal was consistent with theoretical predictions for the gravitational waves produced by the merger of two black holes: one with about 36 solar masses, and the other around 29 solar masses.73334 because the two objects were only 350 km apart just before the merger, yet were more massive than possible for a neutron star, the ligo team concluded that the gravitational waves must have come from a merger of two black holes.73 the signal observed by ligo also included the start of the post-merger ringdown, the signal produced as the newly formed compact object settles down to a stationary state.335 from the ringdown, the ligo team was able to determine that the resulting merged black hole was spinning at 67 of the maximum rate and had a mass of 62 solar masses, having lost three solar masses as gravitational waves during the merger.73334 the observation also provides the first observational evidence for the existence of stellar-mass black hole binaries. furthermore, it is the first observational evidence of stellar-mass black holes weighing 25 solar masses or more.336 since then, many more gravitational wave events have been observed.337 stars orbiting sagittarius a main article: sagittarius a cluster stars moving around sagittarius a, as seen in 2021 the proper motions of stars near the centre of the milky way provide strong observational evidence that these stars are orbiting a supermassive black hole.338 since 1995, astronomers have tracked the motions of 90 stars orbiting an invisible object coincident with the radio source sagittarius a. in 1998, by fitting the motions of the stars to keplerian orbits, the astronomers were able to infer that sagittarius a must be a 2.6106 m object must be contained within a radius of 0.02 light-years.339 since then, one of the starscalled s2has completed a full orbit. from the orbital data, astronomers were able to refine the calculations of the mass of sagittarius a to 4.3106 m, with a radius of less than 0.002 light-years.338 this upper limit radius is larger than the schwarzschild radius for the estimated mass, so the combination does not prove sagittarius a is a black hole. nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume.339 additionally, there"
  },
  {
    "chunk_id": 132,
    "doc_id": "Black_hole.txt",
    "text": "for the estimated mass, so the combination does not prove sagittarius a is a black hole. nevertheless, these observations strongly suggest that the central object is a supermassive black hole as there are no other plausible scenarios for confining so much invisible mass into such a small volume.339 additionally, there is some observational evidence that this object might possess an event horizon, a feature unique to black holes.340 the event horizon telescope image of sagittarius a, released in 2022, provided further confirmation that it is indeed a black hole.341 binaries see also: x-ray binary black hole cygnus x-1 as seen by the chandra x-ray observatory, a bright spot over a black background. a chandra x-ray observatory image of cygnus x-1, which was the first strong black hole candidate discovered x-ray binaries are binary systems that emit a majority of their radiation in the x-ray part of the electromagnetic spectrum. these x-ray emissions result when a compact object accretes matter from an ordinary star.342 the presence of an ordinary star in such a system provides an opportunity for studying the central object and to determine if it might be a black hole. by measuring the orbital period of the binary, the distance to the binary from earth, and the mass of the companion star, scientists can estimate the mass of the compact object.343 the tolman-oppenheimer-volkoff limit (tov limit) dictates the largest mass a nonrotating neutron star can be, and is estimated to be about two solar masses. while a rotating neutron star can be slightly more massive, if the compact object is much more massive than the tov limit, it cannot be a neutron star and is generally expected to be a black hole.294344 the first strong candidate for a black hole, cygnus x-1, was discovered in this way by charles thomas bolton,9 louise webster, and paul murdin8 in 1972.34551 observations of rotation broadening of the optical star reported in 1986 lead to a compact object mass estimate of 16 solar masses, with 7 solar masses as the lower bound.294 in 2011, this estimate was updated to 14.11.0 m for the black hole and 19.21.9 m for the optical stellar companion.346 x-ray binaries can be categorized as either low-mass or high-mass; this classification is based on the mass of the companion star, not the compact object itself.140 in a class of x-ray binaries called soft x-ray transients, the companion star is"
  },
  {
    "chunk_id": 133,
    "doc_id": "Black_hole.txt",
    "text": "black hole and 19.21.9 m for the optical stellar companion.346 x-ray binaries can be categorized as either low-mass or high-mass; this classification is based on the mass of the companion star, not the compact object itself.140 in a class of x-ray binaries called soft x-ray transients, the companion star is of relatively low mass, allowing for more accurate estimates of the black hole mass. these systems actively emit x-rays for only several months once every 1050 years. during the period of low x-ray emission, called quiescence, the accretion disk is extremely faint, allowing detailed observation of the companion star.294 numerous black hole candidates have been measured by this method.347 black holes are also sometimes found in binaries with other compact objects, such as white dwarfs,140 neutron stars,348349 and other black holes.350351 galactic nuclei the centre of nearly every galaxy contains a supermassive black hole.352 the close observational correlation between the mass of this hole and the velocity dispersion of the host galaxys bulge, known as the msigma relation, strongly suggests a connection between the formation of the black hole and that of the galaxy itself.353354 active galactic nucleus see also: active galactic nucleus the center of the milky way, with an inset x-ray image of sagittarius a detection of unusually bright x-ray flare from sagittarius a, a black hole in the centre of the milky way galaxy on 5 january 2015355 astronomers use the term active galaxy to describe galaxies with unusual characteristics, such as unusual spectral line emission and very strong radio emission. theoretical and observational studies have shown that the high levels of activity in the centers of these galaxies, regions called active galactic nuclei (agn), may be explained by accretion onto supermassive black holes. these agn consist of a central black hole that may be millions or billions of times more massive than the sun, a disk of interstellar gas and dust called an accretion disk, and two jets perpendicular to the accretion disk.356357358 although supermassive black holes are expected to be found in most agn, only some galaxies nuclei have been more carefully studied in attempts to both identify and measure the actual masses of the central supermassive black hole candidates. some of the most notable galaxies with supermassive black hole candidates include the andromeda galaxy, messier 32, messier 87, the sombrero galaxy, and the milky way itself.359360 quasi-periodic oscillations main article: quasi-periodic oscillation the x-ray"
  },
  {
    "chunk_id": 134,
    "doc_id": "Black_hole.txt",
    "text": "to both identify and measure the actual masses of the central supermassive black hole candidates. some of the most notable galaxies with supermassive black hole candidates include the andromeda galaxy, messier 32, messier 87, the sombrero galaxy, and the milky way itself.359360 quasi-periodic oscillations main article: quasi-periodic oscillation the x-ray emissions from the disks of accreting black holes sometimes flicker at certain frequencies. these signals are called quasi-periodic oscillations and are thought to be caused by material moving along the inner edge of the accretion disk (the innermost stable circular orbit).361362 some scientists also suggest that these oscillations may be caused by the black holes axis of rotation being out of alignment with the binary systems axis of rotation.362 since the frequency of quasi-periodic oscillations is correlated with the mass and rotation rate of the compact object, it can be used as an alternative way to determine the properties of candidate black holes.361362363 microlensing a diagram of gravitational microlensing: a foreground black hole warps light from a background star, creating two images of the star. the light then travels to earth-based telescopes, where the two images cannot be resolved and appear solely as a single brightened star. the intense gravitational field of a foreground black hole acts like a powerful lens, distorting and brightening the image of a background star. another way black holes can be detected is through observation of effects caused by their strong gravitational field. one such effect is gravitational lensing: the deformation of spacetime around a massive object causes light rays to be deflected, making objects behind them appear distorted.364 when the lensing object is a black hole, this effect can be strong enough to create multiple images of a star or other luminous source.365 however, the distance between the lensed images may be too small for contemporary telescopes to resolvethis phenomenon is called microlensing.366 instead of seeing two images of a lensed star, astronomers see the star brighten slightly as the black hole moves towards the line of sight between the star and earth and then return to its normal luminosity as the black hole moves away.367 the turn of the millennium saw the first 3 candidate detections of black holes in this way,368369 and in january 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole.370371 this was also the first determination of an isolated black hole mass,"
  },
  {
    "chunk_id": 135,
    "doc_id": "Black_hole.txt",
    "text": "moves away.367 the turn of the millennium saw the first 3 candidate detections of black holes in this way,368369 and in january 2022, astronomers reported the first confirmed detection of a microlensing event from an isolated black hole.370371 this was also the first determination of an isolated black hole mass, 7.11.3 m.370372 alternatives see also: exotic star see caption relative sizes of a neutron star, a quark star and the grand canyon while there is a strong case for supermassive black holes, the model for stellar-mass black holes assumes of an upper limit for the mass of a neutron star: objects observed to have more mass are assumed to be black holes. however, the properties of extremely dense matter are poorly understood. new exotic phases of matter could allow other kinds of massive objects.294 quark stars would be made up of quark matter and supported by quark degeneracy pressure, a form of degeneracy pressure even stronger than neutron degeneracy pressure. this would halt gravitational collapse at a higher mass than for a neutron star.373374375 even stronger stars called electroweak stars would convert quarks in their cores into leptons, providing additional pressure to stop the star from collapsing.374376 if, as some extensions of the standard model posit, quarks and leptons are made up of the even-smaller fundamental particles called preons, a very compact star could be supported by preon degeneracy pressure.377 while none of these hypothetical models can explain all of the observations of stellar black hole candidates, a q star is the only alternative which could significantly exceed the mass limit for neutron stars and thus provide an alternative for supermassive black holes.294: 12 a few theoretical objects have been conjectured to match observations of astronomical black hole candidates identically or near-identically, but which function via a different mechanism.378 a dark energy star would convert infalling matter into vacuum energy; this vacuum energy would be much larger than the vacuum energy of outside space, exerting outwards pressure and preventing a singularity from forming.379380 a black star would be gravitationally collapsing slowly enough that quantum effects would keep it just on the cusp of fully collapsing into a black hole.381 a gravastar would consist of a very thin shell and a dark-energy interior providing outward pressure to stop the collapse into a black hole or formation of a singularity; it could even have another gravastar inside, called a nestar.382383384 a diagram"
  },
  {
    "chunk_id": 136,
    "doc_id": "Black_hole.txt",
    "text": "the cusp of fully collapsing into a black hole.381 a gravastar would consist of a very thin shell and a dark-energy interior providing outward pressure to stop the collapse into a black hole or formation of a singularity; it could even have another gravastar inside, called a nestar.382383384 a diagram showing the properties of a black hole vs a gravastar: both have an event horizon and an exterior governed by the schwarzschild metric, but the interior of the black hole is governed by the schwarzschild metric while the interior of the gravastar is governed by the de sitter metric. additionally, all the mass of the black hole is contained in its singularity, while all the mass of the gravastar is contained in its thin shell. the properties of a black hole versus those of a gravastar since the average density of a black hole inside its schwarzschild radius is inversely proportional to the square of its mass, supermassive black holes are much less dense than stellar black holes. the average density of a 108 m black hole is comparable to that of water.294385 consequently, the physics of matter forming a supermassive black hole is much better understood and the possible alternative explanations for supermassive black hole observations are much more mundane. for example, a supermassive black hole could be modelled by a large cluster of very dark objects. however, such alternatives are typically not stable enough to explain the supermassive black hole candidates.294386387 open questions information loss paradox main article: black hole information paradox further information: firewall (physics) unsolved problem in physics is physical information lost in black holes? more unsolved problems in physics according to the no-hair theorem, a black hole is defined by only three parameters: its mass, charge, and angular momentum. this seems to mean that all other information about the matter that went into forming the black hole is lost, as there is no way to determine anything about the black hole from outside other than those three parameters. when black holes were thought to persist forever, this information loss was not problematic, as the information can be thought of as existing inside the black hole. however, black holes slowly evaporate by emitting hawking radiation. this radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information is seemingly be gone forever. this is called the"
  },
  {
    "chunk_id": 137,
    "doc_id": "Black_hole.txt",
    "text": "can be thought of as existing inside the black hole. however, black holes slowly evaporate by emitting hawking radiation. this radiation does not appear to carry any additional information about the matter that formed the black hole, meaning that this information is seemingly be gone forever. this is called the black hole information paradox.388389390 this paradox pits a foundational principle of quantum mechanics against one of general relativity. according to quantum mechanics, a pure state with no entropy can never evolve into a mixed state with entropy, a consequence of unitarity. all hawking radiation, as thermal radiation, is in a mixed quantum state. over vast amounts of time, the entire black hole is converted to this mixed-state radiation. therefore, any quantum system in a pure state that had fallen into the black hole before it radiated away would seemingly have been converted into mixed-state radiation, violating unitarity. since quantum mechanics would not allow this conversion, some scientists have suggested that the pure state of the quantum system is destroyed at the black hole event horizon. however, this conflicts with the equivalence principle of general relativity, which states that there should be no physical difference between free-falling into a black hole and floating in empty space. according to the principle, since a pure state cannot be destroyed in empty space, it should not be destroyed at the event horizon of a black hole either.390391: 11 theoretical studies analyzing the paradox have led to both further paradoxes and new ideas about the intersection of quantum mechanics and general relativity. while there is no consensus on the resolution of the paradox, work on the problem is expected to be important for a theory of quantum gravity.392: 126 supermassive black holes in the early universe two spiral galaxies, one of which has a very bright center two galaxies from the first billion years after the big bang. the galaxy on the left hosts a luminous quasar at its center observations of faraway galaxies have found that ultraluminous quasars, powered by supermassive black holes, existed in the early universe as far as redshift displaystyle zgeq 7.393394395 these black holes have been assumed to be the products of the gravitational collapse of large population iii stars.396397 however, these stellar remnants were not massive enough to produce the quasars observed at early times without accreting beyond the eddington limit, the theoretical maximum rate of black hole accretion.398399"
  },
  {
    "chunk_id": 138,
    "doc_id": "Black_hole.txt",
    "text": "7.393394395 these black holes have been assumed to be the products of the gravitational collapse of large population iii stars.396397 however, these stellar remnants were not massive enough to produce the quasars observed at early times without accreting beyond the eddington limit, the theoretical maximum rate of black hole accretion.398399 physicists have suggested a variety of different mechanisms by which these supermassive black holes may have formed. it has been proposed that smaller black holes may have also undergone mergers to produce the observed supermassive black holes.400401 it is also possible that they were seeded by direct-collapse black holes, in which a large cloud of hot gas avoids fragmentation that would lead to multiple stars, due to low angular momentum or heating from a nearby galaxy. given the right circumstances, a single supermassive star forms and collapses directly into a black hole without undergoing typical stellar evolution.402403 additionally, these supermassive black holes in the early universe may be high-mass primordial black holes, which could have accreted further matter in the centers of galaxies.404 finally, certain mechanisms allow black holes to grow faster than the theoretical eddington limit, such as dense gas in the accretion disk limiting outward radiation pressure that prevents the black hole from accreting.398405 however, the formation of bipolar jets prevent super-eddington rates.289 in science fiction main article: black holes in fiction the black hole and accretion disk used in the movie interstellar, without lens flare and depicted with a faster spin black holes have been portrayed in science fiction in a variety of ways. even before the advent of the term itself, objects with characteristics of black holes appeared in stories such as the 1928 novel the skylark of space with its black sun and the hole in space in the 1935 short story starship invincible.406 as black holes grew to public recognition in the 1960s and 1970s, they began to be featured in films as well as novels, such as disneys the black hole (1979). black holes have also been used in works of the 21st century, such as christopher nolans science fiction epic interstellar (2014).407408 authors and screenwriters have exploited the relativistic effects of black holes, particularly gravitational time dilation.409 for example, interstellar features a black hole planet with a time dilation factor of over 60,000:1, while the 1977 novel gateway depicts a spaceship approaching but never crossing the event horizon of a black hole"
  },
  {
    "chunk_id": 139,
    "doc_id": "Black_hole.txt",
    "text": "authors and screenwriters have exploited the relativistic effects of black holes, particularly gravitational time dilation.409 for example, interstellar features a black hole planet with a time dilation factor of over 60,000:1, while the 1977 novel gateway depicts a spaceship approaching but never crossing the event horizon of a black hole from the perspective of an outside observer due to time dilation effects.410407197 black holes have also been appropriated as wormholes or other methods of faster-than-light travel, such as in the 1974 novel the forever war, where a network of black holes is used for interstellar travel.409411 additionally, black holes can feature as hazards to spacefarers and planets: a black hole threatens a deep-space outpost in 1978 short story the black hole passes, and a binary black hole dangerously alters the orbit of a planet in the 2018 netflix reboot of lost in space.408411 see also astronomy portal image history of science portal icon mathematics portal icon physics portal icon stars portal black brane or black string black hole initiative black hole starship blanet btz black hole dark star (dark matter) golden binary hypothetical black hole (disambiguation) kugelblitz (astrophysics) list of black holes list of nearest black holes outline of black holes planck star soft hair (black holes) sonic black hole susskind-hawking battle timeline of black hole physics virtual black hole white hole notes the (outer) event horizon radius scales as: displaystyle msqrt m2-(jm)2-q2. the set of possible paths, or more accurately the future light cone containing all possible world lines (in this diagram the light cone is represented by the v-shaped region bounded by arrows representing light ray world lines), is tilted in this way in eddingtonfinkelstein coordinates (the diagram is a cartoon version of an eddingtonfinkelstein coordinate diagram), but in other coordinates the light cones are not tilted in this way, for example in schwarzschild coordinates they narrow without tilting as one approaches the event horizon, and in kruskalszekeres coordinates the light cones do not change shape or orientation at all.201: 848 references the event horizon telescope collaboration; akiyama, kazunori; alberdi, antxon; alef, walter; asada, keiichi; azulay, rebecca; baczko, anne-kathrin; ball, david; balokovi, mislav; barrett, john; bintley, dan; blackburn, lindy; boland, wilfred; bouman, katherine l.; bower, geoffrey c. (10 april 2019). first m87 event horizon telescope results. iv. imaging the central supermassive black hole. the astrophysical journal letters. 875 (1): l4. arxiv:1906.11241. bibcode:2019apj...875l...4e. doi:10.38472041-8213ab0e85. issn 2041-8205. astronomers capture first"
  },
  {
    "chunk_id": 140,
    "doc_id": "Black_hole.txt",
    "text": "anne-kathrin; ball, david; balokovi, mislav; barrett, john; bintley, dan; blackburn, lindy; boland, wilfred; bouman, katherine l.; bower, geoffrey c. (10 april 2019). first m87 event horizon telescope results. iv. imaging the central supermassive black hole. the astrophysical journal letters. 875 (1): l4. arxiv:1906.11241. bibcode:2019apj...875l...4e. doi:10.38472041-8213ab0e85. issn 2041-8205. astronomers capture first image of a black hole. new.nsf.gov. 10 april 2019. retrieved 28 january 2025. riazuelo, alain (2019). seeing relativityi. ray tracing in a schwarzschild metric to explore the maximal analytic extension of the metric and making a proper rendering of the stars. international journal of modern physics d. 28 (2): 1950042. arxiv:1511.06025. bibcode:2019ijmpd..2850042r. doi:10.1142s0218271819500421. s2cid 54548877. overbye, dennis (8 june 2015). black hole hunters. nasa. archived from the original on 9 june 2015. retrieved 8 june 2015. hamilton, a. journey into a schwarzschild black hole. jila.colorado.edu. archived from the original on 3 september 2019. retrieved 28 june 2020. schutz, bernard f. (2003). gravity from the ground up. cambridge university press. p. 110. isbn 978-0-521-45506-0. archived from the original on 2 december 2016. davies, p. c. w. (1978). thermodynamics of black holes (pdf). reports on progress in physics (review). 41 (8): 13131355. bibcode:1978rpph...41.1313d. doi:10.10880034-4885418004. s2cid 250916407. archived from the original (pdf) on 10 may 2013. a b webster, b. louise; murdin, paul (1972), cygnus x-1a spectroscopic binary with a heavy companion?, nature (letter), 235 (5332): 3738, bibcode:1972natur.235...37w, doi:10.1038235037a0, s2cid 4195462 a b bolton, c. t. (1972), identification of cygnus x-1 with hde 226868, nature (letter), 235 (5336): 271273, bibcode:1972natur.235..271b, doi:10.1038235271b0, s2cid 4222070 a b c montgomery, colin; orchiston, wayne; whittingham, ian (2009) available online 18 april 2023. michell, laplace and the origin of the black hole concept (pdf). journal of astronomical history and heritage (research article). 12 (2): 9096. bibcode:2009jahh...12...90m. doi:10.3724sp.j.1440-2807.2009.02.01. s2cid 55890996. michell, j. (1784). on the means of discovering the distance, magnitude, c. of the fixed stars, in consequence of the diminution of the velocity of their light, in case such a diminution should be found to take place in any of them, and such other data should be procured from observations, as would be farther necessary for that purpose. philosophical transactions of the royal society. 74: 3557. bibcode:1784rspt...74...35m. doi:10.1098rstl.1784.0008. jstor 106576. a b c d e f g h i j k l m n o thorne, kip s.; hawking, stephen (1994). agrawal, milan (ed.). black holes and time warps: einsteins outrageous legacy (1st ed.). w. w. norton company."
  },
  {
    "chunk_id": 141,
    "doc_id": "Black_hole.txt",
    "text": "philosophical transactions of the royal society. 74: 3557. bibcode:1784rspt...74...35m. doi:10.1098rstl.1784.0008. jstor 106576. a b c d e f g h i j k l m n o thorne, kip s.; hawking, stephen (1994). agrawal, milan (ed.). black holes and time warps: einsteins outrageous legacy (1st ed.). w. w. norton company. isbn 978-0-393-31276-8. retrieved 12 april 2019. a b c d weinberg, steven (1972). gravitation and cosmology. john wiley sons. isbn 978-0-471-92567-5. einstein, albert (1911), einfluss der schwerkraft auf die ausbreitung des lichtes (on the influence of gravity on the propagation of light), annalen der physik, 35 (10): 898908, bibcode:1911anp...340..898e, doi:10.1002andp.19113401005 (also in collected papers vol. 3, document 23) einstein, albert (1915). feldgleichungen der gravitation field equations of gravitation. preussische akademie der wissenschaften, sitzungsberichte: 844847. janssen, michel; renn, jrgen (2015). arch and scaffold: how einstein found his field equations. physics today (feature article). 68 (11): 3036. bibcode:2015pht....68k..30j. doi:10.1063pt.3.2979. hdl:1185800-001m-0000-002a-8ed7-1. oconnor, j j; robertson, e f (may 1996). general relativity. mactutor. school of mathematics and statistics, university of st andrews, scotland. archived from the original on 12 july 2020. retrieved 22 october 2025. fraknoi, andrew; morrison, david; wolff, sidney c. (2022). 24.5 black holes. astronomy 2e (pdf) (2e ed.). openstax. pp. 839846. isbn 978-1-951693-50-3. oclc 1322188620. a b schwarzschild, k. (1916). ber das gravitationsfeld eines massenpunktes nach der einsteinschen theorie on the gravitational field of a mass point according to einsteins theory. sitzungsberichte der kniglich preussischen akademie der wissenschaften. 7: 189196. bibcode:1916spaw.......189s via internet archive. translation: antoci, s.; loinger, a. (12 may 1999). on the gravitational field of a mass point according to einsteins theory. arxiv:physics9905030. and schwarzschild, k. (1916). ber das gravitationsfeld einer kugel aus inkompressibler flssigkeit nach der einsteinschen theorie. sitzungsberichte der kniglich preussischen akademie der wissenschaften. 18: 424434. bibcode:1916skpa.conf..424s. translation: antoci, s. (1999). on the gravitational field of a sphere of incompressible fluid according to einsteins theory. arxiv:physics9912033. droste, j. (1917). the field of a single centre in einsteins theory of gravitation, and the motion of a particle in that field (pdf). physics. proceedings of the section of sciences. 19 (1). koninklijke akademie van wetenschappen: 197215. bibcode:1917knab...19..197d. archived (pdf) from the original on 18 may 2013. retrieved 16 september 2012. kox, a. j. (1992). general relativity in the netherlands: 19151920. in eisenstaedt, jean; kox, a. j. (eds.). studies in the history of general relativity. birkhuser. p. 41. isbn 978-0-8176-3479-7. archived from the original on 10 august 2016. retrieved 23"
  },
  {
    "chunk_id": 142,
    "doc_id": "Black_hole.txt",
    "text": "from the original on 18 may 2013. retrieved 16 september 2012. kox, a. j. (1992). general relativity in the netherlands: 19151920. in eisenstaedt, jean; kox, a. j. (eds.). studies in the history of general relativity. birkhuser. p. 41. isbn 978-0-8176-3479-7. archived from the original on 10 august 2016. retrieved 23 february 2016. t hooft, g. (2009). introduction to the theory of black holes (pdf). institute for theoretical physics spinoza institute. pp. 4748. archived (pdf) from the original on 21 may 2009. retrieved 24 june 2010. eddington, arthur (1926). the internal constitution of the stars. science. vol. 52. cambridge university press. pp. 23340. bibcode:1920sci....52..233e. doi:10.1126science.52.1341.233. pmid 17747682. archived from the original on 11 august 2016. isbn 978-0-521-33708-3 a b bernstein, jeremy (2007). the reluctant father of black holes. scientific american. vol. 17. pp. 411. doi:10.1038scientificamerican0407-4sp. retrieved 3 august 2023. einstein, albert (10 may 1939). on a stationary system with spherical symmetry consisting of many gravitating masses. annals of mathematics. 40 (4): 922936. doi:10.23071968902. jstor 1968902. detweiler, s. (1981). resource letter bh-1: black holes. american journal of physics (paper). 49 (5): 394400. bibcode:1981amjph..49..394d. doi:10.11191.12686. harpaz, a. (1994). stellar evolution. a k peters. p. 105. isbn 978-1-56881-012-6. archived from the original on 11 august 2016. oppenheimer, j. r.; volkoff, g. m. (1939). on massive neutron cores. physical review. 55 (4): 374381. bibcode:1939phrv...55..374o. doi:10.1103physrev.55.374. a b c d e bartusiak, marcia (2015). black hole: how an idea abandoned by newtonians, hated by einstein, and gambled on by hawking became loved. new haven, ct: yale university press. isbn 978-0-300-21363-8. oppenheimer, j.r.; snyder, h. (1939). on continued gravitational contraction. physical review (highlighted article). 56 (5): 455459. bibcode:1939phrv...56..455o. doi:10.1103physrev.56.455. finkelstein, d. (1958). past-future asymmetry of the gravitational field of a point particle. physical review (article). 110 (4): 965967. bibcode:1958phrv..110..965f. doi:10.1103physrev.110.965. luminet, j.-p. (may 1979). image of a spherical black hole with thin accretion disk. astronomy and astrophysics. 75: 228235. bibcode:1979aa....75..228l. issn 0004-6361. french national centre for scientific research (10 april 2019). first ever image of a black hole: a cnrs researcher had simulated it as early as 1979. cnrs. retrieved 18 june 2025. thorne k (2003). 5. warping spacetime. in shellard es, gibbons gw, rankin sj (eds.). the future of theoretical physics and cosmology: celebrating stephen hawkings 60th birthday. cambridge university press. p. 74. isbn 0-521-82081-2. kerr, r. p. (2009). discovering the kerr and kerr-schild metrics. in wiltshire, d. l.; visser, m.; scott, s. m. (eds.). the"
  },
  {
    "chunk_id": 143,
    "doc_id": "Black_hole.txt",
    "text": "warping spacetime. in shellard es, gibbons gw, rankin sj (eds.). the future of theoretical physics and cosmology: celebrating stephen hawkings 60th birthday. cambridge university press. p. 74. isbn 0-521-82081-2. kerr, r. p. (2009). discovering the kerr and kerr-schild metrics. in wiltshire, d. l.; visser, m.; scott, s. m. (eds.). the kerr spacetime. cambridge university press. arxiv:0706.1109. bibcode:2007arxiv0706.1109k. isbn 978-0-521-88512-6. hooper, dan (27 december 2019). the discovery of black holes: from theory to actuality. wondrium daily. virginia, united states: the teaching company. archived from the original on 25 september 2022. retrieved 29 june 2022. newman et, et al. (1965). metric of a rotating, charged mass. journal of mathematical physics (research article). 6 (6): 918. bibcode:1965jmp.....6..918n. doi:10.10631.1704351. israel, w. (1967). event horizons in static vacuum space-times. physical review (article). 164 (5): 1776. bibcode:1967phrv..164.1776i. doi:10.1103physrev.164.1776. carter, b. (1971). axisymmetric black hole has only two degrees of freedom. elementary particles and fields. physical review letters (letter). 26 (6): 331. bibcode:1971phrvl..26..331c. doi:10.1103physrevlett.26.331. carter, b. (1977). the vacuum black hole uniqueness theorem and its conceivable generalisations. proceedings of the 1st marcel grossmann meeting on general relativity. pp. 243254. a b chruciel pt, costa jl, heusler m (2012). stationary black holes: uniqueness and beyond. living reviews in relativity (article). 15 (7) 7. arxiv:1205.6112. bibcode:2012lrr....15....7c. doi:10.12942lrr-2012-7. pmc 5255892. pmid 28179837. penrose, roger (1996). chandrasekhar, black holes, and singularities. journal of astrophysics and astronomy (article). 17 (34): 213231. bibcode:1996japa...17..213p. doi:10.1007bf02702305. penrose, r. (1965). gravitational collapse and space-time singularities (pdf). physical review letters. 14 (3): 57. bibcode:1965phrvl..14...57p. doi:10.1103physrevlett.14.57. s2cid 116755736. archived (pdf) from the original on 11 october 2020. ford, l. h. (2003). the classical singularity theorems and their quantum loopholes. international journal of theoretical physics. 42 (6): 12191227. arxiv:gr-qc0301045. bibcode:2003gr.qc.....1045f. doi:10.1023a:1025754515197. s2cid 14404560. hawking, stephen (18 october 1966). the occurrence of singularities in cosmology (pdf). proceedings of the royal society a. 294 (1439): 511521. bibcode:1966rspsa.294..511h. doi:10.1098rspa.1966.0221. jstor 2415489 via jstor. hewish, a.; et al. (1968). observation of a rapidly pulsating radio source. nature. 217 (5130): 709713. bibcode:1968natur.217..709h. doi:10.1038217709a0. s2cid 4277613. pilkington, j. d. h.; et al. (1968). observations of some further pulsed radio sources. nature. 218 (5137): 126129. bibcode:1968natur.218..126p. doi:10.1038218126a0. s2cid 4253103. hewish, a. (1970). pulsars. annual review of astronomy and astrophysics. 8 (1): 265296. bibcode:1970araa...8..265h. doi:10.1146annurev.aa.08.090170.001405. boissoneault, lorraine (28 february 2018). fifty years ago, a grad students discovery changed the course of astrophysics. smithsonian magazine. retrieved 22 december 2023. rolston, bruce (10 november 1997), the first black hole,"
  },
  {
    "chunk_id": 144,
    "doc_id": "Black_hole.txt",
    "text": "doi:10.1038218126a0. s2cid 4253103. hewish, a. (1970). pulsars. annual review of astronomy and astrophysics. 8 (1): 265296. bibcode:1970araa...8..265h. doi:10.1146annurev.aa.08.090170.001405. boissoneault, lorraine (28 february 2018). fifty years ago, a grad students discovery changed the course of astrophysics. smithsonian magazine. retrieved 22 december 2023. rolston, bruce (10 november 1997), the first black hole, university of toronto, archived from the original on 7 march 2008, retrieved 11 march 2008 a b shipman, h. l.; yu, z; du, y.w (1975), the implausible history of triple star models for cygnus x-1 evidence for a black hole, astrophysical letters, 16 (1): 912, bibcode:1975apl....16....9s, doi:10.1016s0304-8853(99)00384-4 bardeen, j. m.; carter, b.; hawking, s. w. (1973). the four laws of black hole mechanics. communications in mathematical physics. 31 (2): 161170. bibcode:1973cmaph..31..161b. doi:10.1007bf01645742. mr 0334798. s2cid 54690354. zbl 1125.83309. archived from the original on 16 may 2020. retrieved 4 june 2021. a b c hawking, s. w. (1974). black hole explosions?. nature. 248 (5443): 3031. bibcode:1974natur.248...30h. doi:10.1038248030a0. s2cid 4290107. remillard, ronald a.; mcclintock, jeffrey e. (22 september 2006). x-ray properties of black-hole binaries. annual review of astronomy and astrophysics. 44: 4992. arxiv:astro-ph0606352. bibcode:2006araa..44...49r. doi:10.1146annurev.astro.44.051905.092532. issn 0066-4146. herbert, friedman (2002). from the ionosphere to high energy astronomy a personal experience. the century of space science. springer. isbn 0-7923-7196-8. liu, c. z.; li, t. p. (2004). x-ray spectral variability in cygnus x-1. the astrophysical journal. 611 (2): 10841090. arxiv:astro-ph0405246. bibcode:2004apj...611.1084l. doi:10.1086422209. s2cid 208868049. a b bowyer, s.; byram, e. t.; chubb, t. a.; friedman, h. (1965). cosmic x-ray sources. science. 147 (3656): 394398. bibcode:1965sci...147..394b. doi:10.1126science.147.3656.394. pmid 17832788. s2cid 206565068. maron, o.; kijak, j.; kramer, m.; wielebinski, r. (2000). pulsar spectra of radio emission. astronomy and astrophysics supplement series. 147 (2): 195203. arxiv:astro-ph0010233. bibcode:2000aas..147..195m. doi:10.1051aas:2000298. webster, b. louise; murdin, paul (1972). cygnus x-1a spectroscopic binary with a heavy companion?. nature. 235 (5332): 3738. bibcode:1972natur.235...37w. doi:10.1038235037a0. s2cid 4195462. bolton, c. t. (1972). identification of cygnus x-1 with hde 226868. nature. 235 (5336): 271273. bibcode:1972natur.235..271b. doi:10.1038235271b0. s2cid 4222070. a b rolston, bruce (10 november 1997). the first black hole. university of toronto. archived from the original on 7 march 2008. retrieved 11 march 2008. shipman, h. l.; yu, z.; du, y. w. (1975). the implausible history of triple star models for cygnus x-1 evidence for a black hole. astrophysical letters. 16 (1): 912. bibcode:1975apl....16....9s. doi:10.1016s0304-8853(99)00384-4. lynden-bell, d. (1969). galactic nuclei as collapsed old quasars. nature. 223 (5207): 690694. bibcode:1969natur.223..690l. doi:10.1038223690a0. rees, martin j. (1984). black hole"
  },
  {
    "chunk_id": 145,
    "doc_id": "Black_hole.txt",
    "text": "l.; yu, z.; du, y. w. (1975). the implausible history of triple star models for cygnus x-1 evidence for a black hole. astrophysical letters. 16 (1): 912. bibcode:1975apl....16....9s. doi:10.1016s0304-8853(99)00384-4. lynden-bell, d. (1969). galactic nuclei as collapsed old quasars. nature. 223 (5207): 690694. bibcode:1969natur.223..690l. doi:10.1038223690a0. rees, martin j. (1984). black hole models for active galactic nuclei. annual review of astronomy and astrophysics. 22: 471506. bibcode:1984araa..22..471r. doi:10.1146annurev.aa.22.090184.002351. a b ferrarese, laura; ford, holland (2005). supermassive black holes in galactic nuclei: past, present and future research. space science reviews. 116 (34): 523624. arxiv:astro-ph0411247. bibcode:2005ssrv..116..523f. doi:10.1007s11214-005-3947-6. a b c peterson, bradley m. (2014). measuring the masses of supermassive black holes. space science reviews. 183 (14): 253275. bibcode:2014ssrv..183..253p. doi:10.1007s11214-013-9987-4. merritt, david (1999). black holes and galaxy evolution. in combes, f.; mamon, g. a.; charmandaris, v. (eds.). dynamics of galaxies: from the early universe to the present. vol. 197. astronomical society of the pacific. pp. 221232. arxiv:astro-ph9910546. bibcode:2000aspc..197..221m. isbn 978-1-58381-024-8. tremaine, scott; gebhardt, karl; bender, ralf; bower, gary; dressler, alan; faber, s. m.; filippenko, alexei v.; green, richard; grillmair, carl; ho, luis c.; kormendy, john; lauer, tod r.; magorrian, john; pinkney, jason; richstone, douglas (2002). the slope of the black hole mass versus velocity dispersion correlation. the astrophysical journal. 574 (2): 740753. arxiv:astro-ph0203468. bibcode:2002apj...574..740t. doi:10.1086341002. ferrarese, laura; merritt, david (2000). a fundamental relation between supermassive black holes and their host galaxies. the astrophysical journal. 539 (1): l9l12. arxiv:astro-ph0006053. bibcode:2000apj...539l...9f. doi:10.1086312838. nelson, charles h.; green, richard f.; bower, gary; gebhardt, karl; weistrop, donna (2004). the relationship between black hole mass and velocity dispersion in seyfert 1 galaxies. the astrophysical journal. 615 (2): 652661. arxiv:astro-ph0407383. bibcode:2004apj...615..652n. doi:10.1086424657. genzel, r.; eckart, a.; ott, t.; eisenhauer, f. (1997). on the nature of the dark mass in the centre of the milky way. monthly notices of the royal astronomical society. 291: 219234. doi:10.1093mnras291.1.219. ghez, a. m.; klein, b. l.; morris, m.; becklin, e. e. (1998). high proper-motion stars in the vicinity of sagittarius a: evidence for a supermassive black hole at the center of our galaxy. the astrophysical journal. 509 (2): 678686. arxiv:astro-ph9807210. bibcode:1998apj...509..678g. doi:10.1086306528. a b c d e abbott, b.p.; et al. (2016). observation of gravitational waves from a binary black hole merger. phys. rev. lett. 116 (6) 061102. arxiv:1602.03837. bibcode:2016phrvl.116f1102a. doi:10.1103physrevlett.116.061102. pmid 26918975. s2cid 124959784. a b the ligo scientific collaboration and the virgo collaboration (2016). an improved analysis of gw150914 using a fully spin-precessing waveform model. physical"
  },
  {
    "chunk_id": 146,
    "doc_id": "Black_hole.txt",
    "text": "b.p.; et al. (2016). observation of gravitational waves from a binary black hole merger. phys. rev. lett. 116 (6) 061102. arxiv:1602.03837. bibcode:2016phrvl.116f1102a. doi:10.1103physrevlett.116.061102. pmid 26918975. s2cid 124959784. a b the ligo scientific collaboration and the virgo collaboration (2016). an improved analysis of gw150914 using a fully spin-precessing waveform model. physical review x. 6 (4) 041014. arxiv:1606.01210. bibcode:2016phrvx...6d1014a. doi:10.1103physrevx.6.041014. s2cid 18217435. abbott, benjamin p.; et al. (ligo scientific collaboration and virgo collaboration) (2016). observation of gravitational waves from a binary black hole merger. phys. rev. lett. 116 (6) 061102. arxiv:1602.03837. bibcode:2016phrvl.116f1102a. doi:10.1103physrevlett.116.061102. pmid 26918975. s2cid 124959784. observation of gravitational waves from a binary black hole merger (pdf). ligo scientific collaboration. facts. ligo. archived from the original on 4 july 2017. retrieved 24 august 2017. this is equivalent to measuring the distance from earth to the nearest star to an accuracy smaller than the width of a human hair! (that is, to proxima centauri at 4.02081013 km). the nobel prize in physics 2017. nobel foundation. burtnyk, kimberly (20 march 2025). ligo-virgo-kagra announce the 200th gravitational wave detection of o4!. ligo caltech. retrieved 22 october 2025. a b event horizon telescope, the (2019). first m87 event horizon telescope results. i. the shadow of the supermassive black hole. the astrophysical journal. 875 (1): l1. arxiv:1906.11238. bibcode:2019apj...875l...1e. doi:10.38472041-8213ab0ec7. s2cid 145906806. bouman, katherine l.; johnson, michael d.; zoran, daniel; fish, vincent l.; doeleman, sheperd s.; freeman, william t. (2016). computational imaging for vlbi image reconstruction. 2016 ieee conference on computer vision and pattern recognition (cvpr). pp. 913922. arxiv:1512.01413. doi:10.1109cvpr.2016.105. hdl:1721.1103077. isbn 978-1-4673-8851-1. s2cid 9085016. gardiner, aidan (12 april 2018). when a black hole finally reveals itself, it helps to have our very own cosmic reporterastronomers announced wednesday that they had captured the first image of a black hole. the timess dennis overbye answers readers questions. the new york times. archived from the original on 1 january 2022. retrieved 15 april 2019. how many telescopes are currently in the eht network?. event horizon telescope. retrieved 22 october 2025. lutz, ota (8 october 2024). how scientists captured the first image of a black hole. nasa.gov. retrieved 22 october 2025. chown, marcus (12 april 2019). the m87 image will change our understanding of black holes, but why was the photo so hard to capture?. bbc science focus magazine. retrieved 22 october 2025. astronomers reveal first image of the black hole at the heart of our galaxy. event horizon telescope."
  },
  {
    "chunk_id": 147,
    "doc_id": "Black_hole.txt",
    "text": "2025. chown, marcus (12 april 2019). the m87 image will change our understanding of black holes, but why was the photo so hard to capture?. bbc science focus magazine. retrieved 22 october 2025. astronomers reveal first image of the black hole at the heart of our galaxy. event horizon telescope. 12 may 2022. chakrabarti, sukanya; simon, joshua d.; craig, peter a.; reggiani, henrique; brandt, timothy d.; guhathakurta, puragra; dalba, paul a.; kirby, evan n.; chang, philip; hey, daniel r.; savino, alessandro; geha, marla; thompson, ian b. (1 july 2023). a noninteracting galactic black hole candidate in a binary system with a main-sequence star. the astronomical journal. 166 (1): 6. arxiv:2210.05003. bibcode:2023aj....166....6c. doi:10.38471538-3881accf21. issn 0004-6256. el-badry, kareem; rix, hans-walter; quataert, eliot; howard, andrew w.; isaacson, howard; fuller, jim; hawkins, keith; breivik, katelyn; wong, kaze w. k.; rodriguez, antonio c.; conroy, charlie; shahaf, sahar; mazeh, tsevi; arenou, frdric; burdge, kevin b.; bashi, dolev; faigler, simchon; weisz, daniel r.; seeburger, rhys; monter, silvia almada; wojno, jennifer (2023). a sun-like star orbiting a black hole. monthly notices of the royal astronomical society. 518 (1): 10571085. arxiv:2209.06833. bibcode:2023mnras.518.1057e. doi:10.1093mnrasstac3140. el-badry, kareem; rix, hans-walter; et al. (2 november 2022). a sun-like star orbiting a black hole. monthly notices of the royal astronomical society. 518 (1): 10571085. arxiv:2209.06833. bibcode:2023mnras.518.1057e. doi:10.1093mnrasstac3140. astronomers discover closest black hole to earth. noirlab.edu. noirlab. 4 november 2022. retrieved 4 november 2022. el-badry, kareem; rix, hans-walter; cendes, yvette; rodriguez, antonio c.; conroy, charlie; quataert, eliot; hawkins, keith; zari, eleonora; hobson, melissa; breivik, katelyn; rau, arne; berger, edo; shahaf, sahar; seeburger, rhys; burdge, kevin b. (1 february 2023). a red giant orbiting a black hole. monthly notices of the royal astronomical society. 521 (3): 43234348. arxiv:2302.07880. bibcode:2023mnras.521.4323e. doi:10.1093mnrasstad799. panuzzo, p.; mazeh, t.; arenou, f.; holl, b.; caffau, e. (2024). discovery of a dormant 33 solar-mass black hole in pre-release gaia astrometry. the astrophysical journal. 686: l2. arxiv:2404.10486. bibcode:2024aa...686l...2g. doi:10.10510004-6361202449763. poffenberger, leah. 2020 nobel prize in physics. american physical society. retrieved 20 october 2025. oxford mathematician roger penrose jointly wins the nobel prize in physics. university of oxford. 6 october 2020. retrieved 20 october 2025. the nobel prize in physics 2020. nobelprize.org. archived from the original on 24 april 2021. retrieved 8 october 2020. press release: the nobel prize in physics 2020. nobel foundation. retrieved 6 october 2020. overbye, dennis; taylor, derrick bryson (6 october 2020). nobel prize in physics awarded to 3 scientists for"
  },
  {
    "chunk_id": 148,
    "doc_id": "Black_hole.txt",
    "text": "the nobel prize in physics 2020. nobelprize.org. archived from the original on 24 april 2021. retrieved 8 october 2020. press release: the nobel prize in physics 2020. nobel foundation. retrieved 6 october 2020. overbye, dennis; taylor, derrick bryson (6 october 2020). nobel prize in physics awarded to 3 scientists for work on black holesthe prize was awarded half to roger penrose for showing how black holes could form and half to reinhard genzel and andrea ghez for discovering a supermassive object at the milky ways center. the new york times. retrieved 6 october 2020. pioneering physicist john wheeler dies at 96. scientific american. archived from the original on 28 november 2016. retrieved 27 november 2016. overbye, dennis (14 april 2008). john a. wheeler, physicist who coined the term black hole, is dead at 96. the new york times. archived from the original on 22 november 2016. retrieved 27 november 2016. frolov, valeri p.; zelnikov, andrei (1 december 2011). introduction to black hole physics (1st ed.). oxford university press. p. 1. isbn 978-0-19-969229-3. booth, ivan (2005). black-hole boundaries. canadian journal of physics. 83 (11): 10731099. arxiv:gr-qc0508107. bibcode:2005cajph..83.1073b. doi:10.1139p05-063. curiel, erik (2019). the many definitions of a black hole. nature astronomy. 3: 2734. arxiv:1808.01507. bibcode:2019natas...3...27c. doi:10.1038s41550-018-0602-1. susskind, leonard (2008). the black hole war: my battle with stephen hawking to make the world safe for quantum mechanics (1st ed.). new york: little, brown. isbn 978-0-316-01640-7. oclc 181603165. hamilton, andrew j. s.; lisle, jason p. (1 june 2008). the river model of black holes. american journal of physics. 76 (6): 519532. arxiv:gr-qc0411060. bibcode:2008amjph..76..519h. doi:10.11191.2830526. issn 0002-9505. hamilton, andrew. a black hole is a waterfall of space. inside black holes. archived from the original on 20 august 2025. retrieved 24 october 2025. hawking, s. w.; ellis, g. f. r. (1973). large scale structure of space time. cambridge university press. isbn 978-0-521-09906-6. archived from the original on 21 july 2020. retrieved 16 may 2020. appendix b shapiro, s. l.; teukolsky, s. a. (1983). black holes, white dwarfs, and neutron stars: the physics of compact objects. john wiley and sons. p. 357. isbn 978-0-471-87316-7. seeds, michael a.; backman, dana e. (2007). perspectives on astronomy. cengage learning. p. 167. isbn 978-0-495-11352-2. archived from the original on 10 august 2016. wald, r. m. (1997). gravitational collapse and cosmic censorship. in iyer, b. r.; bhawal, b. (eds.). black holes, gravitational radiation and the universe. dordrecht: springer. pp. 6986. arxiv:gr-qc9710068. doi:10.1007978-94-017-0934-7."
  },
  {
    "chunk_id": 149,
    "doc_id": "Black_hole.txt",
    "text": "backman, dana e. (2007). perspectives on astronomy. cengage learning. p. 167. isbn 978-0-495-11352-2. archived from the original on 10 august 2016. wald, r. m. (1997). gravitational collapse and cosmic censorship. in iyer, b. r.; bhawal, b. (eds.). black holes, gravitational radiation and the universe. dordrecht: springer. pp. 6986. arxiv:gr-qc9710068. doi:10.1007978-94-017-0934-7. isbn 978-94-017-0934-7. berger, b. k. (2002). numerical approaches to spacetime singularities. living reviews in relativity. 5 (1) 1: 20021. arxiv:gr-qc0201056. bibcode:2002lrr.....5....1b. doi:10.12942lrr-2002-1. pmc 5256073. pmid 28179859. a b joshi, pankaj s. (1 february 2009). naked singularities. scientific american. vol. 300, no. 2. pp. 3643. jstor 26001219. reynolds, christopher s. (2013). the spin of supermassive black holes. classical and quantum gravity. 30 (24). arxiv:1307.3246. bibcode:2013cqgra..30x4004r. doi:10.10880264-93813024244004. ghosh, rajes; mishra, akash k.; sarkar, sudipta (2021). overcharging extremal black holes. physical review d. 104 (10) 104043. arxiv:2106.10667. bibcode:2021phrvd.104j4043g. doi:10.1103physrevd.104.104043. black hole basics. nasa.gov. 13 march 2024. retrieved 25 october 2025. eilon, ehud; ori, amos (2016). numerical study of the gravitational shock wave inside a spherical charged black hole. physical review d. 94 (10) 104060. arxiv:1610.04355. bibcode:2016phrvd..94j4060e. doi:10.1103physrevd.94.104060. a b daly, ruth a. (2019). black hole spin and accretion disk magnetic field strength estimates for more than 750 active galactic nuclei and multiple galactic black holes. the astrophysical journal. 886 (1): 37. arxiv:1905.11319. bibcode:2019apj...886...37d. doi:10.38471538-4357ab35e6. daly, ruth a.; donahue, megan; odea, christopher p.; sebastian, biny; haggard, daryl; lu, anan (2024). new black hole spin values for sagittarius a obtained with the outflow method. monthly notices of the royal astronomical society. 527: 428436. doi:10.1093mnrasstad3228. a b reynolds, christopher s. (january 2019). observing black holes spin. nature astronomy. 3 (1): 4147. arxiv:1903.11704. bibcode:2019natas...3...41r. doi:10.1038s41550-018-0665-z. issn 2397-3366. s2cid 85543351. archived from the original on 18 november 2020. retrieved 21 august 2020. tamburini, fabrizio; thid, bo; della valle, massimo (2020). measurement of the spin of the m87 black hole from its observed twisted light. monthly notices of the royal astronomical society: letters. 492: l22l27. arxiv:1904.07923. doi:10.1093mnraslslz176. bambi, cosimo; freese, katherine; vagnozzi, sunny; visinelli, luca (2019). testing the rotational nature of the supermassive object m87 from the circularity and size of its first image. physical review d. 100 (4) 044057. arxiv:1904.12983. bibcode:2019phrvd.100d4057b. doi:10.1103physrevd.100.044057. a b abbott, b. p.; et al. (ligo scientific collaboration and virgo collaboration) (1 june 2017). gw170104: observation of a 50-solar-mass binary black hole coalescence at redshift 0.2. physical review letters. 118 (22) 221101. arxiv:1706.01812. bibcode:2017phrvl.118v1101a. doi:10.1103physrevlett.118.221101. pmid 28621973. s2cid 206291714. horbatsch, m.w; burgess, c.p (2012)."
  },
  {
    "chunk_id": 150,
    "doc_id": "Black_hole.txt",
    "text": "044057. arxiv:1904.12983. bibcode:2019phrvd.100d4057b. doi:10.1103physrevd.100.044057. a b abbott, b. p.; et al. (ligo scientific collaboration and virgo collaboration) (1 june 2017). gw170104: observation of a 50-solar-mass binary black hole coalescence at redshift 0.2. physical review letters. 118 (22) 221101. arxiv:1706.01812. bibcode:2017phrvl.118v1101a. doi:10.1103physrevlett.118.221101. pmid 28621973. s2cid 206291714. horbatsch, m.w; burgess, c.p (2012). cosmic black-hole hair growth and quasar oj287. journal of cosmology and astroparticle physics (5): 010. arxiv:1111.4009. bibcode:2012jcap...05..010h. doi:10.10881475-7516201205010. a b zajaek, michal; tursunov, arman; eckart, andreas; britzen, silke (2018). on the charge of the galactic centre black hole. monthly notices of the royal astronomical society. 480 (4): 44084423. arxiv:1808.07327. doi:10.1093mnrassty2182. xu, hao; ong, yen chin; yung, man-hong (2020). cosmic censorship and the evolution of displaystyle d -dimensional charged evaporating black holes. physical review d. 101 (6) 064015. arxiv:1911.11990. bibcode:2020phrvd.101f4015x. doi:10.1103physrevd.101.064015. gong, yi; cao, zhoujian; gao, he; zhang, bing (2019). on neutralization of charged black holes. monthly notices of the royal astronomical society. 488 (2): 27222731. arxiv:1907.05239. doi:10.1093mnrasstz1904. zakharov, a. f.; de paolis, f.; ingrosso, g.; nucita, a. a. (2005). direct measurements of black hole charge with future astrometrical missions. astronomy astrophysics. 442 (3): 795799. arxiv:astro-ph0505286. bibcode:2005aa...442..795z. doi:10.10510004-6361:20053432. turimov, bobur; boboqambarova, madina; ahmedov, bobomurat; stuchlk, zdenk (2022). distinguishable feature of electric and magnetic charged black hole. the european physical journal plus. 137 (2) 222. doi:10.1140epjps13360-022-02390-7. cromartie, h. t.; fonseca, e.; ransom, s. m.; demorest, p. b.; arzoumanian, z.; blumer, h.; brook, p. r.; decesar, m. e.; dolch, t.; ellis, j. a.; ferdman, r. d.; ferrara, e. c.; garver-daniels, n.; gentile, p. a.; jones, m. l.; lam, m. t.; lorimer, d. r.; lynch, r. s.; mclaughlin, m. a.; ng, c.; nice, d. j.; pennucci, t. t.; spiewak, r.; stairs, i. h.; stovall, k.; swiggum, j. k.; zhu, w. w. (2019). relativistic shapiro delay measurements of an extremely massive millisecond pulsar. nature astronomy. 4: 7276. arxiv:1904.06759. doi:10.1038s41550-019-0880-2. drischler, christian; han, sophia; lattimer, james m.; prakash, madappa; reddy, sanjay; zhao, tianqi (2021). limiting masses and radii of neutron stars and their implications. physical review c. 103 (4) 045808. arxiv:2009.06441. bibcode:2021phrvc.103d5808d. doi:10.1103physrevc.103.045808. farr, will m.; sravan, niharika; cantrell, andrew; kreidberg, laura; bailyn, charles d.; mandel, ilya; kalogera, vicky (2011). the mass distribution of stellar-mass black holes. the astrophysical journal. 741 (2): 103. arxiv:1011.1459. bibcode:2011apj...741..103f. doi:10.10880004-637x7412103. a b c carr, bernard; kohri, kazunori; sendouda, yuuiti; yokoyama, junichi (2021). constraints on primordial black holes. reports on progress in physics. 84 (11). arxiv:2002.12778. bibcode:2021rpph...84k6902c. doi:10.10881361-6633ac1e31. pmid 34874316. nakama,"
  },
  {
    "chunk_id": 151,
    "doc_id": "Black_hole.txt",
    "text": "ilya; kalogera, vicky (2011). the mass distribution of stellar-mass black holes. the astrophysical journal. 741 (2): 103. arxiv:1011.1459. bibcode:2011apj...741..103f. doi:10.10880004-637x7412103. a b c carr, bernard; kohri, kazunori; sendouda, yuuiti; yokoyama, junichi (2021). constraints on primordial black holes. reports on progress in physics. 84 (11). arxiv:2002.12778. bibcode:2021rpph...84k6902c. doi:10.10881361-6633ac1e31. pmid 34874316. nakama, tomohiro; yokoyama, junichi (2019). micro black holes formed in the early universe and their cosmological implications. physical review d. 99 (6) 061303. arxiv:1811.05049. bibcode:2019phrvd..99f1303n. doi:10.1103physrevd.99.061303. scardigli, fabio (2000). gravity coupling from micro-black holes. nuclear physics b - proceedings supplements. 88 (13): 291294. arxiv:hep-th9907150. bibcode:2000nuphs..88..291s. doi:10.1016s0920-5632(00)00788-x. belczynski, krzysztof; bulik, tomasz; fryer, chris l.; ruiter, ashley; valsecchi, francesca; vink, jorick s.; hurley, jarrod r. (2010). on the maximum mass of stellar black holes. the astrophysical journal. 714 (2): 12171226. arxiv:0904.2784. bibcode:2010apj...714.1217b. doi:10.10880004-637x71421217. fryer, chris l.; kalogera, vassiliki (2001). theoretical black hole mass distributions. the astrophysical journal. 554 (1): 548560. arxiv:astro-ph9911312. bibcode:2001apj...554..548f. doi:10.1086321359. fryer, chris l. (1999). mass limits for black hole formation. the astrophysical journal. 522 (1): 413418. arxiv:astro-ph9902315. bibcode:1999apj...522..413f. doi:10.1086307647. vink, jorick s.; higgins, erin r.; sander, andreas a c.; sabhahit, gautham n. (2021). maximum black hole mass across cosmic time. monthly notices of the royal astronomical society. 504: 146154. arxiv:2010.11730. doi:10.1093mnrasstab842. a b coleman miller, m.; colbert, e. j. m. (2004). intermediate-mass black holes. international journal of modern physics d. 13 (1): 164. arxiv:astro-ph0308402. bibcode:2004ijmpd..13....1m. doi:10.1142s0218271804004426. bennett, jeffrey (2025). degeneracy pressure in stars and stellar corpses. the physics teacher. 63 (3): 212213. bibcode:2025phtea..63c.212b. doi:10.11195.0260882. a b c d e f g h i j k l m carroll, sean m. (2003). spacetime and geometry: an introduction to general relativity. addison-wesley. isbn 978-0-8053-8732-2., the lecture notes on which the book was based are available for free from sean carrolls website archived 23 march 2017 at the wayback machine a b c dunn, r. j. h.; fender, r. p.; krding, e. g.; belloni, t.; cabanac, c. (2010). a global spectral study of black hole x-ray binaries. monthly notices of the royal astronomical society. 403 (1): 6182. arxiv:0912.0142. bibcode:2010mnras.403...61d. doi:10.1111j.1365-2966.2010.16114.x. lewin, walter h. g.; jan, van paradijs; van den heuvel, edward p.j., eds. (28 january 1997). x-ray binaries. cambridge university press. p. 1. isbn 978-0-521-59934-4. shao, yong; li, xiang-dong (2020). population synthesis of black hole x-ray binaries. the astrophysical journal. 898 (2): 143. arxiv:2006.15961. bibcode:2020apj...898..143s. doi:10.38471538-4357aba118. a b black hole types. nasa.gov. 21 october 2020. retrieved 26 october 2025. caputo, daniel p.; de"
  },
  {
    "chunk_id": 152,
    "doc_id": "Black_hole.txt",
    "text": "(28 january 1997). x-ray binaries. cambridge university press. p. 1. isbn 978-0-521-59934-4. shao, yong; li, xiang-dong (2020). population synthesis of black hole x-ray binaries. the astrophysical journal. 898 (2): 143. arxiv:2006.15961. bibcode:2020apj...898..143s. doi:10.38471538-4357aba118. a b black hole types. nasa.gov. 21 october 2020. retrieved 26 october 2025. caputo, daniel p.; de vries, nathan; patruno, alessandro; portegies zwart, simon (2017). on estimating the total number of intermediate mass black holes. monthly notices of the royal astronomical society stw3336. doi:10.1093mnrasstw3336. coleman miller, m.; hamilton, douglas p. (2002). production of intermediate-mass black holes in globular clusters. monthly notices of the royal astronomical society. 330 (1): 232240. arxiv:astro-ph0106188. bibcode:2002mnras.330..232c. doi:10.1046j.1365-8711.2002.05112.x. rizzuto, francesco paolo; naab, thorsten; spurzem, rainer; giersz, mirek; ostriker, j. p.; stone, n. c.; wang, long; berczik, peter; rampp, m. (2021). intermediate mass black hole formation in compact young massive star clusters. monthly notices of the royal astronomical society. 501 (4): 52575273. arxiv:2008.09571. doi:10.1093mnrasstaa3634. fujii, michiko s.; wang, long; tanikawa, ataru; hirai, yutaka; saitoh, takayuki r. (2024). simulations predict intermediate-mass black hole formation in globular clusters. science. 384 (6703): 14881492. arxiv:2406.06772. bibcode:2024sci...384.1488f. doi:10.1126science.adi4211. pmid 38815090. barai, paramita; de gouveia dal pino, elisabete m. (2019). intermediate-mass black hole growth and feedback in dwarf galaxies at high redshifts. monthly notices of the royal astronomical society. 487 (4): 55495563. arxiv:1807.04768. doi:10.1093mnrasstz1616. beckmann, r. s.; dubois, y.; volonteri, m.; dong-pez, c. a.; trebitsch, m.; devriendt, j.; kaviraj, s.; kimm, t.; peirani, s. (2023). population statistics of intermediate-mass black holes in dwarf galaxies using the newhorizon simulation. monthly notices of the royal astronomical society. 523 (4): 56105623. arxiv:2211.13301. doi:10.1093mnrasstad1544. ruiz-rocha, krystal; yelikar, anjali b.; lange, jacob; gabella, william; weller, robert a.; oshaughnessy, richard; holley-bockelmann, kelly; jani, karan (2025). properties of lite intermediate-mass black hole candidates in ligo-virgos third observing run. the astrophysical journal letters. 985 (2): l37. arxiv:2502.17681. bibcode:2025apj...985l..37r. doi:10.38472041-8213adc5f8. abbott, r.; et al. (2022). search for intermediate-mass black hole binaries in the third observing run of advanced ligo and advanced virgo. astronomy astrophysics. 659: a84. arxiv:2105.15120. bibcode:2022aa...659a..84a. doi:10.10510004-6361202141452. black holes. center for astrophysics. retrieved 26 october 2025. mezcua, mar (2021). black holes. encyclopedia of astrobiology. pp. 18. arxiv:2110.08629. doi:10.1007978-3-642-27833-45510-1. isbn 978-3-642-27833-4. broderick, avery e.; loeb, abraham; narayan, ramesh (2009). the event horizon of sagittarius a. the astrophysical journal. 701 (2): 13571366. arxiv:0903.1105. bibcode:2009apj...701.1357b. doi:10.10880004-637x70121357. yuan, feng; quataert, eliot; narayan, ramesh (2004). on the nature of the variable infrared emission from sagittarius a. the astrophysical journal. 606 (2): 894899."
  },
  {
    "chunk_id": 153,
    "doc_id": "Black_hole.txt",
    "text": "doi:10.1007978-3-642-27833-45510-1. isbn 978-3-642-27833-4. broderick, avery e.; loeb, abraham; narayan, ramesh (2009). the event horizon of sagittarius a. the astrophysical journal. 701 (2): 13571366. arxiv:0903.1105. bibcode:2009apj...701.1357b. doi:10.10880004-637x70121357. yuan, feng; quataert, eliot; narayan, ramesh (2004). on the nature of the variable infrared emission from sagittarius a. the astrophysical journal. 606 (2): 894899. arxiv:astro-ph0401429. bibcode:2004apj...606..894y. doi:10.1086383117. a b c rees, m. j.; volonteri, m. (2007). massive black holes: formation and evolution. in karas, v.; matt, g. (eds.). black holes from stars to galaxiesacross the range of masses. proceedings of the international astronomical union. pp. 5158. arxiv:astro-ph0701512. bibcode:2007iaus..238...51r. doi:10.1017s1743921307004681. isbn 978-0-521-86347-6. s2cid 14844338. choi, jun-hwan; shlosman, isaac; begelman, mitchell c. (2013). supermassive black hole formation at high redshifts via direct collapse: physical processes in the early stage. the astrophysical journal. 774 (2): 149. arxiv:1304.1369. bibcode:2013apj...774..149c. doi:10.10880004-637x7742149. natarajan, priyamvada; treister, ezequiel (2009). is there an upper limit to black hole masses?. monthly notices of the royal astronomical society. 393 (3): 838845. arxiv:0808.2813. bibcode:2009mnras.393..838n. doi:10.1111j.1365-2966.2008.13864.x. dullo, bililign t.; gil de paz, armando; knapen, johan h. (2021). ultramassive black holes in the most massive galaxies: m bh versus m bhr b. the astrophysical journal. 908 (2): 134. arxiv:2012.04471. bibcode:2021apj...908..134d. doi:10.38471538-4357abceae. king, andrew (february 2016). how big can a black hole grow?. monthly notices of the royal astronomical society: letters. 456 (1): l109l112. arxiv:1511.08502. bibcode:2016mnras.456l.109k. doi:10.1093mnraslslv186. s2cid 40147275. clery, daniel (21 december 2015). limit to how big black holes can grow is astonishing. sciencemag.org. retrieved 27 november 2018. wald, robert m. (1984). general relativity. university of chicago press. isbn 978-0-226-87033-5. archived from the original on 11 august 2016. retrieved 23 february 2016. saa, alberto; santarelli, raphael (18 july 2011). destroying a near-extremal kerrnewman black hole. physical review d. 84 (2) 027501. arxiv:1105.3950. bibcode:2011phrvd..84b7501s. doi:10.1103physrevd.84.027501. s2cid 118487989. reynolds, christopher s. (8 september 2021). observational constraints on black hole spin. annual review of astronomy and astrophysics. 59 (1): 117154. arxiv:2011.08948. bibcode:2021araa..59..117r. doi:10.1146annurev-astro-112420-035022. issn 0066-4146. a b mirabel, i. f.; rodrguez, l. f. (1999). sources of relativistic jets in the galaxy. annual review of astronomy and astrophysics. 37: 409443. arxiv:astro-ph9902062. bibcode:1999araa..37..409m. doi:10.1146annurev.astro.37.1.409. relativistic jets. nustar. retrieved 9 november 2025. bagchi, joydeep; vivek, m.; vikram, vinu; hota, ananda; biju, k. g.; sirothia, s. k.; srianand, raghunathan; gopal-krishna; jacob, joe (2014). megaparsec relativistic jets launched from an accreting supermassive black hole in an extreme spiral galaxy. the astrophysical journal. 788 (2): 174. arxiv:1404.6889. bibcode:2014apj...788..174b. doi:10.10880004-637x7882174. a b nemmen, r. s.; georganopoulos, m.; guiriec,"
  },
  {
    "chunk_id": 154,
    "doc_id": "Black_hole.txt",
    "text": "vivek, m.; vikram, vinu; hota, ananda; biju, k. g.; sirothia, s. k.; srianand, raghunathan; gopal-krishna; jacob, joe (2014). megaparsec relativistic jets launched from an accreting supermassive black hole in an extreme spiral galaxy. the astrophysical journal. 788 (2): 174. arxiv:1404.6889. bibcode:2014apj...788..174b. doi:10.10880004-637x7882174. a b nemmen, r. s.; georganopoulos, m.; guiriec, s.; meyer, e. t.; gehrels, n.; sambruna, r. m. (2012). a universal scaling for the energetics of relativistic jets from black hole systems. science. 338 (6113): 14451448. arxiv:1212.3343. bibcode:2012sci...338.1445n. doi:10.1126science.1227416. pmid 23239730. a b blandford, roger; meier, david; readhead, anthony (2019). relativistic jets from active galactic nuclei. annual review of astronomy and astrophysics. 57: 467509. arxiv:1812.06025. bibcode:2019araa..57..467b. doi:10.1146annurev-astro-081817-051948. chen , yongyun ; gu , qiusheng ; fan , junhui ; zhou , hongyan ; yuan , yefei ; gu , weimin ; wu , qinwen ; xiong , dingrong ; guo , xiaotong ; ding , nan .; yu , xiaoling (2021). the powers of relativistic jets depend on the spin of accreting supermassive black holes. the astrophysical journal. 913 (2): 93. arxiv:2104.04242. bibcode:2021apj...913...93c. doi:10.38471538-4357abf4ff. ghisellini, g.; haardt, f.; ceca, r. della; volonteri, m.; sbarrato, t. (2013). the role of relativistic jets in the heaviest and most active supermassive black holes at high redshift. monthly notices of the royal astronomical society. 432 (4): 28182823. doi:10.1093mnrasstt637. ghisellini, g.; tavecchio, f.; maraschi, l.; celotti, a.; sbarrato, t. (2014). the power of relativistic jets is larger than the luminosity of their accretion disks. nature. 515 (7527): 376378. arxiv:1411.5368. bibcode:2014natur.515..376g. doi:10.1038nature13856. pmid 25409827. mirabel, i. f.; rodrguez, l. f. (april 1998). microquasars in our galaxy. nature. 392 (6677): 673676. bibcode:1998natur.392..673m. doi:10.103833603. issn 0028-0836. meier, david l. (2003). the theory and simulation of relativistic jet formation: towards a unified model for micro- and macroquasars. new astronomy reviews. 47 (67): 667672. arxiv:astro-ph0312048. bibcode:2003newar..47..667m. doi:10.1016s1387-6473(03)00120-9. a b lee, hyun kyu; wijers, r.a.m.j.; brown, g.e. (2000). the blandfordznajek process as a central engine for a gamma-ray burst. physics reports. 325 (3): 83114. arxiv:astro-ph9906213. bibcode:2000phr...325...83l. doi:10.1016s0370-1573(99)00084-8. blandford, r. d.; znajek, r. l. (1977). electromagnetic extraction of energy from kerr black holes. monthly notices of the royal astronomical society. 179 (3): 433. arxiv:astro-ph0506302. bibcode:1977mnras.179..433b. doi:10.1093mnras179.3.433. penrose, r. (1969). gravitational collapse: the role of general relativity. rivista del nuovo cimento. 1: 252276. bibcode:1969ncimr...1..252p. narayan, ramesh; mcclintock, jeffrey e.; tchekhovskoy, alexander (2014). energy extraction from spinning black holes via relativistic jets. general relativity, cosmology and astrophysics. pp. 523535. arxiv:1303.3004. doi:10.1007978-3-319-06349-225. isbn 978-3-319-06348-5."
  },
  {
    "chunk_id": 155,
    "doc_id": "Black_hole.txt",
    "text": "(3): 433. arxiv:astro-ph0506302. bibcode:1977mnras.179..433b. doi:10.1093mnras179.3.433. penrose, r. (1969). gravitational collapse: the role of general relativity. rivista del nuovo cimento. 1: 252276. bibcode:1969ncimr...1..252p. narayan, ramesh; mcclintock, jeffrey e.; tchekhovskoy, alexander (2014). energy extraction from spinning black holes via relativistic jets. general relativity, cosmology and astrophysics. pp. 523535. arxiv:1303.3004. doi:10.1007978-3-319-06349-225. isbn 978-3-319-06348-5. black hole anatomy. nasa science. 2 august 2022. archived from the original on 24 april 2025. retrieved 13 october 2025. cunha, pedro; eir, nelson; herdeiro, carlos; lemos, jos (16 march 2020). lensing and shadow of a black hole surrounded by a heavy accretion disk. journey of cosmology and astroparticle physics. 2020 (3): 035. arxiv:1912.08833. bibcode:2020jcap...03..035c. doi:10.10881475-7516202003035 via iopscience. demtrder, wolfgang (2024). astrophysics. undergraduate lecture notes in physics. doi:10.1007978-3-031-22135-4. isbn 978-3-031-22133-0. issn 2192-4791. a b mcclintock, j. e.; remillard, r. a. (2006). black hole binaries. in lewin, w.; van der klis, m. (eds.). compact stellar x-ray sources. p. 157. arxiv:astro-ph0306213. bibcode:2006csxs.book..157m. isbn 978-0-521-82659-4. section 4.1.5. abramowicz, marek a.; fragile, p. chris (2013). foundations of black hole accretion disk theory. living reviews in relativity. 16 (1) 1. arxiv:1104.5499. bibcode:2013lrr....16....1a. doi:10.12942lrr-2013-1. pmc 5256006. pmid 28179840. blaes, omer (2014). general overview of black hole accretion theory. space science reviews. 183 (14): 2141. arxiv:1304.4879. bibcode:2014ssrv..183...21b. doi:10.1007s11214-013-9985-6. a b page, don n.; thorne, kip s. (1974). disk-accretion onto a black hole. time-averaged structure of accretion disk. the astrophysical journal. 191: 499. bibcode:1974apj...191..499p. doi:10.1086152990. lasota, jean-pierre (2016). black hole accretion discs. astrophysics of black holes. astrophysics and space science library. vol. 440. pp. 160. arxiv:1505.02172. doi:10.1007978-3-662-52859-41. isbn 978-3-662-52857-0. beloborodov, a. m. (1998). super-eddington accretion discs around kerr black holes. monthly notices of the royal astronomical society. 297 (3): 739746. arxiv:astro-ph9802129. bibcode:1998mnras.297..739b. doi:10.1046j.1365-8711.1998.01530.x. bisnovatyi-kogan, gennady (2019). accretion into black hole, and formation of magnetically arrested accretion disks. universe. 5 (6): 146. arxiv:1905.13731. bibcode:2019univ....5..146b. doi:10.3390universe5060146. zakharov, a. f.; repin, s. v. (2002). model radiation spectrum for an accretion disk near a rotating black hole. astronomy reports. 46 (5): 360365. bibcode:2002arep...46..360z. doi:10.11341.1479423. wang, zi-liang (2025). exploring the role of accretion disk geometry in shaping black hole shadows. physical review d. 112 (6) 064052. arxiv:2506.21148. bibcode:2025phrvd.112f4052w. doi:10.1103fhqj-wgcm. gimeno-soler, sergio; font, jos a. (2017). magnetised polish doughnuts revisited. astronomy astrophysics. 607: a68. arxiv:1707.03867. bibcode:2017aa...607a..68g. doi:10.10510004-6361201730935. qian, lei; abramowicz, m. a.; fragile, p. c.; hork, j.; machida, m.; straub, o. (2009). the polish doughnuts revisited. astronomy astrophysics. 498 (2): 471477. doi:10.10510004-6361200811518. abramowicz, m.a. (2005). super-eddington black hole accretion. growing black holes: accretion in"
  },
  {
    "chunk_id": 156,
    "doc_id": "Black_hole.txt",
    "text": "a. (2017). magnetised polish doughnuts revisited. astronomy astrophysics. 607: a68. arxiv:1707.03867. bibcode:2017aa...607a..68g. doi:10.10510004-6361201730935. qian, lei; abramowicz, m. a.; fragile, p. c.; hork, j.; machida, m.; straub, o. (2009). the polish doughnuts revisited. astronomy astrophysics. 498 (2): 471477. doi:10.10510004-6361200811518. abramowicz, m.a. (2005). super-eddington black hole accretion. growing black holes: accretion in a cosmological context. eso astrophysics symposia. pp. 257273. doi:10.10071140391349. isbn 978-3-540-25275-7. kishimoto, makoto; antonucci, robert; blaes, omer; lawrence, andy; boisson, catherine; albrecht, marcus; leipski, christian (2008). the characteristic blue spectra of accretion disks in quasars as uncovered in the infrared. nature. 454 (7203): 492494. arxiv:0807.3703. bibcode:2008natur.454..492k. doi:10.1038nature07114. pmid 18650919. fukue, jun; yokoyama, takushi (1988). color photographs of an accretion disk around a black hole. publications of the astronomical society of japan. 40: 1524. doi:10.1093pasj40.1.15. bonning, e. w.; cheng, l.; shields, g. a.; salviander, s.; gebhardt, k. (2007). accretion disk temperatures and continuum colors in qsos. the astrophysical journal. 659 (1): 211217. arxiv:astro-ph0611263. bibcode:2007apj...659..211b. doi:10.1086510712. a b c d thorne, kip (7 november 2014). the science of interstellar. w. w. norton company. isbn 978-0-393-35137-8. james, oliver; tunzelmann, eugnie von; franklin, paul; thorne, kip s. (2015). gravitational lensing by spinning black holes in astrophysics, and in the movie interstellar. classical and quantum gravity. 32 (6). arxiv:1502.03808. bibcode:2015cqgra..32f5001j. doi:10.10880264-9381326065001. guo, sen; huang, yu-xiang; cui, yu-hao; han, yan; jiang, qing-quan; liang, en-wei; lin, kai (2023). unveiling the unconventional optical signatures of regular black holes within accretion disk. the european physical journal c. 83 (11) 1059. arxiv:2310.20523. bibcode:2023epjc...83.1059g. doi:10.1140epjcs10052-023-12208-0. mcclintock, jeffrey e.; narayan, ramesh; davis, shane w.; gou, lijun; kulkarni, akshay; orosz, jerome a.; penna, robert f.; remillard, ronald a.; steiner, james f. (2011). measuring the spins of accreting black holes. classical and quantum gravity. 28 (11). arxiv:1101.0811. bibcode:2011cqgra..28k4009m. doi:10.10880264-93812811114009. a b misner, charles; thorne, kip s.; wheeler, john (1973). gravitation. w. h. freeman and company. isbn 978-0-7167-0344-0. a b c jefremov, paul i.; tsupko, oleg yu.; bisnovatyi-kogan, gennady s. (2015). innermost stable circular orbits of spinning test particles in schwarzschild and kerr space-times. physical review d. 91 (12) 124030. arxiv:1503.07060. bibcode:2015phrvd..91l4030j. doi:10.1103physrevd.91.124030. a b bardeen, james m.; press, william h.; teukolsky, saul a. (1 december 1972). rotating black holes: locally nonrotating frames, energy extraction, and scalar synchrotron radiation. the astrophysical journal. 178: 347370. bibcode:1972apj...178..347b. doi:10.1086151796. a b zhang, yu-peng; wei, shao-wen; guo, wen-di; sui, tao-tao; liu, yu-xiao (2018). innermost stable circular orbit of spinning particle in charged spinning black hole background. physical review d."
  },
  {
    "chunk_id": 157,
    "doc_id": "Black_hole.txt",
    "text": "december 1972). rotating black holes: locally nonrotating frames, energy extraction, and scalar synchrotron radiation. the astrophysical journal. 178: 347370. bibcode:1972apj...178..347b. doi:10.1086151796. a b zhang, yu-peng; wei, shao-wen; guo, wen-di; sui, tao-tao; liu, yu-xiao (2018). innermost stable circular orbit of spinning particle in charged spinning black hole background. physical review d. 97 (8) 084056. arxiv:1711.09361. bibcode:2018phrvd..97h4056z. doi:10.1103physrevd.97.084056. tsupko, o. yu.; bisnovatyi-kogan, g. s.; jefremov, p. i. (2016). parameters of innermost stable circular orbits of spinning test particles: numerical and analytical calculations. gravitation and cosmology. 22 (2): 138147. arxiv:1605.04189. bibcode:2016grco...22..138t. doi:10.1134s0202289316020158. jefremov, paul i.; tsupko, oleg yu.; bisnovatyi-kogan, gennady s. (2017). spin-induced changes in the parameters of isco in kerr spacetime. the fourteenth marcel grossmann meeting. pp. 37153721. doi:10.114297898132266090486. isbn 978-981-322-659-3. vzquez, s.e.; esteban, e.p. (7 december 2004). strong-field gravitational lensing by a kerr black hole. il nuovo cimento b. 119 (5): 489519. arxiv:gr-qc0308023. bibcode:2004ncimb.119..489v. doi:10.1393ncbi2004-10121-y. cramer, claes r. (1997). using the uncharged kerr black hole as a gravitational mirror. general relativity and gravitation. 29 (4): 445454. arxiv:gr-qc9510053. bibcode:1997gregr..29..445c. doi:10.1023a:1018878515046. s2cid 9517046. a b l, h.; lyu, hong-da (2020). schwarzschild black holes have the largest size. physical review d. 101 (4) 044059. arxiv:1911.02019. bibcode:2020phrvd.101d4059l. doi:10.1103physrevd.101.044059. qiao, chen-kai (2022). curvatures, photon spheres, and black hole shadows. physical review d. 106 (8) 084060. arxiv:2208.01771. bibcode:2022phrvd.106h4060q. doi:10.1103physrevd.106.084060. horvath, jorge ernesto (2022). high-energy astrophysics. undergraduate lecture notes in physics. doi:10.1007978-3-030-92159-0. isbn 978-3-030-92158-3. issn 2192-4791. a b nitta, daisuke; chiba, takeshi; sugiyama, naoshi (september 2011). shadows of colliding black holes. physical review d. 84 (6) 063008. arxiv:1106.2425. bibcode:2011phrvd..84f3008n. doi:10.1103physrevd.84.063008. s2cid 119264596. cramer, claes r. (april 1997). using the uncharged kerr black hole as a gravitational mirror. general relativity and gravitation. 29 (4): 445454. arxiv:gr-qc9510053. bibcode:1997gregr..29..445c. doi:10.1023a:1018878515046. issn 0001-7701. teo, edward (2003). spherical photon orbits around a kerr black hole. general relativity and gravitation. 35 (11): 19091926. bibcode:2003gregr..35.1909t. doi:10.1023a:1026286607562. heydarzade, yaghoub; vertogradov, vitalii (2024). dynamical photon spheres in charged black holes and naked singularities. the european physical journal c. 84 (6) 582. arxiv:2311.08930. bibcode:2024epjc...84..582h. doi:10.1140epjcs10052-024-12945-w. chen, ying-xuan; huang, jia-hui; jiang, haoxiang (2023). radii of spherical photon orbits around kerr-newman black holes. physical review d. 107 (4) 044066. arxiv:2210.08509. bibcode:2023phrvd.107d4066c. doi:10.1103physrevd.107.044066. a b visser, matt (2007). the kerr spacetime: a brief introduction. page 35, fig. 3. arxiv:0706.0622 gr-qc. reynolds, christopher s. (2019). observing black holes spin. nature astronomy. 3: 4147. arxiv:1903.11704. bibcode:2019natas...3...41r. doi:10.1038s41550-018-0665-z. researchers clarify dynamics of black hole rotational energy. archived from the original on 17 september 2018."
  },
  {
    "chunk_id": 158,
    "doc_id": "Black_hole.txt",
    "text": "bibcode:2023phrvd.107d4066c. doi:10.1103physrevd.107.044066. a b visser, matt (2007). the kerr spacetime: a brief introduction. page 35, fig. 3. arxiv:0706.0622 gr-qc. reynolds, christopher s. (2019). observing black holes spin. nature astronomy. 3: 4147. arxiv:1903.11704. bibcode:2019natas...3...41r. doi:10.1038s41550-018-0665-z. researchers clarify dynamics of black hole rotational energy. archived from the original on 17 september 2018. retrieved 17 september 2018. first proof of black hole plunging regions. department of physics. university of oxford. 16 may 2024. mummery, andrew; ingram, adam; davis, shane; fabian, andrew (june 2024). continuum emission from within the plunging region of black hole discs. monthly notices of the royal astronomical society. 531 (1): 366386. arxiv:2405.09175. doi:10.1093mnrasstae1160. machida, mami; matsumoto, ryoji (2003). global three-dimensional magnetohydrodynamic simulations of black hole accretion disks: x-ray flares in the plunging region. the astrophysical journal. 585 (1): 429442. arxiv:astro-ph0211240. bibcode:2003apj...585..429m. doi:10.1086346070. prisco, jacopo (17 may 2024). study proves black holes have a plunging region, just as einstein predicted. cnn. davies, paul (1992). the new physics (illustrated ed.). cambridge university press. p. 26. isbn 978-0-521-43831-5. archived from the original on 17 august 2021. retrieved 25 september 2020. extract of page 26 archived 15 august 2021 at the wayback machine fleisch, daniel; kregenow, julia (2013). a students guide to the mathematics of astronomy (illustrated ed.). cambridge university press. p. 168. isbn 978-1-107-03494-5. archived from the original on 17 august 2021. retrieved 25 september 2020. extract of page 168 archived 17 august 2021 at the wayback machine a b wheeler, j. craig (2007). cosmic catastrophes (2nd ed.). cambridge university press. isbn 978-0-521-85714-7. singularities and black holes lightcones and causal structure. plato.stanford.edu. stanford encyclopedia of philosophy. archived from the original on 17 may 2019. retrieved 11 march 2018. inside a black hole. knowing the universe and its secrets. archived from the original on 23 april 2009. retrieved 26 march 2009. what happens to you if you fall into a black hole. math.ucr.edu. john baez. archived from the original on 13 february 2019. retrieved 11 march 2018. susskind, leonard (1 april 1997). black holes and the information paradox. scientific american. no. april 1997. p. 52-57. jstor 24993702. retrieved 9 december 2025. watch: three ways an astronaut could fall into a black hole. 1 february 2014. archived from the original on 15 april 2019. retrieved 13 march 2018. smarr, l. (1973). surface geometry of charged rotating black holes. physical review d. 7 (2): 289295. bibcode:1973phrvd...7..289s. doi:10.1103physrevd.7.289. visser, m. (22 january 2009). the kerr spacetime: a"
  },
  {
    "chunk_id": 159,
    "doc_id": "Black_hole.txt",
    "text": "an astronaut could fall into a black hole. 1 february 2014. archived from the original on 15 april 2019. retrieved 13 march 2018. smarr, l. (1973). surface geometry of charged rotating black holes. physical review d. 7 (2): 289295. bibcode:1973phrvd...7..289s. doi:10.1103physrevd.7.289. visser, m. (22 january 2009). the kerr spacetime: a brief introduction. in wiltshire, d.l.; visser, m.; scott, s.m. (eds.). horizon geometry for kerr black holes with synchronized hair. vol. 97. cambridge university press. arxiv:0706.0622. bibcode:2018phrvd..97l4012d. doi:10.1103physrevd.97.124012. isbn 978-0-521-88512-6. archived from the original on 20 may 2020. retrieved 12 january 2020. delgado, j.f. m.; herdeiro, c.a. r.; radu, e. (2018). horizon geometry for kerr black holes with synchronized hair. physical review d. 97 (12) 124012. arxiv:1804.04910. bibcode:2018phrvd..97l4012d. doi:10.1103physrevd.97.124012. hdl:1077324121. s2cid 55732213. a b c hod, shahar; piran, tsvi (1998). the inner structure of black holes. general relativity and gravitation. 30 (11): 15551562. arxiv:gr-qc9902008. bibcode:1998gregr..30.1555h. doi:10.1023a:1026654519980. a b c poisson, eric; israel, werner (1990). internal structure of black holes. physical review d. 41 (6): 17961809. bibcode:1990phrvd..41.1796p. doi:10.1103physrevd.41.1796. pmid 10012548. a b c d e scheel, m. a.; thorne, k. s. (2014). geometrodynamics: the nonlinear dynamics of curved spacetime. physics-uspekhi. 57 (4): 342351. arxiv:1706.09078. bibcode:2014phyu...57..342s. doi:10.3367ufne.0184.201404b.0367. a b c marolf, donald; ori, amos (2012). outgoing gravitational shock wave at the inner horizon: the late-time limit of black hole interiors. physical review d. 86 (12) 124026. arxiv:1109.5139. bibcode:2012phrvd..86l4026m. doi:10.1103physrevd.86.124026. a b ori, amos (1991). inner structure of a charged black hole: an exact mass-inflation solution. physical review letters. 67 (7): 789792. bibcode:1991phrvl..67..789o. doi:10.1103physrevlett.67.789. pmid 10044989. burko, lior m. (1997). structure of the black holes cauchy-horizon singularity. physical review letters. 79 (25): 49584961. arxiv:gr-qc9710112. bibcode:1997phrvl..79.4958b. doi:10.1103physrevlett.79.4958. burko, lior m.; khanna, gaurav; zenginolu, anl (2016). cauchy-horizon singularity inside perturbed kerr black holes. physical review d. 93 (4) 041501. arxiv:1601.05120. bibcode:2016phrvd..93d1501b. doi:10.1103physrevd.93.041501. hamilton, andrew j. s. (2017). mass inflation followed by belinskii-khalatnikov-lifshitz collapse inside accreting, rotating black holes. physical review d. 96 (8) 084041. arxiv:1703.01921. bibcode:2017phrvd..96h4041h. doi:10.1103physrevd.96.084041. barcel, carlos; boyanov, valentin; carballo-rubio, ral; garay, luis j. (2022). classical mass inflation versus semiclassical inner horizon inflation. physical review d. 106 (12) 124006. arxiv:2203.13539. bibcode:2022phrvd.106l4006b. doi:10.1103physrevd.106.124006. a b c d crowther, karen; de haro, sebastian (29 september 2022). four attitudes towards singularities in the search for a theory of quantum gravity. in vassallo, antonio (ed.). the foundations of spacetime physics. new york. arxiv:2208.05946. doi:10.43249781003219019. isbn 978-1-003-21901-9. alesci, emanuele; bahrami, sina; pranzetti, daniele (2019). quantum gravity predictions for black"
  },
  {
    "chunk_id": 160,
    "doc_id": "Black_hole.txt",
    "text": "c d crowther, karen; de haro, sebastian (29 september 2022). four attitudes towards singularities in the search for a theory of quantum gravity. in vassallo, antonio (ed.). the foundations of spacetime physics. new york. arxiv:2208.05946. doi:10.43249781003219019. isbn 978-1-003-21901-9. alesci, emanuele; bahrami, sina; pranzetti, daniele (2019). quantum gravity predictions for black hole interior geometry. physics letters b. 797 134908. arxiv:1904.12412. bibcode:2019phlb..79734908a. doi:10.1016j.physletb.2019.134908. koshelev, alexey s.; tokareva, anna (2025). nonperturbative quantum gravity denounces singular black holes. physical review d. 111 (8) 086026. bibcode:2025phrvd.111h6026k. doi:10.1103physrevd.111.086026. olmedo, javier; saini, sahil; singh, parampreet (2017). from black holes to white holes: a quantum gravitational, symmetric bounce. classical and quantum gravity. 34 (22). arxiv:1707.07333. bibcode:2017cqgra..34v5011o. doi:10.10881361-6382aa8da8. misner, charles w. (1969). absolute zero of time. physical review. 186 (5): 13281333. bibcode:1969phrv..186.1328m. doi:10.1103physrev.186.1328. doran, rosa; lobo, francisco s. n.; crawford, paulo (2008). interior of a schwarzschild black hole revisited. foundations of physics. 38 (2): 160187. arxiv:gr-qc0609042. bibcode:2008foph...38..160d. doi:10.1007s10701-007-9197-6. hawking, s. w.; penrose, r. (1970). the singularities of gravitational collapse and cosmology. proceedings of the royal society of london. a. mathematical and physical sciences. 314 (1519): 529548. bibcode:1970rspsa.314..529h. doi:10.1098rspa.1970.0021. sizes of black holes? how big is a black hole?. sky telescope. 22 july 2014. archived from the original on 3 april 2019. retrieved 9 october 2018. lewis, g. f.; kwan, j. (2007). no way back: maximizing survival time below the schwarzschild event horizon. publications of the astronomical society of australia. 24 (2): 4652. arxiv:0705.1029. bibcode:2007pasa...24...46l. doi:10.1071as07012. s2cid 17261076. toporensky, alexei; popov, sergei (2023). how to delay death and look further into the future if you fall into a black hole. resonance. 28 (5): 737749. doi:10.1007s12045-023-1602-8. belinskii, v.a.; lifshitz, e.m.; khalatnikov, i.m.; agyei, a.k. (1992). the oscillatory mode of approach to a singularity in homogeneous cosmological models with rotating axes. perspectives in theoretical physics. pp. 677689. doi:10.1016b978-0-08-036364-6.50048-x. isbn 978-0-08-036364-6. garfinkle, david (2007). of singularities and breadmaking. einstein online. archived from the original on 9 july 2025. retrieved 14 october 2025. droz, s.; israel, w.; morsink, s. m. (1996). black holes: the inside story. physics world. 9 (1): 3437. bibcode:1996phyw....9...34d. doi:10.10882058-70589126. a b thorne, kip s. (1993). closed timelike curves (pdf). general relativity and gravitation. lan, chen; yang, hao; guo, yang; miao, yan-gang (2023). regular black holes: a short topic review. international journal of theoretical physics. 62 (9) 202. arxiv:2303.11696. bibcode:2023ijtp...62..202l. doi:10.1007s10773-023-05454-1. olmo, gonzalo; rubiera-garcia, diego (2015). nonsingular black holes in (r) theories. universe. 1 (2): 173185. arxiv:1509.02430. bibcode:2015univ....1..173o. doi:10.3390universe1020173. mathur, samir"
  },
  {
    "chunk_id": 161,
    "doc_id": "Black_hole.txt",
    "text": "relativity and gravitation. lan, chen; yang, hao; guo, yang; miao, yan-gang (2023). regular black holes: a short topic review. international journal of theoretical physics. 62 (9) 202. arxiv:2303.11696. bibcode:2023ijtp...62..202l. doi:10.1007s10773-023-05454-1. olmo, gonzalo; rubiera-garcia, diego (2015). nonsingular black holes in (r) theories. universe. 1 (2): 173185. arxiv:1509.02430. bibcode:2015univ....1..173o. doi:10.3390universe1020173. mathur, samir d. (2005). the fuzzball proposal for black holes: an elementary review. fortschritte der physik. 53 (78): 793. arxiv:hep-th0502050. bibcode:2005forph..53..793m. doi:10.1002prop.200410203. s2cid 15083147. avery, steven g.; chowdhury, borun d.; puhm, andrea (2013). unitarity and fuzzball complementarity: alice fuzzes but may not even know it!. journal of high energy physics (9) 12. arxiv:1210.6996. bibcode:2013jhep...09..012a. doi:10.1007jhep09(2013)012. bojowald, martin (2020). black-hole models in loop quantum gravity. universe. 6 (8): 125. arxiv:2009.13565. bibcode:2020univ....6..125b. doi:10.3390universe6080125. woosley, s. e.; heger, a.; weaver, t. a. (7 november 2002). the evolution and explosion of massive stars. reviews of modern physics. 74 (4): 10151071. bibcode:2002rvmp...74.1015w. doi:10.1103revmodphys.74.1015. issn 0034-6861. zappa, francesco; bernuzzi, sebastiano; pannarale, francesco; mapelli, michela; giacobbo, nicola (25 july 2019). black-hole remnants from black-holeneutron-star mergers. physical review letters. 123 (4) 041102. arxiv:1903.11622. bibcode:2019phrvl.123d1102z. doi:10.1103physrevlett.123.041102. issn 0031-9007. pmid 31491270. inayoshi, kohei; visbal, eli; haiman, zoltn (18 august 2020). the assembly of the first massive black holes. annual review of astronomy and astrophysics. 58: 2797. arxiv:1911.05791. bibcode:2020araa..58...27i. doi:10.1146annurev-astro-120419-014455. issn 0066-4146. ripped apart by a black hole. eso (press release). 17 july 2013. archived from the original on 21 july 2013. retrieved 19 july 2013. janka, h.; langanke, k.; marek, a.; martinezpinedo, g.; muller, b. (2007). theory of core-collapse supernovae. physics reports. 442 (16): 3874. arxiv:astro-ph0612072. bibcode:2007phr...442...38j. doi:10.1016j.physrep.2007.02.002. fryer, chris l.; holz, daniel e.; hughes, scott a. (2002). gravitational wave emission from core collapse of massive stars. the astrophysical journal. 565 (1): 430446. arxiv:astro-ph0106113. bibcode:2002apj...565..430f. doi:10.1086324034. penrose, r. (2002). gravitational collapse: the role of general relativity (pdf). general relativity and gravitation. 34 (7): 1141. bibcode:2002gregr..34.1141p. doi:10.1023a:1016578408204. s2cid 117459073. archived from the original (pdf) on 26 may 2013. baados, eduardo; venemans, bram p.; mazzucchelli, chiara; farina, emanuele p.; walter, fabian; wang, feige; decarli, roberto; stern, daniel; fan, xiaohui; davies, frederick b.; hennawi, joseph f. (1 january 2018). an 800-million-solar-mass black hole in a significantly neutral universe at a redshift of 7.5. nature. 553 (7689): 473476. arxiv:1712.01860. bibcode:2018natur.553..473b. doi:10.1038nature25180. pmid 29211709. s2cid 205263326. boylan-kolchin, michael; weisz, daniel r. (2021). uncertain times: the redshifttime relation from cosmology and stars. monthly notices of the royal astronomical society. 505 (2): 27642783. doi:10.1093mnrasstab1521. klessen, ralf s.; glover, simon"
  },
  {
    "chunk_id": 162,
    "doc_id": "Black_hole.txt",
    "text": "significantly neutral universe at a redshift of 7.5. nature. 553 (7689): 473476. arxiv:1712.01860. bibcode:2018natur.553..473b. doi:10.1038nature25180. pmid 29211709. s2cid 205263326. boylan-kolchin, michael; weisz, daniel r. (2021). uncertain times: the redshifttime relation from cosmology and stars. monthly notices of the royal astronomical society. 505 (2): 27642783. doi:10.1093mnrasstab1521. klessen, ralf s.; glover, simon c. o. (18 august 2023). the first stars: formation, properties, and impact. annual review of astronomy and astrophysics. 61: 65130. arxiv:2303.12500. bibcode:2023araa..61...65k. doi:10.1146annurev-astro-071221-053453. issn 0066-4146. fryer, chris l.; kalogera, vassiliki (10 june 2001). theoretical black hole mass distributions. the astrophysical journal. 554 (1): 548560. bibcode:2001apj...554..548f. doi:10.1086321359. issn 0004-637x. yoo, chul-moon (2022). the basics of primordial black hole formation and abundance estimation. galaxies. 10 (6): 112. arxiv:2211.13512. bibcode:2022galax..10..112y. doi:10.3390galaxies10060112. balzer, ashley (7 may 2024). primordial black holes. nasa svs. archived from the original on 27 august 2025. retrieved 23 november 2025. carr, bernard (26 november 2025). primordial black holes: do they exist and are they useful?. 59th yamada conference on inflating horizon of particle astrophysics and cosmology. arxiv:astro-ph0511743. pacucci, fabio; ferrara, andrea; grazian, andrea; fiore, fabrizio; giallongo, emanuele; puccetti, simonetta (2005). first identification of direct collapse black hole candidates in the early universe in candelsgoods-s. monthly notices of the royal astronomical society. 459 (2). universal academy press: astroph0511743. arxiv:astro-ph0511743. bibcode:2005astro.ph.11743c. doi:10.1093mnrasstw725. isbn 978-4-946443-94-7. philip gibbs. is the big bang a black hole?. john baez. archived from the original on 31 december 2018. retrieved 16 march 2018. sutter, paul (21 june 2023). why didnt the infant universe collapse into a black hole?. space.com. archived from the original on 25 march 2025. retrieved 24 november 2025. musser, george (22 september 2003). according to the big bang theory, all the matter in the universe erupted from a singularity. why didnt all this mattercheek by jowl as it wasimmediately collapse into a black hole?. scientific american. archived from the original on 26 april 2025. retrieved 24 november 2025. kaloper, nemanja; terning, john (2007). how black holes form in high energy collisions. general relativity and gravitation. 39 (10): 15251532. arxiv:0705.0408. bibcode:2007gregr..39.1525k. doi:10.1007s10714-007-0468-5. giddings, s. b.; thomas, s. (2002). high energy colliders as black hole factories: the end of short distance physics. physical review d. 65 (5) 056010. arxiv:hep-ph0106219. bibcode:2002phrvd..65e6010g. doi:10.1103physrevd.65.056010. s2cid 1203487. lhc safety assessment group (2008). review of the safety of lhc collisions (pdf). journal of physics g: nuclear physics. 35 (11) 115004. arxiv:0806.3414. bibcode:2008jphg...35k5004e. doi:10.10880954-38993511115004. s2cid 53370175. archived (pdf) from the original on 14"
  },
  {
    "chunk_id": 163,
    "doc_id": "Black_hole.txt",
    "text": "end of short distance physics. physical review d. 65 (5) 056010. arxiv:hep-ph0106219. bibcode:2002phrvd..65e6010g. doi:10.1103physrevd.65.056010. s2cid 1203487. lhc safety assessment group (2008). review of the safety of lhc collisions (pdf). journal of physics g: nuclear physics. 35 (11) 115004. arxiv:0806.3414. bibcode:2008jphg...35k5004e. doi:10.10880954-38993511115004. s2cid 53370175. archived (pdf) from the original on 14 april 2010. cavagli, m. (2010). particle accelerators as black hole factories?. einstein-online. 4: 1010. archived from the original on 8 may 2013. retrieved 8 may 2013. vesperini, e.; mcmillan, s. l. w.; dercole, a.; et al. (2010). intermediate-mass black holes in early globular clusters. the astrophysical journal letters. 713 (1): l41l44. arxiv:1003.3470. bibcode:2010apj...713l..41v. doi:10.10882041-82057131l41. s2cid 119120429. zwart, s. f. p.; baumgardt, h.; hut, p.; et al. (2004). formation of massive black holes through runaway collisions in dense young star clusters. nature. 428 (6984): 724726. arxiv:astro-ph0402622. bibcode:2004natur.428..724p. doi:10.1038nature02448. pmid 15085124. s2cid 4408378. oleary, r. m.; rasio, f. a.; fregeau, j. m.; et al. (2006). binary mergers and growth of black holes in dense star clusters. the astrophysical journal. 637 (2): 937951. arxiv:astro-ph0508224. bibcode:2006apj...637..937o. doi:10.1086498446. s2cid 1509957. brightman, m.; bachetti, m.; earnshaw, h. p.; frst, f.; garca, j.; grefenstette, b.; heida, m.; kara, e.; madsen, k. k.; middleton, m. j.; stern, d.; tombesi, f.; walton, d. j. (2019). breaking the limit: super-eddington accretion onto black holes and neutron stars. bulletin of the american astronomical society. 51 (3): 352. arxiv:1903.06844. bibcode:2019baas...51c.352b. a b regan, john a.; downes, turlough p.; volonteri, marta; beckmann, ricarda; lupi, alessandro; trebitsch, maxime; dubois, yohan (2019). super-eddington accretion and feedback from the first massive seed black holes. monthly notices of the royal astronomical society. 486 (3): 38923906. doi:10.1093mnrasstz1045. marziani, paola; garnica luna, karla; floris, alberto; del olmo, ascensin; deconto-machado, alice; buendia-rios, tania m.; negrete, c. alenka; dultzin, deborah (2025). super-eddington accretion in quasars. universe. 11 (2): 69. arxiv:2502.14713. bibcode:2025univ...11...69m. doi:10.3390universe11020069. ryu, taeho; perna, rosalba; haiman, zoltn; ostriker, jeremiah p.; stone, nicholas c. (2018). interactions between multiple supermassive black holes in galactic nuclei: a solution to the final parsec problem. monthly notices of the royal astronomical society. 473 (3): 34103433. doi:10.1093mnrasstx2524. vasiliev, eugene; antonini, fabio; merritt, david (2014). the final-parsec problem in nonspherical galaxies revisited. the astrophysical journal. 785 (2): 163. arxiv:1311.1167. bibcode:2014apj...785..163v. doi:10.10880004-637x7852163. what powers a black holes mighty jets?. science aaas. 19 november 2014. archived from the original on 5 may 2019. retrieved 19 march 2018. a b c d e f g h i celotti, a.; miller,"
  },
  {
    "chunk_id": 164,
    "doc_id": "Black_hole.txt",
    "text": "problem in nonspherical galaxies revisited. the astrophysical journal. 785 (2): 163. arxiv:1311.1167. bibcode:2014apj...785..163v. doi:10.10880004-637x7852163. what powers a black holes mighty jets?. science aaas. 19 november 2014. archived from the original on 5 may 2019. retrieved 19 march 2018. a b c d e f g h i celotti, a.; miller, j. c.; sciama, d. w. (1999). astrophysical evidence for the existence of black holes (pdf). classical and quantum gravity. 16 (12a): a3a21. arxiv:astro-ph9912186. bibcode:1999cqgra..16a...3c. doi:10.10880264-93811612a301. s2cid 17677758. archived from the original (pdf) on 27 july 2018. winter, l. m.; mushotzky, r. f.; reynolds, c. s. (2006). xmm-newton archival study of the ultraluminous x-ray population in nearby galaxies. the astrophysical journal. 649 (2): 730752. arxiv:astro-ph0512480. bibcode:2006apj...649..730w. doi:10.1086506579. s2cid 118445260. evans, charles r.; kochanek, christopher s. (1989). the tidal disruption of a star by a massive black hole. the astrophysical journal. 346: l13. bibcode:1989apj...346l..13e. doi:10.1086185567. komossa, s. (2015). tidal disruption of stars by supermassive black holes: status of observations. journal of high energy astrophysics. 7: 148157. arxiv:1505.01093. bibcode:2015jheap...7..148k. doi:10.1016j.jheap.2015.04.006. hayasaki, kimitake; stone, nicholas; loeb, abraham (2016). circularization of tidally disrupted stars around spinning supermassive black holes. monthly notices of the royal astronomical society. 461 (4): 37603780. doi:10.1093mnrasstw1387. a b page, d. n. (2005). hawking radiation and black hole thermodynamics. new journal of physics. 7 (1): 203. arxiv:hep-th0409024. bibcode:2005njph....7..203p. doi:10.10881367-263071203. s2cid 119047329. smerlak, matteo; singh, suprit (2013). new perspectives on hawking radiation. physical review d. 88 (10) 104023. arxiv:1304.2858. bibcode:2013phrvd..88j4023s. doi:10.1103physrevd.88.104023. ruiz, o.; molina, u.; viloria, p. (2019). thermodynamic analysis of kerr-newman black holes. journal of physics: conference series. 1219 (1) 012016. bibcode:2019jphcs1219a2016r. doi:10.10881742-659612191012016. from this, an expression is established for the hawking temperature of a kerr-newman black hole as a function of its mass , angular moment and load . as the black hole loses mass, its temperature increases inversely proportional. siegel, ethan (2017). ask ethan: do black holes grow faster than they evaporate?. forbes (starts with a bang blog). archived from the original on 22 november 2018. retrieved 17 march 2018. sivaram, c. (2001). black hole hawking radiation may never be observed!. general relativity and gravitation. 33 (2): 175181. bibcode:2001gregr..33..175s. doi:10.1023a:1002753400430. s2cid 118913634. evaporating black holes?. einstein online. max planck institute for gravitational physics. 2010. archived from the original on 22 july 2011. retrieved 12 december 2010. giddings, s. b.; mangano, m. l. (2008). astrophysical implications of hypothetical stable tev-scale black holes. physical review d. 78 (3) 035009. arxiv:0806.3381. bibcode:2008phrvd..78c5009g. doi:10.1103physrevd.78.035009."
  },
  {
    "chunk_id": 165,
    "doc_id": "Black_hole.txt",
    "text": "s2cid 118913634. evaporating black holes?. einstein online. max planck institute for gravitational physics. 2010. archived from the original on 22 july 2011. retrieved 12 december 2010. giddings, s. b.; mangano, m. l. (2008). astrophysical implications of hypothetical stable tev-scale black holes. physical review d. 78 (3) 035009. arxiv:0806.3381. bibcode:2008phrvd..78c5009g. doi:10.1103physrevd.78.035009. s2cid 17240525. peskin, m. e. (2008). the end of the world at the large hadron collider?. physics. 1 14. bibcode:2008phyoj...1...14p. doi:10.1103physics.1.14. fichtel, c. e.; bertsch, d. l.; dingus, b. l.; et al. (1994). search of the energetic gamma-ray experiment telescope (egret) data for high-energy gamma-ray microsecond bursts. astrophysical journal. 434 (2): 557559. bibcode:1994apj...434..557f. doi:10.1086174758. naeye, r. testing fundamental physics. nasa. archived from the original on 31 august 2008. retrieved 16 september 2008. ackermann, m.; et al. (2018). search for gamma-ray emission from local primordial black holes with the fermi large area telescope. the astrophysical journal. 857 (1): 49. bibcode:2018apj...857...49a. doi:10.38471538-4357aaac7b. a b frautschi, s. (1982). entropy in an expanding universe. science. 217 (4560): 593599. bibcode:1982sci...217..593f. doi:10.1126science.217.4560.593. pmid 17817517. s2cid 27717447. see page 596: table 1 and section black hole decay and previous sentence on that page. page, don n. (1976). particle emission rates from a black hole: massless particles from an uncharged, nonrotating hole. physical review d. 13 (2): 198206. bibcode:1976phrvd..13..198p. doi:10.1103physrevd.13.198.. see in particular equation (27). hiscock, william a. (1981). models of evaporating black holes. i. physical review d. 23 (12): 28132822. bibcode:1981phrvd..23.2813h. doi:10.1103physrevd.23.2813. stephens, c. r.; hooft, g t; whiting, b. f. (1994). black hole evaporation without information loss. classical and quantum gravity. 11 (3): 621647. arxiv:gr-qc9310006. bibcode:1994cqgra..11..621s. doi:10.10880264-9381113014. saida, hiromi (2007). black hole evaporation in a heat bath as a nonequilibrium process and its final fate. classical and quantum gravity. 24 (3): 691722. arxiv:gr-qc0701030. bibcode:2007cqgra..24..691s. doi:10.10880264-9381243012. elbert, oliver d.; bullock, james s.; kaplinghat, manoj (1 january 2018). counting black holes: the cosmic stellar remnant population and implications for ligo. monthly notices of the royal astronomical society. 473 (1): 11861194. arxiv:1703.02551. doi:10.1093mnrasstx1959. issn 0035-8711. abramowicz, m. a.; kluniak, w.; lasota, j.-p. (december 2002). no observational proof of the black-hole event-horizon. astronomy astrophysics. 396 (3): l31l34. arxiv:astro-ph0207270. bibcode:2002aa...396l..31a. doi:10.10510004-6361:20021645. issn 0004-6361. event horizon telescope collaboration; akiyama, kazunori; alberdi, antxon; alef, walter; algaba, juan carlos; anantua, richard; asada, keiichi; azulay, rebecca; bach, uwe; baczko, anne-kathrin; ball, david; balokovi, mislav; barrett, john; baubck, michi; benson, bradford a. (may 2022). first sagittarius a event horizon telescope results. vi. testing the black hole"
  },
  {
    "chunk_id": 166,
    "doc_id": "Black_hole.txt",
    "text": "issn 0004-6361. event horizon telescope collaboration; akiyama, kazunori; alberdi, antxon; alef, walter; algaba, juan carlos; anantua, richard; asada, keiichi; azulay, rebecca; bach, uwe; baczko, anne-kathrin; ball, david; balokovi, mislav; barrett, john; baubck, michi; benson, bradford a. (may 2022). first sagittarius a event horizon telescope results. vi. testing the black hole metric. the astrophysical journal letters. 930 (2): l17. bibcode:2022apj...930l..17e. doi:10.38472041-8213ac6756. issn 2041-8205. april 2017 observations. event horizon telescope. archived from the original on 10 april 2019. retrieved 11 april 2019. overbye, dennis (24 january 2024). that famous black hole gets a second look - repeated studies of the supermassive black hole in the galaxy messier 87 confirm that it continues to act as einsteins theory predicted it would. the new york times. archived from the original on 24 january 2024. retrieved 25 january 2024. overbye, dennis (10 april 2019). darkness visible, finally: astronomers capture first ever image of a black hole. the new york times. archived from the original on 21 may 2019. retrieved 11 april 2019. a b black holes: the edge of all we know (motion picture). 2020. 99 minutes in. astronomers reveal the first picture of a black hole. the new york times (video). ap. 10 april 2019. archived from the original on 22 may 2019. retrieved 11 april 2019. doeleman, shep (4 april 2016). the event horizon telescope: imaging and time-resolving a black hole. physics berkeley. event occurs at 46:50. archived from the original on 1 december 2016. retrieved 8 july 2016. a b grossman, lisa; conover, emily (10 april 2019). the first picture of a black hole opens a new era of astrophysics. science news. archived from the original on 27 april 2019. retrieved 11 april 2019. event horizon telescope collaboration (2021). first m87 event horizon telescope results. vii. polarization of the ring. the astrophysical journal. 910 (1): l12. arxiv:2105.01169. bibcode:2021apj...910l..12e. doi:10.38472041-8213abe71d. s2cid 233851995. overbye, dennis (26 april 2023). a fresh view of an increasingly familiar black hole - radio astronomers have captured a wide-angle image of one of the most violent locales in the cosmos. the new york times. archived from the original on 26 april 2023. retrieved 26 april 2023. lu, ru-sen; et al. (26 april 2023). a ring-like accretion structure in m87 connecting its black hole and jet. nature. 616 (7958): 686690. arxiv:2304.13252. bibcode:2023natur.616..686l. doi:10.1038s41586-023-05843-w. pmc 10132962. pmid 37100940. johnson, m. d.; fish, v. l.; doeleman, s. s.; marrone, d. p.; plambeck,"
  },
  {
    "chunk_id": 167,
    "doc_id": "Black_hole.txt",
    "text": "26 april 2023. retrieved 26 april 2023. lu, ru-sen; et al. (26 april 2023). a ring-like accretion structure in m87 connecting its black hole and jet. nature. 616 (7958): 686690. arxiv:2304.13252. bibcode:2023natur.616..686l. doi:10.1038s41586-023-05843-w. pmc 10132962. pmid 37100940. johnson, m. d.; fish, v. l.; doeleman, s. s.; marrone, d. p.; plambeck, r. l.; wardle, j. f. c.; akiyama, k.; asada, k.; beaudoin, c. (4 december 2015). resolved magnetic-field structure and variability near the event horizon of sagittarius a. science. 350 (6265): 12421245. arxiv:1512.01220. bibcode:2015sci...350.1242j. doi:10.1126science.aac7087. pmid 26785487. s2cid 21730194. event horizon telescope reveals magnetic fields at milky ways central black hole. cfa.harvard.edu. 3 december 2015. archived from the original on 31 december 2015. retrieved 12 january 2016. event horizon telescope collaboration; akiyama, kazunori; alberdi, antxon; alef, walter; algaba, juan carlos; anantua, richard; asada, keiichi; azulay, rebecca; bach, uwe; baczko, anne-kathrin; ball, david; balokovi, mislav; barrett, john; baubck, michi; benson, bradford a. (1 may 2022). first sagittarius a event horizon telescope results. i. the shadow of the supermassive black hole in the center of the milky way. the astrophysical journal letters. 930 (2): l12. arxiv:2311.08680. bibcode:2022apj...930l..12e. doi:10.38472041-8213ac6674. issn 2041-8205. the ring usually lies near the gravitationally lensed photon orbits that define the boundary of what we hereafter refer to as the black hole shadow. astronomers reveal first image of the black hole at the heart of our galaxy. eventhorizontelescope.org. 12 may 2022. retrieved 22 june 2022. c. bower, geoffrey (may 2022). focus on first sgr a results from the event horizon telescope. the astrophysical journal letters (20418205). overbye, dennis (11 february 2016). physicists detect gravitational waves, proving einstein right. the new york times. archived from the original on 11 february 2016. retrieved 11 february 2016. a b abbott, benjamin p.; et al. (ligo scientific collaboration virgo collaboration) (11 february 2016). properties of the binary black hole merger gw150914. physical review letters. 116 (24) 241102. arxiv:1602.03840. bibcode:2016phrvl.116x1102a. doi:10.1103physrevlett.116.241102. pmid 27367378. s2cid 217406416. cardoso, v.; franzin, e.; pani, p. (2016). is the gravitational-wave ringdown a probe of the event horizon?. physical review letters. 116 (17) 171101. arxiv:1602.07309. bibcode:2016phrvl.116q1101c. doi:10.1103physrevlett.116.171101. pmid 27176511. s2cid 206273829. abbott, b. p.; et al. (ligo scientific collaboration virgo collaboration) (2016). astrophysical implications of the binary black hole merger gw150914. astrophys. j. lett. 818 (2): l22. arxiv:1602.03846. bibcode:2016apj...818l..22a. doi:10.38472041-82058182l22. hdl:182611732. s2cid 209315965. archived from the original on 16 march 2016. detection of gravitational waves. ligo. archived from the original on 20"
  },
  {
    "chunk_id": 168,
    "doc_id": "Black_hole.txt",
    "text": "b. p.; et al. (ligo scientific collaboration virgo collaboration) (2016). astrophysical implications of the binary black hole merger gw150914. astrophys. j. lett. 818 (2): l22. arxiv:1602.03846. bibcode:2016apj...818l..22a. doi:10.38472041-82058182l22. hdl:182611732. s2cid 209315965. archived from the original on 16 march 2016. detection of gravitational waves. ligo. archived from the original on 20 may 2020. retrieved 9 april 2018. a b gillessen, s.; eisenhauer, f.; trippe, s.; et al. (2009). monitoring stellar orbits around the massive black hole in the galactic center. the astrophysical journal. 692 (2): 10751109. arxiv:0810.4674. bibcode:2009apj...692.1075g. doi:10.10880004-637x69221075. s2cid 1431308. a b ghez, a. m.; klein, b. l.; morris, m.; et al. (1998). high proper-motion stars in the vicinity of sagittarius a: evidence for a supermassive black hole at the center of our galaxy. the astrophysical journal. 509 (2): 678686. arxiv:astro-ph9807210. bibcode:1998apj...509..678g. doi:10.1086306528. s2cid 18243528. broderick, avery; loeb, abraham; narayan, ramesh (august 2009). the event horizon of sagittarius a. the astrophysical journal. 701 (2): 13571366. arxiv:0903.1105. bibcode:2009apj...701.1357b. doi:10.10880004-637x70121357. s2cid 12991878. astronomers reveal first image of the black hole at the heart of our galaxy. event horizon telescope. 12 may 2022. archived from the original on 26 september 2025. retrieved 2 december 2025. schatz, h.; rehm, k.e. (2006). x-ray binaries. nuclear physics a. 777: 601622. arxiv:astro-ph0607624. bibcode:2006nupha.777..601s. doi:10.1016j.nuclphysa.2005.05.200. quirrenbach, andreas; frink, sabine; tomsick, john (1 december 2004). masses and luminosities of x-ray binaries (pdf). sim planetquest: science with the space interferometry mission. national aeronautics and space administration. bibcode:2002swsi.conf...33q. cho, adrian (2018). a weight limit emerges for neutron stars. science. 359 (6377): 724725. bibcode:2018sci...359..724c. doi:10.1126science.359.6377.724. pmid 29449468. rolston, b. (10 november 1997). the first black hole. the bulletin. university of toronto. archived from the original on 2 may 2008. retrieved 11 march 2008. orosz, jerome a.; mcclintock, jeffrey e.; aufdenberg, jason p.; remillard, ronald a.; reid, mark j.; narayan, ramesh; gou, lijun (9 november 2011). the mass of the black hole in cygnus x-1. the astrophysical journal. 742 (2): 84. arxiv:1106.3689. bibcode:2011apj...742...84o. doi:10.10880004-637x742284. issn 0004-637x. corral-santana, j. m.; casares, j.; muoz-darias, t.; bauer, f. e.; martnez-pais, i. g.; russell, d. m. (1 march 2016). blackcat: a catalogue of stellar-mass black holes in x-ray transients. astronomy astrophysics. 587: a61. arxiv:1510.08869. bibcode:2016aa...587a..61c. doi:10.10510004-6361201527130. issn 0004-6361. broekgaarden, floor s.; berger, edo (2021). formation of the first two black holeneutron star mergers (gw200115 and gw200105) from isolated binary evolution. the astrophysical journal letters. 920 (1): l13. arxiv:2108.05763. bibcode:2021apj...920l..13b. doi:10.38472041-8213ac2832. chattopadhyay, debatri; stevenson, simon; broekgaarden, floor; antonini,"
  },
  {
    "chunk_id": 169,
    "doc_id": "Black_hole.txt",
    "text": "x-ray transients. astronomy astrophysics. 587: a61. arxiv:1510.08869. bibcode:2016aa...587a..61c. doi:10.10510004-6361201527130. issn 0004-6361. broekgaarden, floor s.; berger, edo (2021). formation of the first two black holeneutron star mergers (gw200115 and gw200105) from isolated binary evolution. the astrophysical journal letters. 920 (1): l13. arxiv:2108.05763. bibcode:2021apj...920l..13b. doi:10.38472041-8213ac2832. chattopadhyay, debatri; stevenson, simon; broekgaarden, floor; antonini, fabio; belczynski, krzysztof (2022). modelling the formation of the first two neutron starblack hole mergers, gw200105 and gw200115: metallicity, chirp masses, and merger remnant spins. monthly notices of the royal astronomical society. 513 (4): 57805789. doi:10.1093mnrasstac1283. ziosi, b. m.; mapelli, m.; branchesi, m.; tormen, g. (2014). dynamics of stellar black holes in young star clusters with different metallicities - ii. black hole-black hole binaries. monthly notices of the royal astronomical society. 441 (4): 37033717. arxiv:1404.7147. doi:10.1093mnrasstu824. sources and types of gravitational waves. ligo caltech. retrieved 26 october 2025. cattaneo, a.; faber, s. m.; binney, j.; dekel, a.; kormendy, j.; mushotzky, r.; babul, a.; best, p. n.; brggen, m.; fabian, a. c.; frenk, c. s.; khalatyan, a.; netzer, h.; mahdavi, a.; silk, j. (july 2009). the role of black holes in galaxy formation and evolution. nature. 460 (7252): 213219. arxiv:0907.1608. bibcode:2009natur.460..213c. doi:10.1038nature08135. issn 0028-0836. pmid 19587763. king, a. (2003). black holes, galaxy formation, and the mbh- relation. the astrophysical journal letters. 596 (1): 2729. arxiv:astro-ph0308342. bibcode:2003apj...596l..27k. doi:10.1086379143. s2cid 9507887. ferrarese, l.; merritt, d. (2000). a fundamental relation between supermassive black holes and their host galaxies. the astrophysical journal letters. 539 (1): 912. arxiv:astro-ph0006053. bibcode:2000apj...539l...9f. doi:10.1086312838. s2cid 6508110. chou, felicia; anderson, janet; watzke, megan (5 january 2015). release 15-001nasas chandra detects record-breaking outburst from milky ways black hole. nasa. archived from the original on 6 january 2015. retrieved 6 january 2015. krolik, j. h. (1999). active galactic nuclei. princeton university press. ch. 1.2. isbn 978-0-691-01151-6. archived from the original on 14 august 2021. retrieved 16 october 2020. sparke, l. s.; gallagher, j. s. (2000). galaxies in the universe: an introduction. cambridge university press. ch. 9.1. isbn 978-0-521-59740-1. archived from the original on 22 march 2022. retrieved 16 october 2020. marconi, a.; risaliti, g.; gilli, r.; hunt, l. k.; maiolino, r.; salvati, m. (2004). local supermassive black holes, relics of active galactic nuclei and the x-ray background. monthly notices of the royal astronomical society. 351 (1): 169185. arxiv:astro-ph0311619. bibcode:2004mnras.351..169m. doi:10.1111j.1365-2966.2004.07765.x. kormendy, j.; richstone, d. (1995). inward boundthe search for supermassive black holes in galactic nuclei. annual review of astronomy and astrophysics. 33 (1):"
  },
  {
    "chunk_id": 170,
    "doc_id": "Black_hole.txt",
    "text": "(2004). local supermassive black holes, relics of active galactic nuclei and the x-ray background. monthly notices of the royal astronomical society. 351 (1): 169185. arxiv:astro-ph0311619. bibcode:2004mnras.351..169m. doi:10.1111j.1365-2966.2004.07765.x. kormendy, j.; richstone, d. (1995). inward boundthe search for supermassive black holes in galactic nuclei. annual review of astronomy and astrophysics. 33 (1): 581624. bibcode:1995araa..33..581k. doi:10.1146annurev.aa.33.090195.003053. melia, fulvio; falcke, heino (2001). the supermassive black hole at the galactic center. annual review of astronomy and astrophysics. 39: 309352. arxiv:astro-ph0106162. bibcode:2001araa..39..309m. doi:10.1146annurev.astro.39.1.309. a b nasa scientists identify smallest known black hole (press release). goddard space flight center. 1 april 2008. archived from the original on 27 december 2008. retrieved 14 march 2009. a b c ingram, adam r.; motta, sara e. (2019). a review of quasi-periodic oscillations from black hole x-ray binaries: observation and theory. new astronomy reviews. 85 101524. arxiv:2001.08758. bibcode:2019newar..8501524i. doi:10.1016j.newar.2020.101524. wagoner, robert v.; silbergleit, alexander s.; ortega-rodrguez, manuel (2001). stable quasi-periodic oscillations and black hole properties from diskoseismology. the astrophysical journal. 559 (1): l25l28. arxiv:astro-ph0107168. bibcode:2001apj...559l..25w. doi:10.1086323655. wambsganss, joachim (1998). gravitational lensing in astronomy. living reviews in relativity. 1 (1) 12. arxiv:astro-ph9812021. bibcode:1998lrr.....1...12w. doi:10.12942lrr-1998-12. pmc 5567250. pmid 28937183. bozza, v.; mancini, l. (2005). gravitational lensing of stars in the central arcsecond of our galaxy. the astrophysical journal. 627 (2): 790802. arxiv:astro-ph0503664. bibcode:2005apj...627..790b. doi:10.1086430664. wambsganss, j. (2006). gravitational microlensing. gravitational lensing: strong, weak and micro. saas-fee advanced courses. vol. 33. pp. 453540. arxiv:astro-ph0604278. doi:10.1007978-3-540-30310-74. isbn 978-3-540-30309-1. mao, shude (2012). astrophysical applications of gravitational microlensing. research in astronomy and astrophysics. 12 (8): 947972. arxiv:1207.3720. bibcode:2012raa....12..947m. doi:10.10881674-4527128005. bennett, d. p.; becker, a. c.; quinn, j. l.; tomaney, a. b.; alcock, c.; allsman, r. a.; alves, d. r.; axelrod, t. s.; calitz, j. j.; cook, k. h.; drake, a. j.; fragile, p. c.; freeman, k. c.; geha, m.; griest, k. (1 november 2002). gravitational microlensing events due to stellar-mass black holes. the astrophysical journal. 579 (2): 639659. arxiv:astro-ph0109467. bibcode:2002apj...579..639b. doi:10.1086342225. issn 0004-637x. mao, shude; smith, martin c.; woniak, p.; udalski, a.; szymaski, m.; kubiak, m.; pietrzyski, g.; soszyski, i.; ebru, k. (1 january 2002). optical gravitational lensing experiment ogle-1999-bul-32: the longest ever microlensing event - evidence for a stellar mass black hole?. monthly notices of the royal astronomical society. 329 (2): 349354. arxiv:astro-ph0108312. bibcode:2002mnras.329..349m. doi:10.1046j.1365-8711.2002.04986.x. issn 0035-8711. a b sahu, k. c. (2022). an isolated stellar-mass black hole detected through astrometric microlensing. astrophysical journal. 933 (1): 83. arxiv:2201.13296. bibcode:2022apj...933...83s. doi:10.38471538-4357ac739e. s2cid 246430448. lam, casey y.; lu,"
  },
  {
    "chunk_id": 171,
    "doc_id": "Black_hole.txt",
    "text": "for a stellar mass black hole?. monthly notices of the royal astronomical society. 329 (2): 349354. arxiv:astro-ph0108312. bibcode:2002mnras.329..349m. doi:10.1046j.1365-8711.2002.04986.x. issn 0035-8711. a b sahu, k. c. (2022). an isolated stellar-mass black hole detected through astrometric microlensing. astrophysical journal. 933 (1): 83. arxiv:2201.13296. bibcode:2022apj...933...83s. doi:10.38471538-4357ac739e. s2cid 246430448. lam, casey y.; lu, jessica r. (1 october 2023). a reanalysis of the isolated black hole candidate ogle-2011-blg-0462moa-2011-blg-191. the astrophysical journal. 955 (2): 116. arxiv:2308.03302. bibcode:2023apj...955..116l. doi:10.38471538-4357aced4a. issn 0004-637x. mereghetti, sandro; sidoli, lara; ponti, gabriele; treves, aldo (23 july 2025). deep chandra x-ray observation of the isolated black hole ogle-2011-blg-0462. monthly notices of the royal astronomical society staf1211. doi:10.1093mnrasstaf1211. issn 0035-8711. kovacs, z.; cheng, k. s.; harko, t. (2009). can stellar mass black holes be quark stars?. monthly notices of the royal astronomical society. 400 (3): 16321642. arxiv:0908.2672. bibcode:2009mnras.400.1632k. doi:10.1111j.1365-2966.2009.15571.x. s2cid 18263809. a b bonkowsky, charles (5 january 2025). between neutron stars and black holes. columbia science review. retrieved 6 december 2025. sotani, hajime; kohri, kazunori; harada, tomohiro (2004). restricting quark matter models by gravitational wave observation. physical review d. 69 (8) 084008. arxiv:gr-qc0310079. bibcode:2004phrvd..69h4008s. doi:10.1103physrevd.69.084008. dai, de-chang; lue, arthur; starkman, glenn; stojkovic, dejan (2010). electroweak stars: how nature may capitalize on the standard models ultimate fuel. journal of cosmology and astroparticle physics (12): 004. arxiv:0912.0520. bibcode:2010jcap...12..004d. doi:10.10881475-7516201012004. hansson, j.; sandin, f. (2005). preon stars: a new class of cosmic compact objects. physics letters b. 616 (12): 17. arxiv:astro-ph0410417. bibcode:2005phlb..616....1h. doi:10.1016j.physletb.2005.04.034. s2cid 119063004. murk, sebastian (2023). nomen non est omen: why it is too soon to identify ultra-compact objects as black holes. international journal of modern physics d. 32 (14) 2342012: 23420122342235. arxiv:2210.03750. bibcode:2023ijmpd..3242012m. doi:10.1142s0218271823420129. s2cid 252781040. bagheri tudeshki, a.; bordbar, g.h.; eslam panah, b. (2022). dark energy star in gravitys rainbow. physics letters b. 835 137523. arxiv:2208.07063. bibcode:2022phlb..83537523b. doi:10.1016j.physletb.2022.137523. ball, philip (31 march 2005). black holes do not exist. nature. doi:10.1038news050328-8. barcel, carlos; liberati, stefano; sonego, sebastiano; visser, matt (2008). fate of gravitational collapse in semiclassical gravity. physical review d. 77 (4) 044032. arxiv:0712.1130. bibcode:2008phrvd..77d4032b. doi:10.1103physrevd.77.044032. ray, saibal; sengupta, rikpratik; nimesh, himanshu (2020). gravastar: an alternative to black hole. international journal of modern physics d. 29 (5): 20300042030260. bibcode:2020ijmpd..2930004r. doi:10.1142s0218271820300049. mazur, pawel o.; mottola, emil (2004). gravitational vacuum condensate stars. proceedings of the national academy of sciences of the united states of america. 101 (26): 95459550. arxiv:gr-qc0407075. bibcode:2004pnas..101.9545m. doi:10.1073pnas.0402717101. pmc 470711. pmid 15210982. s2cid 2607263. jampolski, daniel; rezzolla, luciano (2024). nested solutions of"
  },
  {
    "chunk_id": 172,
    "doc_id": "Black_hole.txt",
    "text": "physics d. 29 (5): 20300042030260. bibcode:2020ijmpd..2930004r. doi:10.1142s0218271820300049. mazur, pawel o.; mottola, emil (2004). gravitational vacuum condensate stars. proceedings of the national academy of sciences of the united states of america. 101 (26): 95459550. arxiv:gr-qc0407075. bibcode:2004pnas..101.9545m. doi:10.1073pnas.0402717101. pmc 470711. pmid 15210982. s2cid 2607263. jampolski, daniel; rezzolla, luciano (2024). nested solutions of gravitational condensate stars. classical and quantum gravity. 41 (6). arxiv:2310.13946. bibcode:2024cqgra..41f5014j. doi:10.10881361-6382ad2317. reid, m. j.; brunthaler, a. (2020). the proper motion of sagittarius a. iii. the case for a supermassive black hole. the astrophysical journal. 892 (1): 39. arxiv:2001.04386. bibcode:2020apj...892...39r. doi:10.38471538-4357ab76cd. maoz, eyal (1998). dynamical constraints on alternatives to supermassive black holes in galactic nuclei. the astrophysical journal. 494 (2): l181l184. arxiv:astro-ph9710309. bibcode:1998apj...494l.181m. doi:10.1086311194. miller, m. coleman (2006). constraints on alternatives to supermassive black holes. monthly notices of the royal astronomical society: letters. 367 (1): l32l36. arxiv:astro-ph0512194. bibcode:2006mnras.367l..32m. doi:10.1111j.1745-3933.2006.00135.x. hawking, s. w. does god play dice?. www.hawking.org.uk. archived from the original on 11 january 2012. retrieved 14 march 2009. anderson, warren g. (1996). the black hole information loss problem. usenet physics faq. archived from the original on 22 january 2009. retrieved 24 march 2009. a b preskill, j. (21 october 1994). black holes and information: a crisis in quantum physics (pdf). caltech theory seminar. archived from the original (pdf) on 18 may 2008. retrieved 17 may 2009. bose, sougato; fuentes, ivette; geraci, andrew a.; khan, saba mehsar; qvarfort, sofia; rademacher, markus; rashid, muddassar; toro, marko; ulbricht, hendrik; wanjura, clara c. (13 february 2025). massive quantum systems as interfaces of quantum mechanics and gravity. reviews of modern physics. 97 (1) 015003. arxiv:2311.09218. bibcode:2025rvmp...97a5003b. doi:10.1103revmodphys.97.015003. issn 0034-6861. raju, suvrat (january 2022). lessons from the information paradox. physics reports. 943: 180. arxiv:2012.05770. bibcode:2022phr...943....1r. doi:10.1016j.physrep.2021.10.001. wang, feige; yang, jinyi; fan, xiaohui; hennawi, joseph f.; barth, aaron j.; banados, eduardo; bian, fuyan; boutsia, konstantina; connor, thomas; davies, frederick b.; decarli, roberto; eilers, anna-christina; farina, emanuele paolo; green, richard; jiang, linhua; li, jiang-tao; mazzucchelli, chiara; nanni, riccardo; schindler, jan-torge; venemans, bram; walter, fabian; wu, xue-bing; yue, minghao (2021). a luminous quasar at redshift 7.642. the astrophysical journal letters. 907 (1): l1. arxiv:2101.03179. bibcode:2021apj...907l...1w. doi:10.38472041-8213abd8c6. mortlock, daniel j.; warren, stephen j.; venemans, bram p.; patel, mitesh; hewett, paul c.; mcmahon, richard g.; simpson, chris; theuns, tom; gonzles-solares, eduardo a.; adamson, andy; dye, simon; hambly, nigel c.; hirst, paul; irwin, mike j.; kuiper, ernst; lawrence, andy; rttgering, huub j. a. (2011). a luminous quasar at a redshift of"
  },
  {
    "chunk_id": 173,
    "doc_id": "Black_hole.txt",
    "text": "j.; warren, stephen j.; venemans, bram p.; patel, mitesh; hewett, paul c.; mcmahon, richard g.; simpson, chris; theuns, tom; gonzles-solares, eduardo a.; adamson, andy; dye, simon; hambly, nigel c.; hirst, paul; irwin, mike j.; kuiper, ernst; lawrence, andy; rttgering, huub j. a. (2011). a luminous quasar at a redshift of z 7.085. nature. 474 (7353): 616619. arxiv:1106.6088. bibcode:2011natur.474..616m. doi:10.1038nature10159. pmid 21720366. salvestrini, francesco; feruglio, chiara; tripodi, roberta; fontanot, fabio; bischetti, manuela; de lucia, gabriella; fiore, fabrizio; hirschmann, michaela; maio, umberto; piconcelli, enrico; saccheo, ivano; tortosa, alessia; valiante, rosa; xie, lizhi; zappacosta, luca (2025). molecular gas and dust properties in z 7 quasar hosts. astronomy astrophysics. 695: a23. arxiv:2412.02688. bibcode:2025aa...695a..23s. doi:10.10510004-6361202453226. trenti, m.; stiavelli, m. (2007). distribution of the very first population iii stars and their relation to bright z 6 quasars. the astrophysical journal. 667 (1): 3848. arxiv:0705.3843. bibcode:2007apj...667...38t. doi:10.1086520502. singh, jasbir; monaco, pierluigi; tan, jonathan c. (2023). the formation of supermassive black holes from population iii.1 seeds. ii. evolution to the local universe. monthly notices of the royal astronomical society. 525: 969982. doi:10.1093mnrasstad2346. a b smith, aaron; bromm, volker (2019). supermassive black holes in the early universe. contemporary physics. 60 (2): 111126. arxiv:1904.12890. bibcode:2019conph..60..111s. doi:10.108000107514.2019.1615715. jeon, myoungwon; pawlik, andreas h.; bromm, volker; milosavljevi, milo (2014). radiative feedback from high-mass x-ray binaries on the formation of the first galaxies and early reionization. monthly notices of the royal astronomical society. 440 (4): 37783796. doi:10.1093mnrasstu444. miralda-escud, jaiyul yoo jordi; miralda-escud, jordi (2004). formation of the black holes in the highest redshift quasars. the astrophysical journal. 614 (1): l25l28. arxiv:astro-ph0406217. bibcode:2004apj...614l..25y. doi:10.1086425416. trakhtenbrot, benny (2019). what do observations tell us about the highest-redshift supermassive black holes?. proceedings of the international astronomical union. 15: 261275. arxiv:2002.00972. doi:10.1017s1743921320003087. mayer, lucio; bonoli, silvia (2019). the route to massive black hole formation via merger-driven direct collapse: a review. reports on progress in physics. 82 (1): 016901. arxiv:1803.06391. bibcode:2019rpph...82a6901m. doi:10.10881361-6633aad6a5. pmid 30057369. agarwal, bhaskar; dalla vecchia, claudio; johnson, jarrett l.; khochfar, sadegh; paardekooper, jan-pieter (2014). the first billion years project: birthplaces of direct collapse black holes. monthly notices of the royal astronomical society. 443: 648657. doi:10.1093mnrasstu1112. shinohara, takumi; he, wanqiu; matsuoka, yoshiki; nagao, tohru; suyama, teruaki; takahashi, tomo (2023). supermassive primordial black holes: a view from clustering of quasars at displaystyle zsim 6. physical review d. 108 (6) 063510. doi:10.1103physrevd.108.063510. mayer, lucio (2019). super-eddington accretion; flow regimes and conditions in high-z galaxies. formation of the first black holes."
  },
  {
    "chunk_id": 174,
    "doc_id": "Black_hole.txt",
    "text": "he, wanqiu; matsuoka, yoshiki; nagao, tohru; suyama, teruaki; takahashi, tomo (2023). supermassive primordial black holes: a view from clustering of quasars at displaystyle zsim 6. physical review d. 108 (6) 063510. doi:10.1103physrevd.108.063510. mayer, lucio (2019). super-eddington accretion; flow regimes and conditions in high-z galaxies. formation of the first black holes. pp. 195222. arxiv:1807.06243. doi:10.114297898132279580011. isbn 978-981-322-794-1. westfahl, gary (2021). black holes. science fiction literature through history: an encyclopedia. abc-clio. pp. 159162. isbn 978-1-4408-6617-3. a b johnson, david kyle (19 june 2019). understanding black holes through science fiction. sci phi journal. retrieved 20 december 2025. a b rodriguez, mario. blame it on the black star: black holes in culture. iafor journal of cultural studies. 8 (2) via researchgate. a b stableford, brian (2006). black hole. science fact and science fiction: an encyclopedia. taylor francis. pp. 6567. isbn 978-0-415-97460-8. langford, david (2005). black holes. in westfahl, gary (ed.). the greenwood encyclopedia of science fiction and fantasy: themes, works, and wonders. greenwood publishing group. pp. 8991. isbn 978-0-313-32951-7. a b fraknoi, andrew (january 2024). science fiction stories with good astronomy physics: a topical index (pdf). astronomical society of the pacific (7.3 ed.). pp. 34. archived (pdf) from the original on 10 february 2024. retrieved 21 june 2024. further reading popular reading begelman, mitchell c.; rees, martin j. (2021). gravitys fatal attraction: black holes in the universe (3rd ed.). cambridge, united kingdom ; new york, ny: cambridge university press. isbn 978-1-108-87112-9. cox, brian; forshaw, jeff (2022). black holes: the key to understanding the universe. william collins. isbn 978-0-00-859706-1. ferguson, kitty (1991). black holes in space-time. watts franklin. isbn 978-0-531-12524-3. hawking, stephen (1988). a brief history of time. bantam books, inc. isbn 978-0-553-38016-3. hawking, stephen; penrose, roger (1996). the nature of space and time. princeton university press. isbn 978-0-691-03791-2. archived from the original on 18 october 2021. retrieved 16 may 2020. levin, janna (2020). black hole survival guide. new york: alfred a. knopf. isbn 978-0-525-65822-1. archived from the original on 22 march 2022. retrieved 6 november 2021. melia, fulvio (2003). the black hole at the center of our galaxy. princeton u press. isbn 978-0-691-09505-9. melia, fulvio (2003). the edge of infinity. supermassive black holes in the universe. cambridge u press. isbn 978-0-521-81405-8. pickover, clifford (1998). black holes: a travelers guide. wiley, john sons, inc. isbn 978-0-471-19704-1. susskind, leonard (2008). the black hole war: my battle with stephen hawking to make the world safe for quantum"
  },
  {
    "chunk_id": 175,
    "doc_id": "Black_hole.txt",
    "text": "fulvio (2003). the edge of infinity. supermassive black holes in the universe. cambridge u press. isbn 978-0-521-81405-8. pickover, clifford (1998). black holes: a travelers guide. wiley, john sons, inc. isbn 978-0-471-19704-1. susskind, leonard (2008). the black hole war: my battle with stephen hawking to make the world safe for quantum mechanics (1st ed.). new york: little, brown. isbn 978-0-316-01640-7. oclc 181603165. university textbooks and monographs carter, b. (1973). black hole equilibrium states. in dewitt-morette, ccile; dewitt, bryce s. (eds.). black holes. new york: gordon and breach. isbn 978-0-677-15610-1. chandrasekhar, subrahmanyan (1999). mathematical theory of black holes. oxford university press. isbn 978-0-19-850370-5. frolov, valeri p.; novikov, igor d. (1998). black hole physics. fundamental theories of physics. vol. 96. doi:10.1007978-94-011-5139-9. isbn 978-0-7923-5146-7. frolov, valeri p.; zelnikov, andrei (2011). introduction to black hole physics. oxford: oxford university press. isbn 978-0-19-969229-3. zbl 1234.83001. archived from the original on 22 march 2022. retrieved 2 january 2022. melia, fulvio (2007). the galactic supermassive black hole. princeton u press. isbn 978-0-691-13129-0. taylor, edwin f.; wheeler, john archibald (2000). exploring black holes. addison wesley longman. isbn 978-0-201-38423-9. wald, robert m. (1992). space, time, and gravity: the theory of the big bang and black holes. university of chicago press. isbn 978-0-226-87029-8. price, richard; creighton, teviet (2008). black holes. scholarpedia. 3 (1): 4277. bibcode:2008schpj...3.4277c. doi:10.4249scholarpedia.4277. review papers hughes, scott a. (2005). trust but verify: the case for astrophysical black holes. arxiv:hep-ph0511217. lecture notes from 2005 slac summer institute. gallo, elena; marolf, donald (2009). resource letter bh-2: black holes. american journal of physics. 77 (4): 294307. arxiv:0806.2316. bibcode:2009amjph..77..294g. doi:10.11191.3056569. s2cid 118494056. cardoso, vitor; pani, paolo (2019). testing the nature of dark compact objects: a status report. living reviews in relativity. 22 (1): 4. arxiv:1904.05363. bibcode:2019lrr....22....4c. doi:10.1007s41114-019-0020-4. s2cid 256465740. mann, robert b.; murk, sebastian; terno, daniel r. (2022). black holes and their horizons in semiclassical and modified theories of gravity. international journal of modern physics d. 31 (9): 22300152230276. arxiv:2112.06515. bibcode:2022ijmpd..3130015m. doi:10.1142s0218271822300154. s2cid 245123647. external links black hole at wikipedias sister projects definitions from wiktionary media from commons news from wikinews quotations from wikiquote textbooks from wikibooks resources from wikiversity data from wikidata logo scholia has a profile for black hole (q589). black holes on in our time at the bbc stanford encyclopedia of philosophy: singularities and black holes by erik curiel and peter bokulich. black holes: gravitys relentless pull interactive multimedia web site about the physics and astronomy of black holes"
  },
  {
    "chunk_id": 176,
    "doc_id": "Black_hole.txt",
    "text": "wikidata logo scholia has a profile for black hole (q589). black holes on in our time at the bbc stanford encyclopedia of philosophy: singularities and black holes by erik curiel and peter bokulich. black holes: gravitys relentless pull interactive multimedia web site about the physics and astronomy of black holes from the space telescope science institute (hubblesite) esas black hole visualization archived 3 may 2019 at the wayback machine frequently asked questions (faqs) on black holes fall into a black hole black holes - basic (nyt; april 2021) videos 16-year-long study tracks stars orbiting sagittarius a movie of black hole candidate from max planck institute cowen, ron (20 april 2015). 3d simulations of colliding black holes hailed as most realistic yet. nature. doi:10.1038nature.2015.17360. computer visualisation of the signal detected by ligo two black holes merge into one (based upon the signal gw150914) vte black holes outline types btz black holeschwarzschildrotatingchargedvirtualkugelblitzsupermassiveprimordialdirect collapseroguemalamenthogarth spacetime size micro extremalelectronstellar microquasarintermediate-masssupermassive active galactic nucleusquasarlqgblazarbl lacfsrq formation stellar evolutiongravitational collapseneutron star related linkstolmanoppenheimervolkoff limitoppenheimersnyder modelwhite dwarf related linkssupernova micronovahypernovarelated linksgamma-ray burstbinary black holequark starsupermassive starquasi-starsupermassive dark starx-ray binary properties astrophysical jetgravitational singularity ring singularitybkl singularityshock singularitytheoremsevent horizonphoton sphereinnermost stable circular orbitergosphere penrose processblandfordznajek processaccretion diskhawking radiationgravitational lens microlenscauchy horizon mass inflationbondi accretionmsigma relationquasi-periodic oscillationthermodynamicsbekenstein boundboussos holographic bound immirzi parameterschwarzschild radiusspaghettification issues information paradox complementaritysoft haircosmic censorshiper eprfinal parsec problemfirewall (physics)holographic principleno-hair theorem metrics schwarzschild (derivation)kerrreissnernordstrmkerrnewmanhayward alternatives nonsingular black hole modelsblack stardark stardark-energy stargravastarmagnetospheric eternally collapsing objectplanck starq starfuzzballgeon analogs optical black holesonic black hole lists black holesmost massivenearestquasarsmicroquasars related outline of black holesblack hole initiativeblack hole starshipblack holes in fictionbig bangbig bouncecompact starexotic star quark starpreon stargravitational wavesgamma-ray burst progenitorsgravity wellhypercompact stellar systemmembrane paradigmnaked singularitypopulation iii starsupermassive starquasi-starsupermassive dark starrossi x-ray timing explorersuperluminal motiontimeline of black hole physicswhite holewormholetidal disruption event notable 1es 19276543c 273a0620-00at2018hyzcentaurus acygnus x-1gaia bh1hercules amarkarian 501ms 0735.67421neve 1oj 287phoenix clusterpks 1302-102pso j030947.49271757.31q09066930sagittarius asdss j08491114swift j164457ton 618ulas j13420928xte j1118480xte j1650-500 category commons vte relativity special relativity background principle of relativity (galilean relativitygalilean transformation)special relativitydoubly special relativity fundamental concepts frame of referencespeed of lighthyperbolic orthogonalityrapiditymaxwells equationsproper lengthproper timeproper accelerationrelativistic mass formulation lorentz transformationtextbooks phenomena time dilationmassenergy equivalence (emc2)length contractionrelativity of simultaneityrelativistic doppler effectthomas precessionladder paradoxtwin paradoxterrell rotation spacetime light coneworld lineminkowski diagrambiquaternionsminkowski space general relativity background introductionmathematical formulation fundamental concepts equivalence principleriemannian geometrypenrose diagramgeodesicsmachs principle formulation adm formalismbssn formalismeinstein field equationslinearized gravitypost-newtonian formalismraychaudhuri equationhamiltonjacobieinstein equationernst equation phenomena black holeevent horizonsingularitytwo-body problem gravitational waves: astronomydetectors"
  },
  {
    "chunk_id": 177,
    "doc_id": "Black_hole.txt",
    "text": "time dilationmassenergy equivalence (emc2)length contractionrelativity of simultaneityrelativistic doppler effectthomas precessionladder paradoxtwin paradoxterrell rotation spacetime light coneworld lineminkowski diagrambiquaternionsminkowski space general relativity background introductionmathematical formulation fundamental concepts equivalence principleriemannian geometrypenrose diagramgeodesicsmachs principle formulation adm formalismbssn formalismeinstein field equationslinearized gravitypost-newtonian formalismraychaudhuri equationhamiltonjacobieinstein equationernst equation phenomena black holeevent horizonsingularitytwo-body problem gravitational waves: astronomydetectors (ligo and collaborationvirgolisa pathfindergeo)hulsetaylor binary other tests: precession of mercurylensing (together with einstein cross and einstein rings)redshiftshapiro delayframe-dragging geodetic effect (lensethirring precession)pulsar timing arrays advanced theories bransdicke theorykaluzakleinquantum gravity solutions cosmological: friedmannlematrerobertsonwalker (friedmann equations)lematretolmankasnerbkl singularitygdelmilne spherical: schwarzschild (interiortolmanoppenheimervolkoff equation)reissnernordstrm axisymmetric: kerr (kerrnewman)weyllewispapapetroutaubnutvan stockum dustdiscs others: pp-waveozsvthschckingalcubierreellis in computational physics: numerical relativity scientists poincarlorentzeinsteinhilbertschwarzschildde sitterweyleddingtonfriedmannlematremilnerobertsonchandrasekharzwickywheelerchoquet-bruhatkerrzeldovichnovikovehlersgerochpenrosehawkingtaylorhulsebondimisneryauthorneweissothers category vte string theory background stringscosmic stringshistory of string theory first superstring revolutionsecond superstring revolutionstring theory landscape theory nambugoto actionpolyakov actionbosonic string theorysuperstring theory type i stringtype ii string type iia stringtype iib stringheterotic stringn2 superstringf-theorystring field theorymatrix string theorynon-critical string theorynon-linear sigma modeltachyon condensationrns formalismgs formalism string duality t-dualitys-dualityu-dualitymontonenolive duality particles and fields gravitondilatontachyonramondramond fieldkalbramond fieldmagnetic monopoledual gravitondual photon branes d-branens5-branem2-branem5-branes-braneblack braneblack holesblack stringbrane cosmologyquiver diagramhananywitten transition conformal field theory virasoro algebramirror symmetryconformal anomalyconformal algebrasuperconformal algebravertex operator algebraloop algebrakacmoody algebrawesszuminowitten model gauge theory anomaliesinstantonschernsimons formbogomolnyiprasadsommerfield boundexceptional lie groups (g2, f4, e6, e7, e8)ade classificationdirac stringp-form electrodynamics geometry worldsheetkaluzaklein theorycompactificationwhy 10 dimensions?khler manifoldricci-flat manifold calabiyau manifoldhyperkhler manifold k3 surfaceg2 manifoldspin(7)-manifoldgeneralized complex manifoldorbifoldconifoldorientifoldmoduli spacehoavawitten theoryk-theory (physics)twisted k-theory supersymmetry supergravityeleven-dimensional supergravitytype i supergravitytype iia supergravitytype iib supergravitysuperspacelie superalgebralie supergroup holography holographic principleadscft correspondence m-theory matrix theoryintroduction to m-theory string theorists aganagiarkani-hamedatiyahbanksberensteinboussocurtrightdijkgraafdistlerdouglasduffdvaliferrarafischlerfriedangatesgliozzigopakumargreengreenegrossgubsergukovguthhansonharveyt hoofthoavagibbonskachrukakukalloshkaluzakapustinklebanovknizhnikkontsevichkleinlindemaldacenamandelstammarolfmartinecminwallamooremotlmukhimyersnanopoulosnstasenekrasovneveunielsenvan nieuwenhuizennovikovoliveooguriovrutpolchinskipolyakovrajaramanramondrandallrandjbar-daemiroekrohmsagnottischerkschwarzseibergsenshenkersiegelsilversteinsnstaudachersteinhardtstromingersundrumsusskindtownsendtrivediturokvafavenezianoverlindeverlindewesswittenyauyoneyazamolodchikovzamolodchikovzaslowzuminozwiebach authority control databases edit this at wikidata international gndfast national united statesfrancebnf datajapanczech republicspainisrael other idrefyale lux"
  },
  {
    "chunk_id": 178,
    "doc_id": "Neural_Networks.txt",
    "text": "an artificial neural network is an interconnected group of nodes, inspired by a simplification of neurons in a brain. here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. neural network (machine learning) in machine learning, a neural network or neural net (nn), also called artificial neural network (ann), is a computational model inspired by the structure and functions of biological neural networks. 12 a neural network consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. these are connected by edges, which model the synapses in the brain. each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. the signal is a real number, and the output of each neuron is computed by some non-linear function of the totality of its inputs, called the activation function. the strength of the signal at each connection is determined by a weight, which adjusts during the learning process. typically, neurons are aggregated into layers. different layers may perform different transformations on their inputs. signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). a network is typically called a deep neural network if it has at least two hidden layers.3 artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. they can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information. neural networks are typically trained through empirical risk minimization, which is based on the idea of optimizing the networks parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.4 gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.4 during the training phase, anns learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. 5 this method allows the network to generalize to unseen data. training 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 146 simplified example of training a neural network in object detection: the"
  },
  {
    "chunk_id": 179,
    "doc_id": "Neural_Networks.txt",
    "text": "anns learn from labeled training data by iteratively updating their parameters to minimize a defined loss function. 5 this method allows the network to generalize to unseen data. training 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 146 simplified example of training a neural network in object detection: the network is trained by multiple images that are known to depict starfish and sea urchins, which are correlated with nodes that represent visual features. the starfish match with a ringed texture and a star outline, whereas most sea urchins match with a striped texture and oval shape. however, the instance of a ring textured sea urchin creates a weakly weighted association between them. subsequent run of the network on an input image (left): 6 the network correctly detects the starfish. however, the weakly weighted association between ringed texture and sea urchin also confers a weak signal to the latter from one of two intermediate nodes. in addition, a shell that was not included in the training gives a weak signal for the oval shape, also resulting in a weak signal for the sea urchin output. these weak signals may result in a false positive result for sea urchin. in reality, textures and outlines would not be represented by single nodes, but rather by associated weight patterns of multiple nodes. todays deep neural networks are based on early work in statistics over 200 years ago. the simplest kind of feedforward neural network (fnn) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. the sum of the products of the weights and the inputs is calculated at each node. the mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. this technique has been known for over two centuries as the method of least squares or linear regression. it was used as a means of finding a good rough linear fit to a set of points by legendre (1805) and gauss (1795) for the prediction of planetary movement.7891011 historically, digital computers such as the von neumann model operate via the execution of explicit instructions with access to memory by a number of processors. some neural networks, on the other hand, originated from efforts to model information processing in biological systems"
  },
  {
    "chunk_id": 180,
    "doc_id": "Neural_Networks.txt",
    "text": "gauss (1795) for the prediction of planetary movement.7891011 historically, digital computers such as the von neumann model operate via the execution of explicit instructions with access to memory by a number of processors. some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. unlike the von neumann model, connectionist computing does not separate memory and processing. warren mcculloch and walter pitts12 (1943) considered a non-learning computational model for neural networks.13 this model paved the way for research to split into two approaches. one approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence. in the late 1940s, d. o. hebb14 proposed a learning hypothesis based on the mechanism of neural plasticity that became known as hebbian learning. it was used in many early neural networks, such as rosenblatts perceptron and the hopfield network. farley and clark15 (1954) used computational machines to simulate a hebbian network. other neural network computational machines were created by rochester, holland, habit and duda (1956).16 in 1958, psychologist frank rosenblatt described the perceptron, one of the first implemented artificial neural networks,17181920 funded by the united states office of naval research. 21 r. d. joseph (1960)22 mentions an even earlier perceptron-like device by farley and clark:10 history early work 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 246 farley and clark of mit lincoln laboratory actually preceded rosenblatt in the development of a perceptron-like device. however, they dropped the subject. the perceptron raised public excitement for research in artificial neural networks, causing the us government to drastically increase funding. this contributed to the golden age of ai fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.23 the first perceptrons did not have adaptive hidden units. however, joseph (1960)22 also discussed multilayer perceptrons with an adaptive hidden layer. rosenblatt (1962)24:section 16 cited and adopted these ideas, also crediting work by h. d. block and b. w. knight. unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning. fundamental research was conducted on anns in the 1960s and 1970s. the first working deep learning algorithm was the group method of data handling, a method to train arbitrarily deep neural networks, published by alexey ivakhnenko and lapa in the soviet union (1965)."
  },
  {
    "chunk_id": 181,
    "doc_id": "Neural_Networks.txt",
    "text": "algorithm for hidden units, i.e., deep learning. fundamental research was conducted on anns in the 1960s and 1970s. the first working deep learning algorithm was the group method of data handling, a method to train arbitrarily deep neural networks, published by alexey ivakhnenko and lapa in the soviet union (1965). they regarded it as a form of polynomial regression,25 or a generalization of rosenblatts perceptron.26 a 1971 paper described a deep network with eight layers trained by this method,27 which is based on layer by layer training through regression analysis. superfluous hidden units are pruned using a separate validation set. since the activation functions of the nodes are kolmogorovgabor polynomials, these were also the first deep networks with multiplicative units or gates.10 the first deep learning multilayer perceptron trained by stochastic gradient descent28 was published in 1967 by shunichi amari. 29 in computer experiments conducted by amaris student saito, a five layer mlp with two modifiable layers learned internal representations to classify nonlinearily separable pattern classes.10 subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique. in 1969, kunihiko fukushima introduced the relu (rectified linear unit) activation function.103031 the rectifier has become the most popular activation function for deep learning.32 nevertheless, research stagnated in the united states following the work of minsky and papert (1969),33 who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. this insight was irrelevant for the deep networks of ivakhnenko (1965) and amari (1967). in 1976 transfer learning was introduced in neural networks learning.3435 deep learning architectures for convolutional neural networks (cnns) with convolutional layers and downsampling layers and weight replication began with the neocognitron introduced by kunihiko fukushima in 1979, though not trained by backpropagation.363738 backpropagation is an efficient application of the chain rule derived by gottfried wilhelm leibniz in 167339 to networks of differentiable nodes. the terminology back-propagating errors was actually introduced in 1962 by rosenblatt,24 but he did not know how to implement this, although henry j. kelley had a continuous precursor of backpropagation in 1960 in the context of control theory. 40 in 1970, seppo linnainmaa published the modern form of backpropagation in deep learning breakthroughs in the 1960s and 1970s backpropagation 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 346 his masters thesis (1970).414210 g.m. ostrovski et al. republished it in 1971.4344 paul werbos applied backpropagation to neural networks"
  },
  {
    "chunk_id": 182,
    "doc_id": "Neural_Networks.txt",
    "text": "40 in 1970, seppo linnainmaa published the modern form of backpropagation in deep learning breakthroughs in the 1960s and 1970s backpropagation 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 346 his masters thesis (1970).414210 g.m. ostrovski et al. republished it in 1971.4344 paul werbos applied backpropagation to neural networks in 19824546 (his 1974 phd thesis, reprinted in a 1994 book,47 did not yet describe the algorithm44 ). in 1986, david e. rumelhart et al. popularised backpropagation but did not cite the original work.48 kunihiko fukushimas convolutional neural network (cnn) architecture of 197936 also introduced max pooling, 49 a popular downsampling procedure for cnns. cnns have become an essential tool for computer vision. the time delay neural network (tdnn) was introduced in 1987 by alex waibel to apply cnn to phoneme recognition. it used convolutions, weight sharing, and backpropagation.5051 in 1988, wei zhang applied a backpropagation-trained cnn to alphabet recognition.52 in 1989, yann lecun et al. created a cnn called lenet for recognizing handwritten zip codes on mail. training required 3 days.53 in 1990, wei zhang implemented a cnn on optical computing hardware.54 in 1991, a cnn was applied to medical image object segmentation55 and breast cancer detection in mammograms.56 lenet-5 (1998), a 7-level cnn by yann lecun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 3232 pixel images.57 from 1988 onward,5859 the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments. 60 one origin of rnn was statistical mechanics. in 1972, shunichi amari proposed to modify the weights of an ising model by hebbian learning rule as a model of associative memory, adding in the component of learning.61 this was popularized as the hopfield network by john hopfield (1982).62 another origin of rnn was neuroscience. the word recurrent is used to describe loop-like structures in anatomy. in 1901, cajal observed recurrent semicircles in the cerebellar cortex. 63 hebb considered reverberating circuit as an explanation for short-term memory.64 the mcculloch and pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.12 in 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a crossbar adaptive array,6566 used"
  },
  {
    "chunk_id": 183,
    "doc_id": "Neural_Networks.txt",
    "text": "pitts paper (1943) considered neural networks that contain cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.12 in 1982 a recurrent neural network with an array architecture (rather than a multilayer perceptron architecture), namely a crossbar adaptive array,6566 used direct recurrent connections from the output to the supervisor (teaching) inputs. in addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. eliminating the external supervisor, it introduced the self-learning method in neural networks. in cognitive psychology, the journal american psychologist in early 1980s carried out a debate on the relation between cognition and emotion. zajonc in 1980 stated that emotion is computed first and is independent from cognition, while lazarus in 1982 stated that cognition is computed first and is inseparable from emotion.6768 in 1982 the crossbar adaptive array gave a neural network model of cognition-emotion relation.6569 it was an example of a debate where an ai system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology. convolutional neural networks recurrent neural networks 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 446 two early influential works were the jordan network (1986) and the elman network (1990), which applied rnn to study cognitive psychology. in the 1980s, backpropagation did not work well for deep rnns. to overcome this problem, in 1991, jrgen schmidhuber proposed the neural sequence chunker or neural history compressor7071 which introduced the important concepts of self-supervised pre-training (the p in chatgpt) and neural knowledge distillation. 10 in 1993, a neural history compressor system solved a very deep learning task that required more than 1000 subsequent layers in an rnn unfolded in time.72 in 1991, sepp hochreiters diploma thesis73 identified and analyzed the vanishing gradient problem7374 and proposed recurrent residual connections to solve it. he and schmidhuber introduced long short-term memory (lstm), which set accuracy records in multiple applications domains.7576 this was not yet the modern version of lstm, which required the forget gate, which was introduced in 1999.77 it became the default choice for rnn architecture. during 19851995, inspired by statistical mechanics, several architectures and methods were developed by terry sejnowski, peter dayan, geoffrey hinton, etc., including the boltzmann machine, 78 restricted boltzmann machine, 79 helmholtz machine, 80 and the wake-sleep algorithm. 81 these were designed for unsupervised learning of deep generative models. between"
  },
  {
    "chunk_id": 184,
    "doc_id": "Neural_Networks.txt",
    "text": "rnn architecture. during 19851995, inspired by statistical mechanics, several architectures and methods were developed by terry sejnowski, peter dayan, geoffrey hinton, etc., including the boltzmann machine, 78 restricted boltzmann machine, 79 helmholtz machine, 80 and the wake-sleep algorithm. 81 these were designed for unsupervised learning of deep generative models. between 2009 and 2012, anns began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. 8283 in 2011, a cnn named dannet8485 by dan ciresan, ueli meier, jonathan masci, luca maria gambardella, and jrgen schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.38 it then won more contests.8687 they also showed how max-pooling cnns on gpu improved performance significantly.88 in october 2012, alexnet by alex krizhevsky, ilya sutskever, and geoffrey hinton89 won the large-scale imagenet competition by a significant margin over shallow machine learning methods. further incremental improvements included the vgg-16 network by karen simonyan and andrew zisserman90 and googles inceptionv3. 91 in 2012, ng and dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.92 unsupervised pre-training and increased computing power from gpus and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as deep learning.5 radial basis function and wavelet networks were introduced in 2013. these can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.93 generative adversarial network (gan) (ian goodfellow et al., 2014)94 became state of the art in generative modeling during 20142018 period. the gan principle was originally published in 1991 by jrgen schmidhuber who called it artificial curiosity: two neural networks contest with each other in the form of a zero-sum game, where one networks gain is the other networks loss.9596 the first network is a generative model that models a probability distribution over output patterns. deep learning 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 546 neuron and myelinated axon, with signal flow from inputs at dendrites to outputs at axon terminals the second network learns by gradient descent to predict the reactions of the environment to these patterns. excellent image quality is achieved by nvidias stylegan (2018)97 based on the progressive gan by tero karras et al.98 here, the gan generator is"
  },
  {
    "chunk_id": 185,
    "doc_id": "Neural_Networks.txt",
    "text": "from inputs at dendrites to outputs at axon terminals the second network learns by gradient descent to predict the reactions of the environment to these patterns. excellent image quality is achieved by nvidias stylegan (2018)97 based on the progressive gan by tero karras et al.98 here, the gan generator is grown from small to large scale in a pyramidal fashion. image generation by gan reached popular success, and provoked discussions concerning deepfakes. 99 diffusion models (2015)100 eclipsed gans in generative modeling since then, with systems such as dalle 2 (2022) and stable diffusion (2022). in 2014, the state of the art was training very deep neural network with 20 to 30 layers.101 stacking too many layers led to a steep reduction in training accuracy,102 known as the degradation problem.103 in 2015, two techniques were developed to train very deep networks: the highway network was published in may 2015,104 and the residual neural network (resnet) in december 2015.105106 resnet behaves like an open-gated highway net. during the 2010s, the seq2seq model was developed, and attention mechanisms were added. it led to the modern transformer architecture in 2017 in attention is all you need. 107 it requires computation time that is quadratic in the size of the context window. jrgen schmidhubers fast weight controller (1992)108 scales linearly and was later shown to be equivalent to the unnormalized linear transformer.10911010 transformers have increasingly become the model of choice for natural language processing. 111 many modern large language models such as chatgpt, gpt-4, and bert use this architecture. anns began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. they soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. anns have the ability to learn and model non-linearities and complex relationships. this is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. the network forms a directed, weighted graph. 112 an artificial neural network consists of simulated neurons. each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. all the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. each link has a weight, determining the strength of one nodes influence on another,113 allowing weights to choose the signal between"
  },
  {
    "chunk_id": 186,
    "doc_id": "Neural_Networks.txt",
    "text": "via links like a biological axon-synapse-dendrite connection. all the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. each link has a weight, determining the strength of one nodes influence on another,113 allowing weights to choose the signal between neurons. anns are composed of artificial neurons which are conceptually derived from biological neurons. each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.114 the inputs can be the feature values of a sample of external data, such as images or models artificial neurons 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 646 documents, or they can be the outputs of other neurons. the outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image. to find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. we add a bias term to this sum.115 this weighted sum is sometimes called the activation. this weighted sum is then passed through a (usually nonlinear) activation function to produce the output. the initial inputs are external data, such as images and documents. the ultimate outputs accomplish the task, such as recognizing an object in an image.116 the neurons are typically organized into multiple layers, especially in deep learning. neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. the layer that receives external data is the input layer. the layer that produces the ultimate result is the output layer. in between them are zero or more hidden layers. single layer and unlayered networks are also used. between two layers, multiple connection patterns are possible. they can be fully connected, with every neuron in one layer connecting to every neuron in the next layer. they can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.117 neurons with only such connections form a directed acyclic graph and are known as feedforward networks. 118 alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks. 119 a hyperparameter is a constant parameter defining any configurable part of"
  },
  {
    "chunk_id": 187,
    "doc_id": "Neural_Networks.txt",
    "text": "in that layer.117 neurons with only such connections form a directed acyclic graph and are known as feedforward networks. 118 alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks. 119 a hyperparameter is a constant parameter defining any configurable part of the learning process, whose value is set prior to training.120 examples of hyperparameters include learning rate, batch size and regularization parameters.121 . the performance of a neural network is strongly influenced by the choice of hyperparameter values, and thus the hyperparameters are often optimized as part of the training process, a process called hyperparameter tuning or hyperparameter optimization.122 learning is the adaptation of the network to better handle a task by considering sample observations. learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. this is done by minimizing the observed errors. learning is complete when examining additional observations does not usefully reduce the error rate. even after learning, the error rate typically does not reach 0. if after learning, the error rate is too high, the network typically must be redesigned. practically this is done by defining a cost function that is evaluated periodically during learning. as long as its output continues to decline, learning continues. the cost is frequently defined as a statistic whose value can only be approximated. the outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. learning attempts to reduce the total of the differences across the observations. most learning models can be viewed as a straightforward application of optimization theory and statistical estimation. 112123 organization hyperparameter learning 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 746 the learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.124 a high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. optimizations such as quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. in order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.125 the concept"
  },
  {
    "chunk_id": 188,
    "doc_id": "Neural_Networks.txt",
    "text": "are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. in order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.125 the concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. a momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change. while it is possible to define a cost function ad hoc, frequently the choice is determined by the functions desirable properties (such as convexity) because it arises from the model (e.g. in a probabilistic model, the models posterior probability can be used as an inverse cost). backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. the error amount is effectively divided among the connections. technically, backpropagation calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. the weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines, 126 no-prop networks,127 training without backtracking,128 weightless networks,129130 and non-connectionist neural networks. machine learning is commonly separated into three main learning paradigms, supervised learning, 131 unsupervised learning132 and reinforcement learning. 133 each corresponds to a particular learning task. supervised learning uses a set of paired inputs and desired outputs. the learning task is to produce the desired output for each input. in this case, the cost function is related to eliminating incorrect deductions.134 a commonly used cost is the mean-squared error, which tries to minimize the average squared error between the networks output and the desired output. tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). this can be thought of as learning with a teacher, in the form of a function that provides continuous feedback on the quality of solutions obtained thus far. learning rate cost function backpropagation learning paradigms supervised learning 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 846 in unsupervised learning, input data is given along with the cost function, some function"
  },
  {
    "chunk_id": 189,
    "doc_id": "Neural_Networks.txt",
    "text": "the form of a function that provides continuous feedback on the quality of solutions obtained thus far. learning rate cost function backpropagation learning paradigms supervised learning 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 846 in unsupervised learning, input data is given along with the cost function, some function of the data and the networks output. the cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). as a trivial example, consider the model where is a constant and the cost . minimizing this cost produces a value of that is equal to the mean of the data. the cost function can be much more complicated. its form depends on the application: for example, in compression it could be related to the mutual information between and , whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering. in applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. the goal is to win the game, i.e., generate the most positive (lowest cost) responses. in reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. at each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. the rules and the long-term cost usually only can be estimated. at any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly. formally, the environment is modeled as a markov decision process (mdp) with states and actions . because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution , the observation distribution and the transition distribution , while a policy is defined as the conditional distribution over actions given the observations. taken together, the two define a markov chain (mc). the aim is to discover the lowest-cost mc. anns"
  },
  {
    "chunk_id": 190,
    "doc_id": "Neural_Networks.txt",
    "text": "probability distributions are used instead: the instantaneous cost distribution , the observation distribution and the transition distribution , while a policy is defined as the conditional distribution over actions given the observations. taken together, the two define a markov chain (mc). the aim is to discover the lowest-cost mc. anns serve as the learning component in such applications.135136 dynamic programming coupled with anns (giving neurodynamic programming)137 has been applied to problems such as those involved in vehicle routing, 138 video games, natural resource management139140 and medicine141 because of anns ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks. self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (caa).142 it is a system with only one input, situation s, and only one output, action (or behavior) a. it has neither external advice input nor external reinforcement input from the environment. the caa computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. the system is unsupervised learning reinforcement learning self-learning 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 946 driven by the interaction between cognition and emotion.143 given the memory matrix, w w(a,s), the crossbar self-learning algorithm in each iteration performs the following computation: in situation s perform action a; receive consequence situation s; compute emotion of being in consequence situation v(s); update crossbar memory w(a,s) w(a,s) v(s). the backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. the caa exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it receives initial emotions (only once) about to be encountered situations in the behavioral environment. having received the genome vector (species vector) from the genetic environment, the caa will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.144 neuroevolution can create neural network topologies and weights using evolutionary computation. it is competitive with sophisticated gradient descent approaches.145146 one advantage of neuroevolution is that it may be less prone to get caught in dead ends.147 stochastic neural networks originating from sherringtonkirkpatrick models are a type of artificial neural network built by introducing random variations into the network,"
  },
  {
    "chunk_id": 191,
    "doc_id": "Neural_Networks.txt",
    "text": "using evolutionary computation. it is competitive with sophisticated gradient descent approaches.145146 one advantage of neuroevolution is that it may be less prone to get caught in dead ends.147 stochastic neural networks originating from sherringtonkirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the networks artificial neurons stochastic transfer functions , or by giving them stochastic weights. this makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima. 148 stochastic neural networks trained using a bayesian approach are known as bayesian neural networks. 149 topological deep learning, first introduced in 2017,150 is an emerging approach in machine learning that integrates topology with deep neural networks to address highly intricate and highorder data. initially rooted in algebraic topology, tdl has since evolved into a versatile framework incorporating tools from other mathematical disciplines, such as differential topology and geometric topology. as a successful example of mathematical deep learning, tdl continues to inspire advancements in mathematical artificial intelligence, fostering a mutually beneficial relationship between ai and mathematics. in a bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. evolutionary methods, 151 gene expression programming, 152 simulated annealing, 153 expectationmaximization, non-parametric methods and particle swarm optimization154 are other learning algorithms. convergent recursion is a learning algorithm for cerebellar model articulation controller (cmac) neural networks.155156 neuroevolution stochastic neural network topological deep learning other 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1046 two modes of learning are available: stochastic and batch. in stochastic learning, each input creates a weight adjustment. in batch learning, weights are adjusted based on a batch of inputs, accumulating errors over the batch. stochastic learning introduces noise into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. however, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batchs average error. a common compromise is to use mini-batches, small batches with samples in each batch selected stochastically from the entire data set. anns have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. the simplest types have one or more static components, including number of units, number of layers,"
  },
  {
    "chunk_id": 192,
    "doc_id": "Neural_Networks.txt",
    "text": "small batches with samples in each batch selected stochastically from the entire data set. anns have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. the simplest types have one or more static components, including number of units, number of layers, unit weights and topology. dynamic types allow one or more of these to evolve via learning. the latter is much more complicated but can shorten learning periods and produce better results. some types allowrequire learning to be supervised by the operator, while others operate independently. some types operate purely in hardware, while others are purely software and run on general purpose computers. some of the main breakthroughs include: convolutional neural networks that have proven particularly successful in processing visual and other two-dimensional data;157158 where long short-term memory avoids the vanishing gradient problem159 and can handle signals that have a mix of low and high frequency components aiding large-vocabulary speech recognition,160161 text-to-speech synthesis, 162163164 and photo-real talking heads.165convolutional neural networks have also been applied to fraud detection166167 . competitive networks such as generative adversarial networks in which multiple networks (of varying structure) compete with each other, on tasks such as winning a game168 or on deceiving the opponent about the authenticity of an input.94 using artificial neural networks requires an understanding of their characteristics. choice of model: this depends on the data representation and the application. model parameters include the number, type, and connectedness of network layers, as well as the size of each and the connection type (full, pooling, etc.). overly complex models learn slowly. learning algorithm: numerous trade-offs exist between learning algorithms. almost any algorithm will work well with the correct hyperparameters169 for training on a particular data set. however, selecting and tuning an algorithm for training on unseen data requires significant experimentation. robustness: if the model, cost function and learning algorithm are selected appropriately, the resulting ann can become robust. neural architecture search (nas) uses machine learning to automate ann design. various approaches to nas have designed networks that compare well with hand-designed systems. the basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the modes types network design 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1146 results as feedback to teach the nas network.170 available systems include automl and autokeras.171 scikit-learn library provides functions to help with building"
  },
  {
    "chunk_id": 193,
    "doc_id": "Neural_Networks.txt",
    "text": "is to propose a candidate model, evaluate it against a dataset, and use the modes types network design 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1146 results as feedback to teach the nas network.170 available systems include automl and autokeras.171 scikit-learn library provides functions to help with building a deep network from scratch. we can then implement a deep network with tensorflow or keras. hyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for cnns), etc.172 the python code snippet provides an overview of the training function, which uses the training dataset, number of hidden layer units, learning rate, and number of iterations as parameters: when neural networks are deployed in real-world applications, the statistical properties of the input data may change over time, a phenomenon known as concept drift or non-stationarity. drift can reduce predictive accuracy and lead to unreliable or biased decisions if it is not detected and def train(x, y, nhidden, learningrate, niter): training function. args: x: argument x. y: argument y. nhidden: the number of hidden layer units. learningrate: the learning rate. niter: the number of iterations. returns: dict: a dictionary. m, ninput x.shape 1. random initialize weights and biases w1 np.random.randn(ninput, nhidden) b1 np.zeros((1, nhidden)) w2 np.random.randn(nhidden, 1) b2 np.zeros((1, 1)) 2. in each iteration, feed all layers with the latest weights and biases for i in range(niter 1): z2 np.dot(x, w1) b1 a2 sigmoid(z2) z3 np.dot(a2, w2) b2 a3 z3 dz3 a3 - y dw2 np.dot(a2.t, dz3) db2 np.sum(dz3, axis0, keepdimstrue) dz2 np.dot(dz3, w2.t) sigmoidderivative(z2) dw1 np.dot(x.y, dz2) db1 np.sum(dz2, axis0) 3. update weights and biases with gradients w1 - learningrate dw1 m w2 - learningrate dw2 m b1 - learningrate db1 m b2 - learningrate db2 m if i 1000 0: print(epoch, i, loss: , np.mean(np.square(dz3))) model w1: w1, b1: b1, w2: w2, b2: b2 return model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 monitoring and concept drift detection of anns 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1246 corrected. in practice, this means that the"
  },
  {
    "chunk_id": 194,
    "doc_id": "Neural_Networks.txt",
    "text": "20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 monitoring and concept drift detection of anns 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1246 corrected. in practice, this means that the models accuracy in deployment may differ substantially from the levels observed during training or cross-validation. several strategies have been developed to monitor neural networks for drift and degradation: error-based monitoring: comparing current predictions against ground-truth labels when they become available. this approach directly quantifies predictive performance but may be impractical when labels are delayed or costly to obtain. data distribution monitoring: detecting changes in the input data distribution using statistical tests, divergence measures, or density-ratio estimation. representation monitoring: tracking the distribution of internal embeddings or hidden-layer features. shifts in the latent representation can indicate nonstationarity even when labels are unavailable. statistical methods such as statistical process control charts have been adapted for this purpose.173 because of their ability to model and reproduce nonlinear processes, artificial neural networks have found applications in many disciplines. these include: function approximation, 174 or regression analysis, 175 (including time series prediction, fitness approximation, 176 and modeling) data processing177 (including filtering, clustering, blind source separation, 178 and compression) nonlinear system identification93 and control (including vehicle control, trajectory prediction,179 adaptive control, process control, and natural resource management) pattern recognition (including radar systems, face identification, signal classification,180 novelty detection, 3d reconstruction, 181 object recognition, and sequential decision making182 ) sequence recognition (including gesture, speech, and handwritten and printed text recognition183 ) sensor data analysis184 (including image analysis) robotics (including directing manipulators and prostheses) data mining (including knowledge discovery in databases) finance185 (such as ex-ante models for specific financial long-run forecasts and artificial financial markets) quantum chemistry186 general game playing187 generative ai188 data visualization machine translation social network filtering189 e-mail spam filtering medical diagnosis190 anns have been used to diagnose several types of cancers191192 and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.193194 applications 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1346 anns have been used to accelerate reliability analysis of infrastructures subject to natural disasters195196 and to predict foundation settlements.197 it can also be useful to mitigate flood by the use of anns for modelling rainfall-runoff.198 anns have also been used for building blackbox models in geoscience: hydrology,"
  },
  {
    "chunk_id": 195,
    "doc_id": "Neural_Networks.txt",
    "text": "wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1346 anns have been used to accelerate reliability analysis of infrastructures subject to natural disasters195196 and to predict foundation settlements.197 it can also be useful to mitigate flood by the use of anns for modelling rainfall-runoff.198 anns have also been used for building blackbox models in geoscience: hydrology, 199200 ocean modelling and coastal engineering, 201202 and geomorphology. 203 anns have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. for example, machine learning has been used for classifying android malware,204 for identifying domains belonging to threat actors and for detecting urls posing a security risk.205 research is underway on ann systems designed for penetration testing, for detecting botnets,206 credit cards frauds207 and network intrusions. anns have been proposed as a tool to solve partial differential equations in physics208209210 and simulate the properties of many-body open quantum systems. 211212213214 in brain research anns have studied short-term behavior of individual neurons, 215 the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level. it is possible to create a profile of a users interests from pictures, using artificial neural networks trained for object recognition.216 beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. for instance, graph neural networks (gnns) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. this application underscores the adaptability and potential of anns in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.217 the multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. however, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters. a specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal turing machine, 218 using a finite number of neurons and standard linear connections. further, the use of irrational values for weights results in a machine with super-turing power.219220 a models capacity property corresponds to its ability to model any given"
  },
  {
    "chunk_id": 196,
    "doc_id": "Neural_Networks.txt",
    "text": "precision real number-valued weights) has the power of a universal turing machine, 218 using a finite number of neurons and standard linear connections. further, the use of irrational values for weights results in a machine with super-turing power.219220 a models capacity property corresponds to its ability to model any given function. it is related to the amount of information that can be stored in the network and to the notion of complexity. two notions of capacity are known by the community. the information capacity and the vc dimension. the information capacity of a perceptron is intensively discussed in sir david mackays book221 theoretical properties computational power capacity 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1446 which summarizes work by thomas cover. 222 the capacity of a network of standard neurons (not convolutional) can be derived by four rules223 that derive from understanding a neuron as an electrical element. the information capacity captures the functions modelable by the network given any data as input. the second notion, is the vc dimension. vc dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. this is, given input data in a specific form. as noted in,221 the vc dimension for arbitrary inputs is half the information capacity of a perceptron. the vc dimension for arbitrary points is sometimes referred to as memory capacity.224 models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. thirdly, for sufficiently large data or parameters, some methods become impractical. another issue worthy to mention is that training may cross some saddle point which may lead the convergence to the wrong direction. the convergence behavior of certain types of ann architectures are more understood than others. when the width of network approaches to infinity, the ann is well described by its first order taylor expansion throughout training, and so inherits the convergence behavior of affine models. 225226 another example is when parameters are small, it is observed that anns often fit target functions from low to high frequencies. this behavior is referred to as the spectral bias, or frequency principle, of neural networks.227228229230 this phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such"
  },
  {
    "chunk_id": 197,
    "doc_id": "Neural_Networks.txt",
    "text": "is when parameters are small, it is observed that anns often fit target functions from low to high frequencies. this behavior is referred to as the spectral bias, or frequency principle, of neural networks.227228229230 this phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as jacobi method. deeper neural networks have been observed to be more biased towards low frequency functions.231 applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. this arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. two approaches address over-training. the first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error. the second is to use some form of regularization. this concept emerges in a probabilistic (bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the empirical risk and the structural risk, which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting. supervised neural networks that use a mean squared error (mse) cost function can use formal statistical methods to determine the confidence of the trained model. the mse on a validation set can be used as an estimate for variance. this value can then be used to calculate the confidence interval of network output, assuming a normal distribution. a confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified. convergence generalization and statistics 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1546 confidence analysis of a neural network by assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. this is useful in classification as it gives a certainty measure on classifications. the softmax activation function is: a common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.232 any learning machine needs sufficient representative examples in order to capture the underlying structure that"
  },
  {
    "chunk_id": 198,
    "doc_id": "Neural_Networks.txt",
    "text": "in classification as it gives a certainty measure on classifications. the softmax activation function is: a common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.232 any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches andor introducing a recursive least squares algorithm for cmac. 155 dean pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turnsit should not learn to always turn right).233 a central claim of anns is that they embody new and powerful general principles for processing information. these principles are ill-defined. this allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. in 1997, alexander dewdney, a former scientific american columnist, commented that as a result, artificial neural networks have a something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. no human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything.234 criticism training theory 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1646 one response to dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft235 to detecting credit card fraud to mastering the game of go. technology writer roger bridgman commented: neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasnt?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be an opaque, unreadable table...valueless as a scientific resource. in spite of his emphatic declaration that science is not technology, dewdney seems here"
  },
  {
    "chunk_id": 199,
    "doc_id": "Neural_Networks.txt",
    "text": "hasnt?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be an opaque, unreadable table...valueless as a scientific resource. in spite of his emphatic declaration that science is not technology, dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. an unreadable table that a useful machine could read would still be well worth having.236 although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. moreover, recent emphasis on the explainability of ai has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. for example, bengio and lecun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.237 biological brains use both shallow and deep circuits as reported by brain anatomy,238 displaying a wide variety of invariance. weng239 argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies. large and effective neural networks require considerable computing resources.240 while the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von neumann architecture may consume vast amounts of memory and storage. furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons which require enormous cpu power and time. some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by gpgpus (on gpus), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.38 the use of accelerators such as fpgas and gpus can reduce training times from months to days.240241 neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-neumann chips to directly implement neural networks in circuitry. another type of chip optimized for"
  },
  {
    "chunk_id": 200,
    "doc_id": "Neural_Networks.txt",
    "text": "layers deeper than before.38 the use of accelerators such as fpgas and gpus can reduce training times from months to days.240241 neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-neumann chips to directly implement neural networks in circuitry. another type of chip optimized for neural network processing is called a tensor processing unit, or tpu.242 hardware 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1746 analyzing what has been learned by an ann is much easier than analyzing what has been learned by a biological neural network. furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. for example, local vs. non-local learning and shallow vs. deep architecture.243 advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.244245 neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.246247 these inherited biases become especially critical when the anns are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.246 this imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement. 247248 for example, in 2018, amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.248 the program would penalize any resume with the word woman or the name of any womens college. however, the use of synthetic data can help reduce dataset bias and increase representation in datasets.249 practical counterexamples hybrid approaches dataset bias 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1846 a single-layer feedforward artificial neural network. arrows originating from are omitted for clarity. there are p inputs to this network and q outputs. in this system, the value of the qth output, , is calculated as a two-layer feedforward artificial neural network an artificial neural network an ann dependency graph gallery 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1946 a single-layer feedforward"
  },
  {
    "chunk_id": 201,
    "doc_id": "Neural_Networks.txt",
    "text": "are p inputs to this network and q outputs. in this system, the value of the qth output, , is calculated as a two-layer feedforward artificial neural network an artificial neural network an ann dependency graph gallery 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 1946 a single-layer feedforward artificial neural network with 4 inputs, 6 hidden nodes and 2 outputs. given position state and direction, it outputs wheel based control values. a two-layer feedforward artificial neural network with 8 inputs, 2x8 hidden nodes and 2 outputs. given position state, direction and other environment values, it outputs thruster based control values. parallel pipeline structure of cmac neural network. this learning algorithm can converge in one step. artificial neural networks (anns) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine. in the realm of image processing, anns are employed in tasks such as image classification, object recognition, and image segmentation. for instance, deep convolutional neural networks (cnns) have been important in handwritten digit recognition, achieving state-of-the-art performance.250 this demonstrates the ability of anns to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.250 by modeling speech signals, anns are used for tasks like speaker identification and speech-to-text conversion. deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.250251 these recent advancements and future directions image processing speech recognition 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 2046 advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products. in natural language processing, anns are used for tasks such as text classification, sentiment analysis, and machine translation. they have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.250251 this has implications for automated customer service, content moderation, and language understanding technologies.252 in the domain of control systems, anns are used to model dynamic systems for tasks such as system identification, control design, and optimization. for instance, deep feedforward neural networks are important in system identification and"
  },
  {
    "chunk_id": 202,
    "doc_id": "Neural_Networks.txt",
    "text": "on content.250251 this has implications for automated customer service, content moderation, and language understanding technologies.252 in the domain of control systems, anns are used to model dynamic systems for tasks such as system identification, control design, and optimization. for instance, deep feedforward neural networks are important in system identification and control applications.253 anns are used for stock market prediction and credit scoring: in investing, anns can process vast amounts of financial data, recognize complex patterns, and forecast stock market trends, aiding investors and risk managers in making informed decisions.250 in credit scoring, anns offer data-driven, personalized assessments of creditworthiness, improving the accuracy of default predictions and automating the lending process.251 anns require high-quality data and careful tuning, and their black-box nature can pose challenges in interpretation. nevertheless, ongoing advancements suggest that anns continue to play a role in finance, offering valuable insights and enhancing risk management strategies. anns are able to process and analyze vast medical datasets. they enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.251 in drug discovery, anns speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.250 additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.251 ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ann applications in medicine. anns such as generative adversarial networks (gan) and transformers are used for content creation across numerous industries.254 this is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. for instance, dall-e is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.255 in the field of music, transformers are used to create original music for commercials and natural language processing control systems finance medicine content creation 28122025 23:07 neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 2146 documentaries through companies such as aiva and jukedeck. 256 in the marketing industry, generative models are used to create personalized advertisements for consumers.254 additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as"
  },
  {
    "chunk_id": 203,
    "doc_id": "Neural_Networks.txt",
    "text": "neural network (machine learning) - wikipedia https:en.wikipedia.orgwikineuralnetwork(machinelearning) 2146 documentaries through companies such as aiva and jukedeck. 256 in the marketing industry, generative models are used to create personalized advertisements for consumers.254 additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between warner bros and technology company cinelytic established in 2020.257 furthermore, neural networks have found uses in video game creation, where non-player characters (npcs) can make decisions based on all the characters currently in the game.258"
  },
  {
    "chunk_id": 204,
    "doc_id": "General_Relativity.txt",
    "text": "general relativity, also known as the general theory of relativity, and as einsteins theory of gravity, is the geometric theory of gravitation published by albert einstein in may 1916 and is the accepted description of gravitation in modern physics. general relativity generalizes special relativity and refines newtons law of universal gravitation, providing a unified description of gravity as a geometric property of space and time, or four-dimensional spacetime. in particular, the curvature of spacetime is directly related to the energy, momentum and stress of whatever is present, including matter and radiation. the relation is specified by the einstein field equations, a system of second-order partial differential equations. newtons law of universal gravitation, which describes gravity in classical mechanics, can be seen as a prediction of general relativity for the almost flat spacetime geometry around stationary mass distributions. some predictions of general relativity, however, are beyond newtons law of universal gravitation in classical physics. these predictions concern the passage of time, the geometry of space, the motion of bodies in free fall, and the propagation of light, and include gravitational time dilation, gravitational lensing, the gravitational redshift of light, the shapiro time delay and singularitiesblack holes. so far, all tests of general relativity have been in agreement with the theory. the time-dependent solutions of general relativity enable us to extrapolate the history of the universe into the past and future, and have provided the modern framework for cosmology, thus leading to the discovery of the big bang and cosmic microwave background radiation. despite the introduction of a number of alternative theories, general relativity continues to be the simplest theory consistent with experimental data. reconciliation of general relativity with the laws of quantum physics remains a problem, however, as no self-consistent theory of quantum gravity has been found. it is not yet known how gravity can be unified with the three non-gravitational interactions: strong, weak and electromagnetic. einsteins theory has astrophysical implications, including the prediction of black holesregions of space in which space and time are distorted in such a way that nothing, not even light, can escape from them. black holes are the end-state for massive stars. microquasars and active galactic nuclei are believed to be stellar black holes and supermassive black holes. it also predicts gravitational lensing, where the bending of light results in distorted and multiple images of the same distant astronomical phenomenon. other predictions include the existence of"
  },
  {
    "chunk_id": 205,
    "doc_id": "General_Relativity.txt",
    "text": "are the end-state for massive stars. microquasars and active galactic nuclei are believed to be stellar black holes and supermassive black holes. it also predicts gravitational lensing, where the bending of light results in distorted and multiple images of the same distant astronomical phenomenon. other predictions include the existence of gravitational waves, which have been observed directly by the physics collaboration ligo and other observatories. in addition, general relativity has provided the basis for cosmological models of an expanding universe. widely acknowledged as a theory of extraordinary beauty, general relativity has often been described as the most beautiful of all existing physical theories.2 history main articles: history of general relativity and classical theories of gravitation henri poincars 1905 theory of the dynamics of the electron was a relativistic theory which he applied to all forces, including gravity. while others thought that gravity was instantaneous or of electromagnetic origin, he suggested that relativity was something due to our methods of measurement. in his theory, he showed that gravitational waves propagate at the speed of light.3 soon afterwards, einstein started thinking about how to incorporate gravity into his relativistic framework. in 1907, beginning with a simple thought experiment involving an observer in free fall (ffo), he embarked on what would be an eight-year search for a relativistic theory of gravity. after numerous detours and false starts, his work culminated in the presentation to the prussian academy of science in november 1915 of what are known as the einstein field equations, which form the core of einsteins general theory of relativity.4 these equations specify how the geometry of space and time is influenced by whatever matter and radiation are present.5 a version of non-euclidean geometry, called riemannian geometry, enabled einstein to develop general relativity by providing the key mathematical framework on which he fit his physical ideas of gravity.6 this idea was pointed out by mathematician marcel grossmann and published by grossmann and einstein in 1913.7 the einstein field equations are nonlinear and are considered difficult to solve. einstein used approximation methods in working out initial predictions of the theory. but in 1916, the astrophysicist karl schwarzschild found the first non-trivial exact solution to the einstein field equations, the schwarzschild metric. this solution laid the groundwork for the description of the final stages of gravitational collapse, and the objects known today as black holes. in the same year, the first steps towards generalizing"
  },
  {
    "chunk_id": 206,
    "doc_id": "General_Relativity.txt",
    "text": "the astrophysicist karl schwarzschild found the first non-trivial exact solution to the einstein field equations, the schwarzschild metric. this solution laid the groundwork for the description of the final stages of gravitational collapse, and the objects known today as black holes. in the same year, the first steps towards generalizing schwarzschilds solution to electrically charged objects were taken, eventually resulting in the reissnernordstrm solution, which is associated with electrically charged black holes.8 in 1917, einstein applied his theory to the universe as a whole, initiating the field of relativistic cosmology. in line with contemporary thinking, he assumed a static universe, adding a new parameter to his original field equationsthe cosmological constantto match that observational presumption.9 by 1929, however, the work of hubble and others had shown that the universe is expanding. this is readily described by the expanding cosmological solutions found by friedmann in 1922, which do not require a cosmological constant. lematre used these solutions to formulate the earliest version of the big bang models, in which the universe has evolved from an extremely hot and dense earlier state.10 einstein later declared the cosmological constant the biggest blunder of his life.11 during that period, general relativity remained something of a curiosity among physical theories. it was clearly superior to newtonian gravity, being consistent with special relativity and accounting for several effects unexplained by the newtonian theory. einstein showed in 1915 how his theory explained the anomalous perihelion advance of the planet mercury without any arbitrary parameters (fudge factors),12 and in 1919 an expedition led by eddington confirmed general relativitys prediction for the deflection of starlight by the sun during the total solar eclipse of 29 may 1919,13 instantly making einstein famous.14 yet the theory remained outside the mainstream of theoretical physics and astrophysics until developments between approximately 1960 and 1975 known as the golden age of general relativity.15 physicists began to understand the concept of a black hole, and to identify quasars as one of these objects astrophysical manifestations.16 ever more precise solar system tests confirmed the theorys predictive power,17 and relativistic cosmology also became amenable to direct observational tests.18 general relativity has acquired a reputation as a theory of extraordinary beauty.21920 subrahmanyan chandrasekhar has noted that at multiple levels, general relativity exhibits what francis bacon has termed a strangeness in the proportion (i.e. elements that excite wonderment and surprise). it juxtaposes fundamental concepts (space and time versus matter"
  },
  {
    "chunk_id": 207,
    "doc_id": "General_Relativity.txt",
    "text": "tests.18 general relativity has acquired a reputation as a theory of extraordinary beauty.21920 subrahmanyan chandrasekhar has noted that at multiple levels, general relativity exhibits what francis bacon has termed a strangeness in the proportion (i.e. elements that excite wonderment and surprise). it juxtaposes fundamental concepts (space and time versus matter and motion) which had previously been considered as entirely independent. chandrasekhar also noted that einsteins only guides in his search for an exact theory were the principle of equivalence and his sense that a proper description of gravity should be geometrical at its basis, so that there was an element of revelation in the manner in which einstein arrived at his theory.21 other elements of beauty associated with the general theory of relativity are its simplicity and symmetry, the manner in which it incorporates invariance and unification, and its perfect logical consistency.22 in the preface to relativity: the special and the general theory, einstein said the present book is intended, as far as possible, to give an exact insight into the theory of relativity to those readers who, from a general scientific and philosophical point of view, are interested in the theory, but who are not conversant with the mathematical apparatus of theoretical physics. the work presumes a standard of education corresponding to that of a university matriculation examination, and, despite the shortness of the book, a fair amount of patience and force of will on the part of the reader. the author has spared himself no pains in his endeavour to present the main ideas in the simplest and most intelligible form, and on the whole, in the sequence and connection in which they actually originated.23 from classical mechanics to general relativity general relativity can be understood by examining its similarities with and departures from classical physics. the first step is the realization that classical mechanics and newtons law of gravity admit a geometric description. the combination of this description with the laws of special relativity results in a heuristic derivation of general relativity.2425 geometry of newtonian gravity according to general relativity, objects in a gravitational field behave similarly to objects within an accelerating enclosure. for example, an observer will see a ball fall the same way in a rocket (left) as it does on earth (right), provided that the acceleration of the rocket is equal to 9.8 ms2 (the acceleration due to gravity on the surface of the"
  },
  {
    "chunk_id": 208,
    "doc_id": "General_Relativity.txt",
    "text": "to objects within an accelerating enclosure. for example, an observer will see a ball fall the same way in a rocket (left) as it does on earth (right), provided that the acceleration of the rocket is equal to 9.8 ms2 (the acceleration due to gravity on the surface of the earth). at the base of classical mechanics is the notion that a bodys motion can be described as a combination of free (or inertial) motion, and deviations from this free motion. such deviations are caused by external forces acting on a body in accordance with newtons second law of motion, which states that the net force acting on a body is equal to that bodys (inertial) mass multiplied by its acceleration.26 the preferred inertial motions are related to the geometry of space and time: in the standard reference frames of classical mechanics, objects in free motion move along straight lines at constant speed. in modern parlance, their paths are geodesics, straight world lines in curved spacetime.27 conversely, one might expect that inertial motions, once identified by observing the actual motions of bodies and making allowances for the external forces (such as electromagnetism or friction), can be used to define the geometry of space, as well as a time coordinate. however, there is an ambiguity once gravity comes into play. according to newtons law of gravity, and independently verified by experiments such as that of etvs and its successors (see etvs experiment), there is a universality of free fall (also known as the weak equivalence principle, or the universal equality of inertial and passive-gravitational mass): the trajectory of a test body in free fall depends only on its position and initial speed, but not on any of its material properties.28 a simplified version of this is embodied in einsteins elevator experiment, illustrated in the figure on the right: for an observer in an enclosed room, it is impossible to decide, by mapping the trajectory of bodies such as a dropped ball, whether the room is stationary in a gravitational field and the ball accelerating, or in free space aboard a rocket that is accelerating at a rate equal to that of the gravitational field versus the ball which upon release has nil acceleration.29 given the universality of free fall, there is no observable distinction between inertial motion and motion under the influence of the gravitational force. this suggests the definition of"
  },
  {
    "chunk_id": 209,
    "doc_id": "General_Relativity.txt",
    "text": "that is accelerating at a rate equal to that of the gravitational field versus the ball which upon release has nil acceleration.29 given the universality of free fall, there is no observable distinction between inertial motion and motion under the influence of the gravitational force. this suggests the definition of a new class of inertial motion, namely that of objects in free fall under the influence of gravity. this new class of preferred motions, too, defines a geometry of space and timein mathematical terms, it is the geodesic motion associated with a specific connection which depends on the gradient of the gravitational potential. space, in this construction, still has the ordinary euclidean geometry. however, spacetime as a whole is more complicated. as can be shown using simple thought experiments following the free-fall trajectories of different test particles, the result of transporting spacetime vectors that can denote a particles velocity (time-like vectors) will vary with the particles trajectory; mathematically speaking, the newtonian connection is not integrable. from this, one can deduce that spacetime is curved. the resulting newtoncartan theory is a geometric formulation of newtonian gravity using only covariant concepts, i.e. a description which is valid in any desired coordinate system.30 in this geometric description, tidal effectsthe relative acceleration of bodies in free fallare related to the derivative of the connection, showing how the modified geometry is caused by the presence of mass.31 relativistic generalization light cone of event a as intriguing as geometric newtonian gravity may be, its basis, classical mechanics, is merely a limiting case of (special) relativistic mechanics.32 in the language of symmetry: where gravity can be neglected, physics is lorentz invariant as in special relativity rather than galilei invariant as in classical mechanics. (the defining symmetry of special relativity is the poincar group, which includes translations, rotations, boosts and reflections.) the differences between the two become significant when dealing with speeds approaching the speed of light, and with high-energy phenomena.33 with lorentz symmetry, additional structures come into play. they are defined by the set of light cones (see image). the light-cones define a causal structure: for each event a, there is a set of events that can, in principle, either influence or be influenced by a via signals or interactions that do not need to travel faster than light (such as event b in the image), and a set of events for which such an influence is"
  },
  {
    "chunk_id": 210,
    "doc_id": "General_Relativity.txt",
    "text": "event a, there is a set of events that can, in principle, either influence or be influenced by a via signals or interactions that do not need to travel faster than light (such as event b in the image), and a set of events for which such an influence is impossible (such as event c in the image). these sets are observer-independent.34 in conjunction with the world-lines of freely falling particles, the light-cones can be used to reconstruct the spacetimes semi-riemannian metric, at least up to a positive scalar factor. in mathematical terms, this defines a conformal structure35 or conformal geometry. special relativity is defined in the absence of gravity. for practical applications, it is a suitable model whenever gravity can be neglected. bringing gravity into play, and assuming the universality of free fall motion, an analogous reasoning as in the previous section applies: there are no global inertial frames. instead there are approximate inertial frames moving alongside freely falling particles. translated into the language of spacetime: the straight time-like lines that define a gravity-free inertial frame are deformed to lines that are curved relative to each other, suggesting that the inclusion of gravity necessitates a change in spacetime geometry.36 a priori, it is not clear whether the new local frames in free fall coincide with the reference frames in which the laws of special relativity holdthat theory is based on the propagation of light, and thus on electromagnetism, which could have a different set of preferred frames. but using different assumptions about the special-relativistic frames (such as their being earth-fixed, or in free fall), one can derive different predictions for the gravitational redshift, that is, the way in which the frequency of light shifts as the light propagates through a gravitational field (cf. below). the actual measurements show that free-falling frames are the ones in which light propagates as it does in special relativity.37 the generalization of this statement, namely that the laws of special relativity hold to good approximation in freely falling (and non-rotating) reference frames, is known as the einstein equivalence principle, a crucial guiding principle for generalizing special-relativistic physics to include gravity.38 the same experimental data shows that time as measured by clocks in a gravitational fieldproper time, to give the technical termdoes not follow the rules of special relativity. in the language of spacetime geometry, it is not measured by the minkowski metric. as in"
  },
  {
    "chunk_id": 211,
    "doc_id": "General_Relativity.txt",
    "text": "generalizing special-relativistic physics to include gravity.38 the same experimental data shows that time as measured by clocks in a gravitational fieldproper time, to give the technical termdoes not follow the rules of special relativity. in the language of spacetime geometry, it is not measured by the minkowski metric. as in the newtonian case, this is suggestive of a more general geometry. at small scales, all reference frames that are in free fall are equivalent, and approximately minkowskian. consequently, we are dealing with a curved generalization of minkowski space. the metric tensor that defines the geometryin particular, how lengths and angles are measuredis not the minkowski metric of special relativity, it is a generalization known as a semi- or pseudo-riemannian metric. furthermore, each riemannian metric is naturally associated with one particular kind of connection, the levi-civita connection, and this is, in fact, the connection that satisfies the equivalence principle and makes space locally minkowskian (that is, in suitable locally inertial coordinates, the metric is minkowskian, and its first partial derivatives and the connection coefficients vanish).39 einsteins equations main articles: einstein field equations and mathematics of general relativity having formulated the relativistic, geometric version of the effects of gravity, the question of gravitys source remains. in newtonian gravity, the source is mass. in special relativity, mass turns out to be part of a more general quantity called the stressenergy tensor, which includes both energy and momentum densities as well as stress: pressure and shear.40 using the equivalence principle, this tensor is readily generalized to curved spacetime. drawing further upon the analogy with geometric newtonian gravity, it is natural to assume that the field equation for gravity relates this tensor and the ricci tensor, which describes a particular class of tidal effects: the change in volume for a small cloud of test particles that are initially at rest, and then fall freely. in special relativity, conservation of energymomentum corresponds to the statement that the stressenergy tensor is divergence-free. this formula, too, is readily generalized to curved spacetime by replacing partial derivatives with their curved-manifold counterparts, covariant derivatives studied in differential geometry. with this additional conditionthe covariant divergence of the stressenergy tensor, and hence of whatever is on the other side of the equation, is zerothe simplest nontrivial set of equations are what are called einsteins (field) equations: einsteins field equations displaystyle gmu nu equiv rmu nu -textstyle 1 over 2r,gmu nu kappa tmu"
  },
  {
    "chunk_id": 212,
    "doc_id": "General_Relativity.txt",
    "text": "additional conditionthe covariant divergence of the stressenergy tensor, and hence of whatever is on the other side of the equation, is zerothe simplest nontrivial set of equations are what are called einsteins (field) equations: einsteins field equations displaystyle gmu nu equiv rmu nu -textstyle 1 over 2r,gmu nu kappa tmu nu , on the left-hand side is the einstein tensor, displaystyle gmu nu , which is symmetric and a specific divergence-free combination of the ricci tensor displaystyle rmu nu and the metric. in particular, displaystyle rgmu nu rmu nu is the curvature scalar. the ricci tensor itself is related to the more general riemann curvature tensor as displaystyle rmu nu ralpha mu alpha nu . on the right-hand side, displaystyle kappa is a constant and displaystyle tmu nu is the stressenergy tensor. all tensors are written in abstract index notation.41 matching the theorys prediction to observational results for planetary orbits or, equivalently, assuring that the weak-gravity, low-speed limit is newtonian mechanics, the proportionality constant displaystyle kappa is found to be textstyle kappa 8pi gc4, where displaystyle g is the newtonian constant of gravitation and displaystyle c the speed of light in vacuum.42 when there is no matter present, so that the stressenergy tensor vanishes, the results are the vacuum einstein equations, displaystyle rmu nu 0. in general relativity, the world line of a particle free from all external, non-gravitational force is a particular type of geodesic in curved spacetime. in other words, a freely moving or falling particle always moves along a geodesic. the geodesic equation is: displaystyle d2xmu over ds2gamma mu alpha beta dxalpha over dsdxbeta over ds0, where displaystyle s is a scalar parameter of motion (e.g. the proper time), and displaystyle gamma mu alpha beta are christoffel symbols (sometimes called the affine connection coefficients or levi-civita connection coefficients) which is symmetric in the two lower indices. greek indices may take the values: 0, 1, 2, 3 and the summation convention is used for repeated indices displaystyle alpha and displaystyle beta . the quantity on the left-hand-side of this equation is the acceleration of a particle, and so this equation is analogous to newtons laws of motion which likewise provide formulae for the acceleration of a particle. this equation of motion employs the einstein notation, meaning that repeated indices are summed (i.e. from zero to three). the christoffel symbols are functions of the four spacetime coordinates, and so"
  },
  {
    "chunk_id": 213,
    "doc_id": "General_Relativity.txt",
    "text": "this equation is analogous to newtons laws of motion which likewise provide formulae for the acceleration of a particle. this equation of motion employs the einstein notation, meaning that repeated indices are summed (i.e. from zero to three). the christoffel symbols are functions of the four spacetime coordinates, and so are independent of the velocity or acceleration or other characteristics of a test particle whose motion is described by the geodesic equation. total force in general relativity see also: two-body problem in general relativity in general relativity, the effective gravitational potential energy of an object of mass m revolving around a massive central body m is given by4344 displaystyle uf(r)-frac gmmrfrac l22mr2-frac gml2mc2r3 a conservative total force can then be obtained as its negative gradient displaystyle ff(r)-frac gmmr2frac l2mr3-frac 3gml2mc2r4 where l is the angular momentum. the first term represents the force of newtonian gravity, which is described by the inverse-square law. the second term represents the centrifugal force in the circular motion. the third term represents the relativistic effect. alternatives to general relativity main article: alternatives to general relativity there are alternatives to general relativity built upon the same premises, which include additional rules andor constraints, leading to different field equations. examples are whiteheads theory, bransdicke theory, teleparallelism, f(r) gravity and einsteincartan theory.45 definition and basic applications see also: mathematics of general relativity and physical theories modified by general relativity the derivation outlined in the previous section contains all the information needed to define general relativity, describe its key properties, and address a question of crucial importance in physics, namely how the theory can be used for model-building. definition and basic properties general relativity is a metric theory of gravitation. at its core are einsteins equations, which describe the relation between the geometry of a four-dimensional pseudo-riemannian manifold representing spacetime, and the distribution of energy, momentum and stress contained in that spacetime.46 phenomena that in classical mechanics are ascribed to the action of the force of gravity (such as free-fall, orbital motion, and spacecraft trajectories), correspond to inertial motion within a curved geometry of spacetime in general relativity; there is no gravitational force deflecting objects from their natural, straight paths. instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow.47 the curvature is, in turn, caused by the stressenergy of matter. paraphrasing the relativist john"
  },
  {
    "chunk_id": 214,
    "doc_id": "General_Relativity.txt",
    "text": "is no gravitational force deflecting objects from their natural, straight paths. instead, gravity corresponds to changes in the properties of space and time, which in turn changes the straightest-possible paths that objects will naturally follow.47 the curvature is, in turn, caused by the stressenergy of matter. paraphrasing the relativist john archibald wheeler, spacetime tells matter how to move; matter tells spacetime how to curve.48 while general relativity replaces the scalar gravitational potential of classical physics by a symmetric rank-two tensor, the latter reduces to the former in certain limiting cases. for weak gravitational fields and low speed relative to the speed of light, the theorys predictions converge on those of newtons law of universal gravitation.49 as it is constructed using tensors, general relativity exhibits general covariance: its lawsand further laws formulated within the general relativistic frameworktake on the same form in all coordinate systems.50 furthermore, the theory does not contain any invariant geometric background structures, i.e. it is background-independent. it thus satisfies a more stringent general principle of relativity, namely that the laws of physics are the same for all observers.51 locally, as expressed in the equivalence principle, spacetime is minkowskian, and the laws of physics exhibit local lorentz invariance.52 model-building and basic applications the core concept of general-relativistic model-building is that of a solution of einsteins equations. given both einsteins equations and suitable equations for the properties of matter, such a solution consists of a specific semi-riemannian manifold (usually defined by giving the metric in specific coordinates), and specific matter fields defined on that manifold. matter and geometry must satisfy einsteins equations, so in particular, the matters stressenergy tensor must be divergence-free. the matter must, of course, also satisfy whatever additional equations were imposed on its properties. in short, such a solution is a model universe that satisfies the laws of general relativity, and possibly additional laws governing whatever matter might be present.53 einsteins equations are nonlinear partial differential equations and, as such, difficult to solve exactly.54 nevertheless, a number of exact solutions are known, although only a few have direct physical applications.55 the best-known exact solutions, and also those most interesting from a physics point of view, are the schwarzschild solution, the reissnernordstrm solution and the kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe,56 and the friedmannlematrerobertsonwalker and de sitter universes, each describing an expanding cosmos.57 exact solutions of great"
  },
  {
    "chunk_id": 215,
    "doc_id": "General_Relativity.txt",
    "text": "those most interesting from a physics point of view, are the schwarzschild solution, the reissnernordstrm solution and the kerr metric, each corresponding to a certain type of black hole in an otherwise empty universe,56 and the friedmannlematrerobertsonwalker and de sitter universes, each describing an expanding cosmos.57 exact solutions of great theoretical interest include the gdel universe (which opens up the intriguing possibility of time travel in curved spacetimes), the taubnut solution (a model universe that is homogeneous, but anisotropic), and anti-de sitter space (which has recently come to prominence in the context of what is called the maldacena conjecture).58 given the difficulty of finding exact solutions, einsteins field equations are also solved frequently by numerical integration on a computer, or by considering small perturbations of exact solutions. in the field of numerical relativity, powerful computers are employed to simulate the geometry of spacetime and to solve einsteins equations for interesting situations such as two colliding black holes.59 in principle, such methods may be applied to any system, given sufficient computer resources, and may address fundamental questions such as naked singularities. approximate solutions may also be found by perturbation theories such as linearized gravity60 and its generalization, the post-newtonian expansion, both of which were developed by einstein. the latter provides a systematic approach to solving for the geometry of a spacetime that contains a distribution of matter that moves slowly compared with the speed of light. the expansion involves a series of terms; the first terms represent newtonian gravity, whereas the later terms represent ever smaller corrections to newtons theory due to general relativity.61 an extension of this expansion is the parametrized post-newtonian (ppn) formalism, which allows quantitative comparisons between the predictions of general relativity and alternative theories.62 consequences of einsteins theory general relativity has a number of physical consequences. some follow directly from the theorys axioms, whereas others have become clear only in the course of many years of research that followed einsteins initial publication. gravitational time dilation and frequency shift main article: gravitational time dilation schematic representation of the gravitational redshift of a light wave escaping from the surface of a massive body assuming that the equivalence principle holds,63 gravity influences the passage of time. light sent down into a gravity well is blueshifted, whereas light sent in the opposite direction (i.e., climbing out of the gravity well) is redshifted; collectively, these two effects are known as the gravitational"
  },
  {
    "chunk_id": 216,
    "doc_id": "General_Relativity.txt",
    "text": "of a massive body assuming that the equivalence principle holds,63 gravity influences the passage of time. light sent down into a gravity well is blueshifted, whereas light sent in the opposite direction (i.e., climbing out of the gravity well) is redshifted; collectively, these two effects are known as the gravitational frequency shift. more generally, processes close to a massive body run more slowly when compared with processes taking place farther away; this effect is known as gravitational time dilation.64 gravitational redshift has been measured in the laboratory65 and using astronomical observations.66 gravitational time dilation in the earths gravitational field has been measured numerous times using atomic clocks,67 while ongoing validation is provided as a side effect of the operation of the global positioning system (gps).68 tests in stronger gravitational fields are provided by the observation of binary pulsars.69 all results are in agreement with general relativity.70 however, at the existing level of accuracy, these observations cannot distinguish between general relativity and other theories in which the equivalence principle is valid.71 in the vicinity of a non-rotating sphere, the time dilation due to gravity, derived from the schwarzschild metric, is displaystyle t0tfsqrt 1-frac 2gmrc2 where displaystyle t0 is the proper time between two events for an observer close to the massive sphere, i.e. deep within the gravitational field displaystyle tf is the coordinate time between the events for an observer at an arbitrarily large distance from the massive object (this assumes the far-away observer is using schwarzschild coordinates, a coordinate system where a clock at infinite distance from the massive sphere would tick at one second per second of coordinate time, while closer clocks would tick at less than that rate), displaystyle g is the gravitational constant, displaystyle m is the mass of the object creating the gravitational field, displaystyle r is the radial coordinate of the observer within the gravitational field (this coordinate is analogous to the classical distance from the center of the object, but is actually a schwarzschild coordinate; the equation in this form has real solutions for displaystyle rrrm s), displaystyle c is the speed of light. light deflection and gravitational time delay main articles: schwarzschild geodesics, kepler problem in general relativity, gravitational lens, and shapiro delay deflection of light (sent out from the location shown in blue) near a compact body (shown in gray) general relativity predicts that the path of light will follow the curvature"
  },
  {
    "chunk_id": 217,
    "doc_id": "General_Relativity.txt",
    "text": "light deflection and gravitational time delay main articles: schwarzschild geodesics, kepler problem in general relativity, gravitational lens, and shapiro delay deflection of light (sent out from the location shown in blue) near a compact body (shown in gray) general relativity predicts that the path of light will follow the curvature of spacetime as it passes near a massive object. this effect was initially confirmed by observing the light of stars or distant quasars being deflected as it passes the sun.72 this and related predictions follow from the fact that light follows what is called a light-like or null geodesica generalization of the straight lines along which light travels in classical physics. such geodesics are the generalization of the invariance of lightspeed in special relativity.73 as one examines suitable model spacetimes (either the exterior schwarzschild solution or, for more than a single mass, the post-newtonian expansion),74 several effects of gravity on light propagation emerge. although the bending of light can also be derived by extending the universality of free fall to light,75 the angle of deflection resulting from such calculations is only half the value given by general relativity.76 closely related to light deflection is the shapiro time delay, the phenomenon that light signals take longer to move through a gravitational field than they would in the absence of that field. there have been numerous successful tests of this prediction.77 in the parameterized post-newtonian formalism (ppn), measurements of both the deflection of light and the gravitational time delay determine a parameter called , which encodes the influence of gravity on the geometry of space.78 gravitational waves main article: gravitational wave ring of test particles deformed by a passing (linearized, amplified for better visibility) gravitational wave predicted in 19167980 by albert einstein, there are gravitational waves: ripples in the metric of spacetime that propagate at the speed of light. these are one of several analogies between weak-field gravity and electromagnetism in that, they are analogous to electromagnetic waves. on 11 february 2016, the advanced ligo team announced that they had directly detected gravitational waves from a pair of black holes merging.818283 the simplest type of such a wave can be visualized by its action on a ring of freely floating particles. a sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right).84 since einsteins equations are non-linear, arbitrarily strong"
  },
  {
    "chunk_id": 218,
    "doc_id": "General_Relativity.txt",
    "text": "simplest type of such a wave can be visualized by its action on a ring of freely floating particles. a sine wave propagating through such a ring towards the reader distorts the ring in a characteristic, rhythmic fashion (animated image to the right).84 since einsteins equations are non-linear, arbitrarily strong gravitational waves do not obey linear superposition, making their description difficult. however, linear approximations of gravitational waves are sufficiently accurate to describe the exceedingly weak waves that are expected to arrive here on earth from far-off cosmic events, which typically result in relative distances increasing and decreasing by 1021 or less. data analysis methods routinely make use of the fact that these linearized waves can be fourier decomposed.85 some exact solutions describe gravitational waves without any approximation, e.g., a wave train traveling through empty space86 or gowdy universes, varieties of an expanding cosmos filled with gravitational waves.87 but for gravitational waves produced in astrophysically relevant situations, such as the merger of two black holes, numerical methods are the only way to construct appropriate models.88 orbital effects and the relativity of direction main article: two-body problem in general relativity general relativity differs from classical mechanics in a number of predictions concerning orbiting bodies. it predicts an overall rotation (precession) of planetary orbits, as well as orbital decay caused by the emission of gravitational waves and effects related to the relativity of direction. precession of apsides newtonian (red) vs. einsteinian orbit (blue) of a lone planet orbiting a star. the influence of other planets is ignored. main article: apsidal precession in general relativity, the apsides of any orbit (the point of the orbiting bodys closest approach to the systems center of mass) will precess; the orbit is not an ellipse, but akin to an ellipse that rotates on its focus, resulting in a rose curve-like shape (see image). einstein first derived this result by using an approximate metric representing the newtonian limit and treating the orbiting body as a test particle. for him, the fact that his theory gave a straightforward explanation of mercurys anomalous perihelion shift, discovered earlier by urbain le verrier in 1859, was important evidence that he had at last identified the correct form of the gravitational field equations.89 the effect can also be derived by using either the exact schwarzschild metric (describing spacetime around a spherical mass)90 or the much more general post-newtonian formalism.91 it is due to"
  },
  {
    "chunk_id": 219,
    "doc_id": "General_Relativity.txt",
    "text": "verrier in 1859, was important evidence that he had at last identified the correct form of the gravitational field equations.89 the effect can also be derived by using either the exact schwarzschild metric (describing spacetime around a spherical mass)90 or the much more general post-newtonian formalism.91 it is due to the influence of gravity on the geometry of space and to the contribution of self-energy to a bodys gravity (encoded in the nonlinearity of einsteins equations).92 relativistic precession has been observed for all planets that allow for accurate precession measurements (mercury, venus, and earth),93 as well as in binary pulsar systems, where it is larger by five orders of magnitude.94 in general relativity the perihelion shift displaystyle sigma , expressed in radians per revolution, is approximately given by:95 displaystyle sigma frac 24pi 3l2t2c2(1-e2) , where: displaystyle l is the semi-major axis displaystyle t is the orbital period displaystyle c is the speed of light in a vacuum displaystyle e is the orbital eccentricity orbital decay orbital decay for psr j07373039: time shift, tracked over 16 years (2021).96 according to general relativity, a binary system will emit gravitational waves, thereby losing energy. due to this loss, the distance between the two orbiting bodies decreases, and so does their orbital period. within the solar system or for ordinary double stars, the effect is too small to be observable. this is not the case for a close binary pulsar, a system of two orbiting neutron stars, one of which is a pulsar: from the pulsar, observers on earth receive a regular series of radio pulses that can serve as a highly accurate clock, which allows precise measurements of the orbital period. because neutron stars are immensely compact, significant amounts of energy are emitted in the form of gravitational radiation.97 the first observation of a decrease in orbital period due to the emission of gravitational waves was made by hulse and taylor, using the binary pulsar psr191316 they had discovered in 1974. this was the first detection of gravitational waves, albeit indirect, for which they were awarded the 1993 nobel prize in physics.98 since then, several other binary pulsars have been found, in particular the double pulsar psr j07373039, where both stars are pulsars99 and which was last reported to also be in agreement with general relativity in 2021 after 16 years of observations.96 geodetic precession and frame-dragging main articles: geodetic precession and frame"
  },
  {
    "chunk_id": 220,
    "doc_id": "General_Relativity.txt",
    "text": "then, several other binary pulsars have been found, in particular the double pulsar psr j07373039, where both stars are pulsars99 and which was last reported to also be in agreement with general relativity in 2021 after 16 years of observations.96 geodetic precession and frame-dragging main articles: geodetic precession and frame dragging several relativistic effects are directly related to the relativity of direction.100 one is geodetic precession: the axis direction of a gyroscope in free fall in curved spacetime will change when compared, for instance, with the direction of light received from distant starseven though such a gyroscope represents the way of keeping a direction as stable as possible (parallel transport).101 for the moonearth system, this effect has been measured with the help of lunar laser ranging.102 more recently, it has been measured for test masses aboard the satellite gravity probe b to a precision of better than 0.3.103104 near a rotating mass, there are gravitomagnetic or frame-dragging effects. a distant observer will determine that objects close to the mass get dragged around. this is most extreme for rotating black holes where, for any object entering a zone known as the ergosphere, rotation is inevitable.105 such effects can again be tested through their influence on the orientation of gyroscopes in free fall.106 somewhat controversial tests have been performed using the lageos satellites, confirming the relativistic prediction.107 also the mars global surveyor probe around mars has been used.108 astrophysical applications gravitational lensing main article: gravitational lensing einstein cross: four images of the same astronomical object, produced by a gravitational lens the deflection of light by gravity is responsible for a new class of astronomical phenomena. if a massive object is situated between the astronomer and a distant target object with appropriate mass and relative distances, the astronomer will see multiple distorted images of the target. such effects are known as gravitational lensing.109 depending on the configuration, scale, and mass distribution, there can be two or more images, a bright ring known as an einstein ring, or partial rings called arcs.110 the earliest example was discovered in 1979;111 since then, more than a hundred gravitational lenses have been observed.112 even if the multiple images are too close to each other to be resolved, the effect can still be measured, e.g., as an overall brightening of the target object; a number of such microlensing events have been observed.113 gravitational lensing has developed into a"
  },
  {
    "chunk_id": 221,
    "doc_id": "General_Relativity.txt",
    "text": "hundred gravitational lenses have been observed.112 even if the multiple images are too close to each other to be resolved, the effect can still be measured, e.g., as an overall brightening of the target object; a number of such microlensing events have been observed.113 gravitational lensing has developed into a tool of observational astronomy. it is used to detect the presence and distribution of dark matter, provide a natural telescope for observing distant galaxies, and to obtain an independent estimate of the hubble constant. statistical evaluations of lensing data provide valuable insight into the structural evolution of galaxies.114 gravitational-wave astronomy main articles: gravitational wave and gravitational-wave astronomy artists impression of the space-borne gravitational wave detector lisa observations of binary pulsars provide strong indirect evidence for the existence of gravitational waves (see orbital decay, above). detection of these waves is a major goal of contemporary relativity-related research.115 several land-based gravitational wave detectors are in operation, for example the interferometric detectors geo 600, ligo (two detectors), tama 300 and virgo.116 various pulsar timing arrays are using millisecond pulsars to detect gravitational waves in the 109 to 106 hertz frequency range, which originate from binary supermassive blackholes.117 a european space-based detector, elisa ngo, is under development,118 with a precursor mission (lisa pathfinder) having launched in december 2015.119 observations of gravitational waves promise to complement observations in the electromagnetic spectrum.120 they are expected to yield information about black holes and other dense objects such as neutron stars and white dwarfs, about certain kinds of supernova implosions, and about processes in the very early universe, including the signature of certain types of hypothetical cosmic string.121 in february 2016, the advanced ligo team announced that they had detected gravitational waves from a black hole merger.818283 black holes and other compact objects main article: black hole simulation based on the equations of general relativity: a star collapsing to form a black hole while emitting gravitational waves whenever the ratio of an objects mass to its radius becomes sufficiently large, general relativity predicts the formation of a black hole, a region of space from which nothing, not even light, can escape. in the accepted models of stellar evolution, neutron stars of around 1.4 solar masses, and stellar black holes with a few to a few dozen solar masses, are thought to be the final state for the evolution of massive stars.122 usually a galaxy has one supermassive black"
  },
  {
    "chunk_id": 222,
    "doc_id": "General_Relativity.txt",
    "text": "light, can escape. in the accepted models of stellar evolution, neutron stars of around 1.4 solar masses, and stellar black holes with a few to a few dozen solar masses, are thought to be the final state for the evolution of massive stars.122 usually a galaxy has one supermassive black hole with a few million to a few billion solar masses in its center,123 and its presence is thought to have played an important role in the formation of the galaxy and larger cosmic structures.124 astronomically, the most important property of compact objects is that they provide a supremely efficient mechanism for converting gravitational energy into electromagnetic radiation.125 accretion, the falling of dust or gaseous matter onto stellar or supermassive black holes, is thought to be responsible for some spectacularly luminous astronomical objects, especially diverse kinds of active galactic nuclei on galactic scales and stellar-size objects such as microquasars.126 in particular, accretion can lead to relativistic jets, focused beams of highly energetic particles that are being flung into space at almost light speed.127 general relativity plays a central role in modelling all these phenomena,128 and observations provide strong evidence for the existence of black holes with the properties predicted by the theory.129 black holes are also sought-after targets in the search for gravitational waves (cf. section gravitational waves, above). merging black hole binaries should lead to some of the strongest gravitational wave signals reaching detectors on earth, and the phase directly before the merger (chirp) could be used as a standard candle to deduce the distance to the merger eventsand hence serve as a probe of cosmic expansion at large distances.130 the gravitational waves produced as a stellar black hole plunges into a supermassive one should provide direct information about the supermassive black holes geometry.131 cosmology main article: physical cosmology this blue horseshoe is a distant galaxy that has been magnified and warped into a nearly complete ring by the strong gravitational pull of the massive foreground luminous red galaxy. the existing models of cosmology are based on einsteins field equations, which include the cosmological constant displaystyle lambda since it has important influence on the large-scale dynamics of the cosmos, displaystyle rmu nu -textstyle 1 over 2r,gmu nu lambda gmu nu frac 8pi gc4,tmu nu where displaystyle gmu nu is the spacetime metric.132 isotropic and homogeneous solutions of these enhanced equations, the friedmannlematrerobertsonwalker solutions,133 allow physicists to model a universe"
  },
  {
    "chunk_id": 223,
    "doc_id": "General_Relativity.txt",
    "text": "has important influence on the large-scale dynamics of the cosmos, displaystyle rmu nu -textstyle 1 over 2r,gmu nu lambda gmu nu frac 8pi gc4,tmu nu where displaystyle gmu nu is the spacetime metric.132 isotropic and homogeneous solutions of these enhanced equations, the friedmannlematrerobertsonwalker solutions,133 allow physicists to model a universe that has evolved over the past 14 billion years from a hot, early big bang phase.134 once a small number of parameters (for example the universes mean matter density) have been fixed by astronomical observation,135 further observational data can be used to put the models to the test.136 predictions, all successful, include the initial abundance of chemical elements formed in a period of primordial nucleosynthesis,137 the large-scale structure of the universe,138 and the existence and properties of a thermal echo from the early cosmos, the cosmic background radiation.139 astronomical observations of the cosmological expansion rate allow the total amount of matter in the universe to be estimated, although the nature of that matter remains mysterious in part. about 90 of all matter appears to be dark matter, which has mass (or, equivalently, gravitational influence), but does not interact electromagnetically and, hence, cannot be observed directly.140 there is no generally accepted description of this new kind of matter, within the framework of known particle physics141 or otherwise.142 observational evidence from redshift surveys of distant supernovae and measurements of the cosmic background radiation also show that the evolution of the universe is significantly influenced by a cosmological constant resulting in an acceleration of cosmic expansion or, equivalently, by a form of energy with an unusual equation of state, known as dark energy, the nature of which remains unclear.143 an inflationary phase,144 an additional phase of strongly accelerated expansion at cosmic times of around 1033 seconds, was hypothesized in 1980 to account for several puzzling observations that were unexplained by classical cosmological models, such as the nearly perfect homogeneity of the cosmic background radiation.145 recent measurements of the cosmic background radiation have resulted in the first evidence for this scenario.146 however, there are a bewildering variety of possible inflationary scenarios, which cannot be restricted by existing observations.147 an even larger question is the physics of the earliest universe, prior to the inflationary phase and close to where the classical models predict the big bang singularity. an authoritative answer would require a complete theory of quantum gravity, which has not yet been developed148 (cf."
  },
  {
    "chunk_id": 224,
    "doc_id": "General_Relativity.txt",
    "text": "restricted by existing observations.147 an even larger question is the physics of the earliest universe, prior to the inflationary phase and close to where the classical models predict the big bang singularity. an authoritative answer would require a complete theory of quantum gravity, which has not yet been developed148 (cf. the section on quantum gravity, below). exotic solutions: time travel, warp drives kurt gdel showed149 that solutions to einsteins equations exist that contain closed timelike curves (ctcs), which allow for loops in time. the solutions require extreme physical conditions unlikely ever to occur in practice, and it remains an open question whether further laws of physics will eliminate them completely. since then, othersimilarly impracticalgr solutions containing ctcs have been found, such as the tipler cylinder and traversable wormholes. stephen hawking introduced chronology protection conjecture, which is an assumption beyond those of standard general relativity to prevent time travel. some exact solutions in general relativity such as alcubierre drive offer examples of warp drive but these solutions require exotic matter distribution, and generally suffer from semiclassical instability. 150 advanced concepts asymptotic symmetries main article: bondimetznersachs group the spacetime symmetry group for special relativity is the poincar group, which is a ten-dimensional group of three lorentz boosts, three rotations, and four spacetime translations. it is logical to ask what symmetries, if any, might apply in general relativity. a tractable case might be to consider the symmetries of spacetime as seen by observers located far away from all sources of the gravitational field. the naive expectation for asymptotically flat spacetime symmetries might be simply to extend and reproduce the symmetries of flat spacetime of special relativity, viz., the poincar group. in 1962 hermann bondi, m. g. van der burg, a. w. metzner151 and rainer k. sachs152 addressed this asymptotic symmetry problem in order to investigate the flow of energy at infinity due to propagating gravitational waves. their first step was to decide on some physically sensible boundary conditions to place on the gravitational field at light-like infinity to characterize what it means to say a metric is asymptotically flat, making no a priori assumptions about the nature of the asymptotic symmetry groupnot even the assumption that such a group exists. then after designing what they considered to be the most sensible boundary conditions, they investigated the nature of the resulting asymptotic symmetry transformations that leave invariant the form of the boundary conditions appropriate"
  },
  {
    "chunk_id": 225,
    "doc_id": "General_Relativity.txt",
    "text": "assumptions about the nature of the asymptotic symmetry groupnot even the assumption that such a group exists. then after designing what they considered to be the most sensible boundary conditions, they investigated the nature of the resulting asymptotic symmetry transformations that leave invariant the form of the boundary conditions appropriate for asymptotically flat gravitational fields. what they found was that the asymptotic symmetry transformations actually do form a group and the structure of this group does not depend on the particular gravitational field that happens to be present. this means that, as expected, one can separate the kinematics of spacetime from the dynamics of the gravitational field at least at spatial infinity. the puzzling surprise in 1962 was their discovery of a rich infinite-dimensional group (the so-called bms group) as the asymptotic symmetry group, instead of the finite-dimensional poincar group, which is a subgroup of the bms group. not only are the lorentz transformations asymptotic symmetry transformations, there are also additional transformations that are not lorentz transformations but are asymptotic symmetry transformations. in fact, they found an additional infinity of transformation generators known as supertranslations. this implies the conclusion that general relativity (gr) does not reduce to special relativity in the case of weak fields at long distances. it turns out that the bms symmetry, suitably modified, could be seen as a restatement of the universal soft graviton theorem in quantum field theory (qft), which relates universal infrared (soft) qft with gr asymptotic spacetime symmetries.153 causal structure and global geometry main article: causal structure penrosecarter diagram of an infinite minkowski universe in general relativity, no material body can catch up with or overtake a light pulse. no influence from an event a can reach any other location x before light sent out at a to x. in consequence, an exploration of all light worldlines (null geodesics) yields key information about the spacetimes causal structure. this structure can be displayed using penrosecarter diagrams in which infinitely large regions of space and infinite time intervals are shrunk (compactified) so as to fit onto a finite map, while light still travels along diagonals as in standard spacetime diagrams.154 aware of the importance of causal structure, roger penrose and others developed what is known as global geometry. in global geometry, the object of study is not one particular solution (or family of solutions) to einsteins equations. rather, relations that hold true for all geodesics,"
  },
  {
    "chunk_id": 226,
    "doc_id": "General_Relativity.txt",
    "text": "as in standard spacetime diagrams.154 aware of the importance of causal structure, roger penrose and others developed what is known as global geometry. in global geometry, the object of study is not one particular solution (or family of solutions) to einsteins equations. rather, relations that hold true for all geodesics, such as the raychaudhuri equation, and additional non-specific assumptions about the nature of matter (usually in the form of energy conditions) are used to derive general results.155 event horizons main articles: horizon (general relativity), no hair theorem, and black hole mechanics using global geometry, some spacetimes can be shown to contain boundaries called horizons, which demarcate one region from the rest of spacetime. the best-known examples are black holes: if mass is compressed into a sufficiently compact region of space (as specified in the hoop conjecture, the relevant length scale is the schwarzschild radius, given by the equation displaystyle rtextsfrac 2gmc2,156), no light from inside can escape to the outside. since no object can overtake a light pulse, all interior matter is imprisoned as well. passage from the exterior to the interior is still possible, showing that the boundary, the black holes horizon, is not a physical barrier.157 the ergosphere of a rotating black hole, which plays a key role when it comes to extracting energy from such a black hole early studies of black holes relied on explicit solutions of einsteins equations, notably the spherically symmetric schwarzschild solution (used to describe a static black hole) and the axisymmetric kerr solution (used to describe a rotating, stationary black hole, and introducing interesting features such as the ergosphere). using global geometry, later studies have revealed more general properties of black holes. with time they become rather simple objects characterized by eleven parameters specifying: electric charge, massenergy, linear momentum, angular momentum, and location at a specified time. this is stated by the black hole uniqueness theorem: black holes have no hair, that is, no distinguishing marks like the hairstyles of humans. irrespective of the complexity of a gravitating object collapsing to form a black hole, the object that results (having emitted gravitational waves) is very simple.158 even more remarkably, there is a general set of laws known as black hole mechanics, which is analogous to the laws of thermodynamics. for instance, by the second law of black hole mechanics, the area of the event horizon of a general black hole will never"
  },
  {
    "chunk_id": 227,
    "doc_id": "General_Relativity.txt",
    "text": "waves) is very simple.158 even more remarkably, there is a general set of laws known as black hole mechanics, which is analogous to the laws of thermodynamics. for instance, by the second law of black hole mechanics, the area of the event horizon of a general black hole will never decrease with time, analogous to the entropy of a thermodynamic system. this limits the energy that can be extracted by classical means from a rotating black hole (e.g. by the penrose process).159 there is strong evidence that the laws of black hole mechanics are, in fact, a subset of the laws of thermodynamics, and that the black hole area is proportional to its entropy.160 this leads to a modification of the original laws of black hole mechanics: for instance, as the second law of black hole mechanics becomes part of the second law of thermodynamics, it is possible for the black hole area to decrease as long as other processes ensure that entropy increases overall. as thermodynamical objects with nonzero temperature, black holes should emit thermal radiation. semiclassical calculations indicate that indeed they do, with the surface gravity playing the role of temperature in plancks law. this radiation is known as hawking radiation (cf. the quantum theory section, below).161 there are many other types of horizons. in an expanding universe, an observer may find that some regions of the past cannot be observed (particle horizon), and some regions of the future cannot be influenced (event horizon).162 even in flat minkowski space, when described by an accelerated observer (rindler space), there will be horizons associated with a semiclassical radiation known as unruh radiation.163 singularities main article: spacetime singularity another general feature of general relativity is the appearance of spacetime boundaries known as singularities. spacetime can be explored by following up on timelike and lightlike geodesicsall possible ways that light and particles in free fall can travel. but some solutions of einsteins equations have ragged edgesregions known as spacetime singularities, where the paths of light and falling particles come to an abrupt end, and geometry becomes ill-defined. in the more interesting cases, these are curvature singularities, where geometrical quantities characterizing spacetime curvature, such as the ricci scalar, take on infinite values.164 well-known examples of spacetimes with future singularitieswhere worldlines endare the schwarzschild solution, which describes a singularity inside an eternal static black hole,165 or the kerr solution with its ring-shaped singularity inside"
  },
  {
    "chunk_id": 228,
    "doc_id": "General_Relativity.txt",
    "text": "these are curvature singularities, where geometrical quantities characterizing spacetime curvature, such as the ricci scalar, take on infinite values.164 well-known examples of spacetimes with future singularitieswhere worldlines endare the schwarzschild solution, which describes a singularity inside an eternal static black hole,165 or the kerr solution with its ring-shaped singularity inside an eternal rotating black hole.166 the friedmannlematrerobertsonwalker solutions and other spacetimes describing universes have past singularities on which worldlines begin, namely big bang singularities, and some have future singularities (big crunch) as well.167 given that these examples are all highly symmetricand thus simplifiedit is tempting to conclude that the occurrence of singularities is an artifact of idealization.168 the famous singularity theorems, proved using the methods of global geometry, say otherwise: singularities are a generic feature of general relativity, and unavoidable once the collapse of an object with realistic matter properties has proceeded beyond a certain stage169 and also at the beginning of a wide class of expanding universes.170 however, the theorems say little about the properties of singularities, and much of current research is devoted to characterizing these entities generic structure (hypothesized e.g. by the bkl conjecture).171 the cosmic censorship hypothesis states that all realistic future singularities (no perfect symmetries, matter with realistic properties) are safely hidden away behind a horizon, and thus invisible to all distant observers. while no formal proof yet exists, numerical simulations offer supporting evidence of its validity.172 evolution equations main article: initial value formulation (general relativity) each solution of einsteins equation encompasses the whole history of a universeit is not just some snapshot of how things are, but a whole, possibly matter-filled, spacetime. it describes the state of matter and geometry everywhere and at every moment in that particular universe. due to its general covariance, einsteins theory is not sufficient by itself to determine the time evolution of the metric tensor. it must be combined with a coordinate condition, which is analogous to gauge fixing in other field theories.173 to understand einsteins equations as partial differential equations, it is helpful to formulate them in a way that describes the evolution of the universe over time. this is done in 31 formulations, where spacetime is split into three space dimensions and one time dimension. the best-known example is the adm formalism.174 these decompositions show that the spacetime evolution equations of general relativity are well-behaved: solutions always exist, and are uniquely defined, once suitable initial conditions have"
  },
  {
    "chunk_id": 229,
    "doc_id": "General_Relativity.txt",
    "text": "this is done in 31 formulations, where spacetime is split into three space dimensions and one time dimension. the best-known example is the adm formalism.174 these decompositions show that the spacetime evolution equations of general relativity are well-behaved: solutions always exist, and are uniquely defined, once suitable initial conditions have been specified.175 such formulations of einsteins field equations are the basis of numerical relativity.176 global and quasi-local quantities main article: mass in general relativity the notion of evolution equations is intimately tied in with another aspect of general relativistic physics. in einsteins theory, it turns out to be impossible to find a general definition for a seemingly simple property such as a systems total mass (or energy). the main reason is that the gravitational fieldlike any physical fieldmust be ascribed a certain energy, but that it proves to be fundamentally impossible to localize that energy.177 nevertheless, there are possibilities to define a systems total mass, either using a hypothetical infinitely distant observer (adm mass)178 or suitable symmetries (komar mass).179 if one excludes from the systems total mass the energy being carried away to infinity by gravitational waves, the result is the bondi mass at null infinity.180 just as in classical physics, it can be shown that these masses are positive.181 corresponding global definitions exist for momentum and angular momentum.182 there have also been a number of attempts to define quasi-local quantities, such as the mass of an isolated system formulated using only quantities defined within a finite region of space containing that system. the hope is to obtain a quantity useful for general statements about isolated systems, such as a more precise formulation of the hoop conjecture.183 relationship with quantum theory if general relativity were considered to be one of the two pillars of modern physics, then quantum theory, the basis of understanding matter from elementary particles to solid-state physics, would be the other.184 however, how to reconcile quantum theory with general relativity is still an open question. quantum field theory in curved spacetime main article: quantum field theory in curved spacetime ordinary quantum field theories, which form the basis of modern elementary particle physics, are defined in flat minkowski space, which is an excellent approximation when it comes to describing the behavior of microscopic particles in weak gravitational fields like those found on earth.185 in order to describe situations in which gravity is strong enough to influence (quantum) matter,"
  },
  {
    "chunk_id": 230,
    "doc_id": "General_Relativity.txt",
    "text": "of modern elementary particle physics, are defined in flat minkowski space, which is an excellent approximation when it comes to describing the behavior of microscopic particles in weak gravitational fields like those found on earth.185 in order to describe situations in which gravity is strong enough to influence (quantum) matter, yet not strong enough to require quantization itself, physicists have formulated quantum field theories in curved spacetime. these theories rely on general relativity to describe a curved background spacetime, and define a generalized quantum field theory to describe the behavior of quantum matter within that spacetime.186 using this formalism, it can be shown that black holes emit a blackbody spectrum of particles known as hawking radiation leading to the possibility that they evaporate over time.187 as briefly mentioned above, this radiation plays an important role for the thermodynamics of black holes.188 quantum gravity main article: quantum gravity see also: string theory, canonical general relativity, loop quantum gravity, causal dynamical triangulation, and causal sets projection of a calabiyau manifold, one of the ways of compactifying the extra dimensions posited by string theory the demand for consistency between a quantum description of matter and a geometric description of spacetime,189 as well as the appearance of singularities (where curvature length scales become microscopic), indicate the need for a full theory of quantum gravity: for an adequate description of the interior of black holes, and of the very early universe, a theory is required in which gravity and the associated geometry of spacetime are described in the language of quantum physics.190 despite major efforts, no complete and consistent theory of quantum gravity is currently known, even though a number of candidates exist.191192 attempts to generalize ordinary quantum field theories, used in elementary particle physics to describe fundamental interactions, so as to include gravity have led to serious problems.193 some have argued that at low energies, this approach proves successful, in that it results in an acceptable effective (quantum) field theory of gravity.194 at very high energies, however, the perturbative results are badly divergent and lead to models devoid of predictive power (perturbative non-renormalizability).195 simple spin network of the type used in loop quantum gravity one attempt to overcome these limitations is string theory, a quantum theory not of point particles, but of minute one-dimensional extended objects.196 the theory promises to be a unified description of all particles and interactions, including gravity;197 the price to"
  },
  {
    "chunk_id": 231,
    "doc_id": "General_Relativity.txt",
    "text": "spin network of the type used in loop quantum gravity one attempt to overcome these limitations is string theory, a quantum theory not of point particles, but of minute one-dimensional extended objects.196 the theory promises to be a unified description of all particles and interactions, including gravity;197 the price to pay is unusual features such as six extra dimensions of space in addition to the usual three.198 in what is called the second superstring revolution, it was conjectured that both string theory and a unification of general relativity and supersymmetry known as supergravity199 form part of a hypothesized eleven-dimensional model known as m-theory, which would constitute a uniquely defined and consistent theory of quantum gravity.200 another approach starts with the canonical quantization procedures of quantum theory. using the initial-value-formulation of general relativity (cf. evolution equations above), the result is the wheelerdewitt equation (an analogue of the schrdinger equation) which turns out to be ill-defined without a proper ultraviolet (lattice) cutoff.201 however, with the introduction of what are now known as ashtekar variables,202 this leads to a model known as loop quantum gravity. space is represented by a web-like structure called a spin network, evolving over time in discrete steps.203 depending on which features of general relativity and quantum theory are accepted unchanged, and on what level changes are introduced,204 there are numerous other attempts to arrive at a viable theory of quantum gravity, some examples being the lattice theory of gravity based on the feynman path integral approach and regge calculus,191 dynamical triangulations,205 causal sets,206 twistor models207 or the path integral based models of quantum cosmology.208 observation of gravitational waves from binary black hole merger gw150914 all candidate theories still have major formal and conceptual problems to overcome. they also face the common problem that, as yet, there is no way to put quantum gravity predictions to experimental tests (and thus to decide between the candidates where their predictions vary), although there is hope for this to change as future data from cosmological observations and particle physics experiments becomes available.209 current status general relativity has emerged as a highly successful model of gravitation and cosmology, which has so far unambiguously fitted observational and experimental data. however, there are strong theoretical reasons to consider the theory to be incomplete.210 the problem of quantum gravity and the question of the reality of spacetime singularities remain open.211 observational data that is taken as"
  },
  {
    "chunk_id": 232,
    "doc_id": "General_Relativity.txt",
    "text": "model of gravitation and cosmology, which has so far unambiguously fitted observational and experimental data. however, there are strong theoretical reasons to consider the theory to be incomplete.210 the problem of quantum gravity and the question of the reality of spacetime singularities remain open.211 observational data that is taken as evidence for dark energy and dark matter could also indicate the need to consider alternatives or modifications of general relativity. even taken as is, general relativity provides many possibilities for further exploration. mathematical relativists seek to understand the nature of singularities and the fundamental properties of einsteins equations,212 while numerical relativists run increasingly powerful computer simulations, such as those describing merging black holes.213 in february 2016, it was announced that gravitational waves were directly detected by the advanced ligo team on 14 september 2015.83214215 a century after its introduction, general relativity remains a highly active area of research.216"
  },
  {
    "chunk_id": 233,
    "doc_id": "The_Big_Bang.txt",
    "text": "the big bang is a physical theory that describes how the universe expanded from an initial state of high density and temperature. various cosmological models based on the big bang concept explain a broad range of phenomena,123 including the abundance of light elements, the cosmic microwave background (cmb) radiation, and large-scale structure. the uniformity of the universe, known as the horizon and flatness problems, is explained through cosmic inflation: a phase of accelerated expansion during the earliest stages. detailed measurements of the expansion rate of the universe place the initial singularity at an estimated 13.7870.02 billion years ago, which is considered the age of the universe. a wide range of empirical evidence strongly favors the big bang event, which is now widely accepted.4 extrapolating this cosmic expansion backward in time using the known laws of physics, the models describe an extraordinarily hot and dense primordial universe. physics lacks a widely accepted theory that can model the earliest conditions of the big bang.5 as the universe expanded, it cooled sufficiently to allow the formation of subatomic particles, and later atoms. these primordial elementsmostly hydrogen, with some helium and lithiumthen coalesced under the force of gravity aided by dark matter, forming early stars and galaxies. measurements of the redshifts of supernovae indicate that the expansion of the universe is accelerating, an observation attributed to a concept called dark energy. the concept of an expanding universe was introduced by the physicist alexander friedmann in 1922 with the mathematical derivation of the friedmann equations.6789 the earliest empirical observation of an expanding universe is known as hubbles law, published in work by physicist edwin hubble in 1929, which discerned that galaxies are moving away from earth at a rate that accelerates proportionally with distance. independent of friedmanns work, and independent of hubbles observations, in 1931 physicist georges lematre proposed that the universe emerged from a primeval atom, introducing the modern notion of the big bang. in 1964, the cmb was discovered. over the next few years measurements showed this radiation to be uniform over directions in the sky and the shape of the energy versus intensity curve, both consistent with the big bang models of high temperatures and densities in the distant past. by the late 1960s most cosmologists were convinced that competing steady-state model of cosmic evolution was incorrect.10 there remain aspects of the observed universe that are not yet adequately explained by the"
  },
  {
    "chunk_id": 234,
    "doc_id": "The_Big_Bang.txt",
    "text": "intensity curve, both consistent with the big bang models of high temperatures and densities in the distant past. by the late 1960s most cosmologists were convinced that competing steady-state model of cosmic evolution was incorrect.10 there remain aspects of the observed universe that are not yet adequately explained by the big bang models. these include the unequal abundances of matter and antimatter known as baryon asymmetry, the detailed nature of dark matter surrounding galaxies, and the origin of dark energy.11 features of the models assumptions big bang cosmology models depend on three major assumptions: the universality of physical laws, the cosmological principle, and that the matter content can be modeled as a perfect fluid.12 the universality of physical laws is one of the underlying principles of the theory of relativity. the cosmological principle states that on large scales the universe is homogeneous and isotropicappearing the same in all directions regardless of location.13 a perfect fluid has no viscosity; the pressure of a perfect fluid is proportional to its density.14: 49 these ideas were initially taken as postulates, but later efforts were made to test each of them. for example, the first assumption has been tested by observations showing that the largest possible deviation of the fine-structure constant over much of the age of the universe is of order 105.15 the key physical law behind these models, general relativity has passed stringent tests on the scale of the solar system and binary stars.1617 the cosmological principle has been confirmed to a level of 105 via observations of the temperature of the cmb. at the scale of the cmb horizon, the universe has been measured to be homogeneous with an upper bound on the order of 10 inhomogeneity, as of 1995.18 expansion prediction main article: expansion of the universe the cosmological principle dramatically simplifies the equations of general relativity, giving the friedmannlematrerobertsonwalker metric to describe the geometry of the universe and, with the assumption of a perfect fluid, the friedmann equations giving the time dependence of that geometry. the only parameter at this level of description is the mass-energy density: the geometry of the universe and its expansion is a direct consequence of its density.19: 73 all of the major features of big bang cosmology are related to these results.14: 49 mass-energy density estimated relative distribution for components of the energy density of the universe. (in february 2015, the european-led research team"
  },
  {
    "chunk_id": 235,
    "doc_id": "The_Big_Bang.txt",
    "text": "of the universe and its expansion is a direct consequence of its density.19: 73 all of the major features of big bang cosmology are related to these results.14: 49 mass-energy density estimated relative distribution for components of the energy density of the universe. (in february 2015, the european-led research team behind the planck cosmology probe released new data refining these values to 4.9 ordinary matter, 25.9 dark matter and 69.1 dark energy.) in big bang cosmology, the massenergy density controls the shape and evolution of the universe. by combining astronomical observations with known laws of thermodynamics and particle physics, cosmologists have worked out the components of the density over the lifespan of the universe. in the current universe, luminous matter, the stars, planets, and so on makes up less than 5 of the density. dark matter accounts for 27 and dark energy the remaining 68.20 horizons main article: cosmological horizon an important feature of the big bang spacetime is the presence of particle horizons. since the universe has a finite age, and light travels at a finite speed, there may be events in the past whose light has not yet had time to reach earth. this places a limit or a past horizon on the most distant objects that can be observed. conversely, because space is expanding, and more distant objects are receding ever more quickly, light emitted by us today may never catch up to very distant objects. this defines a future horizon, which limits the events in the future that we will be able to influence. the presence of either type of horizon depends on the details of the friedmannlematrerobertsonwalker (flrw) metric that describes the expansion of the universe.21 our understanding of the universe back to very early times suggests that there is a past horizon, though in practice our view is also limited by the opacity of the universe at early times. so our view cannot extend further backward in time, though the horizon recedes in space. if the expansion of the universe continues to accelerate, there is a future horizon as well.21 thermalization some processes in the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximate thermodynamic equilibrium. others were fast enough to reach thermalization. the parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the"
  },
  {
    "chunk_id": 236,
    "doc_id": "The_Big_Bang.txt",
    "text": "the early universe occurred too slowly, compared to the expansion rate of the universe, to reach approximate thermodynamic equilibrium. others were fast enough to reach thermalization. the parameter usually used to find out whether a process in the very early universe has reached thermal equilibrium is the ratio between the rate of the process (usually rate of collisions between particles) and the hubble parameter. the larger the ratio, the more time particles had to thermalize before they were too far away from each other.22 timeline main article: chronology of the universe according to the big bang models, the universe at the beginning was very hot and very compact, and since then it has been expanding and cooling. singularity see also: gravitational singularity, initial singularity, and planck units cosmology existing theories of physics cannot tell us about the moment of the big bang.5 extrapolation of the expansion of the universe backwards in time using only classical general relativity yields a gravitational singularity with infinite density and temperature at a finite time in the past.23 however this classical gravitational theory is expected to be inadequate to describe physics under these conditions.19: 275 thus the meaning of this singularity in the context of the big bang is unclear.24 the earliest time that general relativity can be applied is called the planck time.19: 274 earlier, during the planck epoch, when the temperature of the universe was close to the planck scale (around 1032 k or 1028 ev) quantum gravity effects are expected to be dominant. to date there is no accepted theory of quantum gravity; above the planck energy scale, undiscovered physics could influence the expansion history of the universe. inflation and baryogenesis main articles: inflation (cosmology) and baryogenesis the earliest phases of the big bang are subject to much speculation, given the lack of available data. in the most common models the universe was filled homogeneously and isotropically with a very high energy density and huge temperatures and pressures, and was very rapidly expanding and cooling. the period up to 1043 seconds into the expansion, the planck epoch, was a phase in which the four fundamental forcesthe electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational forcewere unified as one.25 in this stage, the characteristic scale length of the universe was the planck length, 1.61035 m, and consequently had a temperature of approximately 1032 degrees celsius. even the very"
  },
  {
    "chunk_id": 237,
    "doc_id": "The_Big_Bang.txt",
    "text": "the four fundamental forcesthe electromagnetic force, the strong nuclear force, the weak nuclear force, and the gravitational forcewere unified as one.25 in this stage, the characteristic scale length of the universe was the planck length, 1.61035 m, and consequently had a temperature of approximately 1032 degrees celsius. even the very concept of a particle breaks down in these conditions. a proper understanding of this period awaits the development of a theory of quantum gravity.2627 the planck epoch was succeeded by the grand unification epoch beginning at 1043 seconds, where gravitation separated from the other forces as the universes temperature fell.25 at approximately 1037 seconds into the expansion, a phase transition caused a cosmic inflation, during which the universe grew exponentially, unconstrained by the light speed invariance, and temperatures dropped by a factor of 100,000. this concept is motivated by the flatness problem, where the density of matter and energy is very close to the critical density needed to produce a flat universe. that is, the shape of the universe has no overall geometric curvature due to gravitational influence. microscopic quantum fluctuations that occurred because of heisenbergs uncertainty principle were frozen in by inflation, becoming amplified into the seeds that would later form the large-scale structure of the universe.28 at a time around 1036 seconds, the electroweak epoch begins when the strong nuclear force separates from the other forces, with only the electromagnetic force and weak nuclear force remaining unified.29 all of the mass-energy in all of the galaxies currently visible started in a sphere with a radius around 4 x 10-29 m then grew to a sphere with a radius around 0.9 m by the end of inflation.30: 202 reheating followed as the inflaton field decayed, until the universe obtained the temperatures required for the production of a quarkgluon plasma as well as all other elementary particles.3132 temperatures were so high that the random motions of particles were at relativistic speeds, and particleantiparticle pairs of all kinds were being continuously created and destroyed in collisions.33 at some point, an unknown reaction called baryogenesis violated the conservation of baryon number, leading to a very small excess of quarks and leptons over antiquarks and antileptonsof the order of one part in 30 million. this resulted in the predominance of matter over antimatter in the present universe.34 cooling main articles: big bang nucleosynthesis and cosmic microwave background a map of the universe, with"
  },
  {
    "chunk_id": 238,
    "doc_id": "The_Big_Bang.txt",
    "text": "to a very small excess of quarks and leptons over antiquarks and antileptonsof the order of one part in 30 million. this resulted in the predominance of matter over antimatter in the present universe.34 cooling main articles: big bang nucleosynthesis and cosmic microwave background a map of the universe, with specks and strands of light of different colors. panoramic view of the entire near-infrared sky reveals the distribution of galaxies beyond the milky way. galaxies are color-coded by redshift. the universe continued to decrease in density and fall in temperature, hence the typical energy of each particle was decreasing. symmetry-breaking phase transitions put the fundamental forces of physics and the parameters of elementary particles into their present form, with the electromagnetic force and weak nuclear force separating at about 1012 seconds.2935 after about 1011 seconds, the picture becomes less speculative, since particle energies drop to values that can be attained in particle accelerators. at about 106 seconds, quarks and gluons combined to form baryons such as protons and neutrons. the small excess of quarks over antiquarks led to a small excess of baryons over antibaryons. the temperature was no longer high enough to create either new protonantiproton or neutronantineutron pairs. a mass annihilation immediately followed, leaving just one in 108 of the original matter particles and none of their antiparticles.36 a similar process happened at about 1 second for electrons and positrons. after these annihilations, the remaining protons, neutrons and electrons were no longer moving relativistically and the energy density of the universe was dominated by photons (with a minor contribution from neutrinos). a few minutes into the expansion, when the temperature was about a billion kelvin and the density of matter in the universe was comparable to the current density of earths atmosphere, neutrons combined with protons to form the universes deuterium and helium nuclei in a process called big bang nucleosynthesis (bbn).37 most protons remained uncombined as hydrogen nuclei.38 as the universe cooled, the rest energy density of matter came to gravitationally dominate over that of the photon and neutrino radiation at a time of about 50,000 years. at a time of about 380,000 years, the universe cooled enough that electrons and nuclei combined into neutral atoms (mostly hydrogen) in an event called recombination. this process made the previously opaque universe transparent, and the photons that last scattered during this epoch comprise the cosmic microwave background.38 structure formation"
  },
  {
    "chunk_id": 239,
    "doc_id": "The_Big_Bang.txt",
    "text": "at a time of about 380,000 years, the universe cooled enough that electrons and nuclei combined into neutral atoms (mostly hydrogen) in an event called recombination. this process made the previously opaque universe transparent, and the photons that last scattered during this epoch comprise the cosmic microwave background.38 structure formation main article: structure formation abell 2744 galaxy cluster hubble frontier fields view39 after the recombination epoch, the slightly denser regions of the uniformly distributed matter gravitationally attracted nearby matter and thus grew even denser, forming gas clouds, stars, galaxies, and the other astronomical structures observable today.33 the details of this process depend on the amount and type of matter in the universe. the four possible types of matter are known as cold dark matter (cdm), warm dark matter, hot dark matter, and baryonic matter. the best measurements available, from the wilkinson microwave anisotropy probe (wmap), show that the data is well-fit by a lambda-cdm model in which dark matter is assumed to be cold. this cdm is estimated to make up about 23 of the matterenergy of the universe, while baryonic matter makes up about 4.6.40 cosmic acceleration main article: accelerating expansion of the universe independent lines of evidence from type ia supernovae and the cmb imply that the universe today is dominated by a mysterious form of energy known as dark energy, which appears to homogeneously permeate all of space. observations suggest that 73 of the total energy density of the present day universe is in this form. when the universe was very young it was likely infused with dark energy, but with everything closer together, gravity predominated, braking the expansion. eventually, after billions of years of expansion, the declining density of matter relative to the density of dark energy allowed the expansion of the universe to begin to accelerate.11 dark energy in its simplest formulation is modeled by a cosmological constant term in einstein field equations of general relativity, but its composition and mechanism are unknown. more generally, the details of its equation of state and relationship with the standard model of particle physics continue to be investigated both through observation and theory.11 all of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the lambda-cdm model of cosmology, which uses the independent frameworks of quantum mechanics and general relativity. there are no easily testable models that would describe the situation prior to"
  },
  {
    "chunk_id": 240,
    "doc_id": "The_Big_Bang.txt",
    "text": "both through observation and theory.11 all of this cosmic evolution after the inflationary epoch can be rigorously described and modeled by the lambda-cdm model of cosmology, which uses the independent frameworks of quantum mechanics and general relativity. there are no easily testable models that would describe the situation prior to approximately 1015 seconds.41 understanding this earliest of eras in the history of the universe is one of the greatest unsolved problems in physics. concept history main article: history of the big bang theory see also: timeline of cosmological theories etymology english astronomer fred hoyle is credited with coining the term big bang during a talk for a march 1949 bbc radio broadcast,42 saying: these theories were based on the hypothesis that all the matter in the universe was created in one big bang at a particular time in the remote past.4344 however, it did not catch on until the 1970s.44 it is popularly reported that hoyle, who favored an alternative steady-state cosmological model, intended this to be pejorative,454647 but hoyle explicitly denied this and said it was just a striking image meant to highlight the difference between the two models.484951 helge kragh writes that the evidence for the claim that it was meant as a pejorative is unconvincing, and mentions a number of indications that it was not a pejorative.44 a primordial singularity is sometimes called the big bang,52 but the term can also refer to a more generic early hot, dense phase.53 the term itself has been argued to be a misnomer because it evokes an explosion.4454 the argument is that whereas an explosion suggests expansion into a surrounding space, the big bang only describes the intrinsic expansion of the contents of the universe.5556 another issue pointed out by santhosh mathew is that bang implies sound, which is not an important feature of the model.46 however, an attempt to find a more suitable alternative was not successful.44 according to timothy ferris:4757 the term big bang was coined with derisive intent by fred hoyle, and its endurance testifies to sir freds creativity and wit. indeed, the term survived an international competition in which three judges the television science reporter hugh downs, the astronomer carl sagan, and myself sifted through 13,099 entries from 41 countries and concluded that none was apt enough to replace it. no winner was declared, and like it or not, we are stuck with big bang. before"
  },
  {
    "chunk_id": 241,
    "doc_id": "The_Big_Bang.txt",
    "text": "competition in which three judges the television science reporter hugh downs, the astronomer carl sagan, and myself sifted through 13,099 entries from 41 countries and concluded that none was apt enough to replace it. no winner was declared, and like it or not, we are stuck with big bang. before the name hubble extreme deep field (xdf) xdf size compared to the size of the moon (xdf is the small box to the left of, and nearly below, the moon) several thousand galaxies, each consisting of billions of stars, are in this small view. xdf (2012) view each light speck is a galaxy some of these are as old as 13.2 billion years58 the universe is estimated to contain 200 billion galaxies. xdf image shows fully mature galaxies in the foreground plane nearly mature galaxies from 5 to 9 billion years ago protogalaxies, blazing with young stars, beyond 9 billion years. early cosmological models developed from observations of the structure of the universe and from theoretical considerations. in 1912, vesto slipher measured the first doppler shift of a spiral nebula (spiral nebula is the obsolete term for spiral galaxies), and soon discovered that almost all such nebulae were receding from earth. he did not grasp the cosmological implications of this fact, and indeed at the time it was highly controversial whether or not these nebulae were island universes outside our milky way.5960 ten years later, alexander friedmann, a russian cosmologist and mathematician, derived the friedmann equations from the einstein field equations, showing that the universe might be expanding in contrast to the static universe model advocated by albert einstein at that time.6162 in 1924, american astronomer edwin hubbles measurement of the great distance to the nearest spiral nebulae showed that these systems were indeed other galaxies. starting that same year, hubble painstakingly developed a series of distance indicators, the forerunner of the cosmic distance ladder, using the 100-inch (2.5 m) hooker telescope at mount wilson observatory. this allowed him to estimate distances to galaxies whose redshifts had already been measured, mostly by slipher. in 1929, hubble discovered a correlation between distance and recessional velocitynow known as hubbles law.6364 independently deriving friedmanns equations in 1927, georges lematre, a belgian physicist and roman catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe.6566 he inferred the relation that hubble would later observe, given the cosmological"
  },
  {
    "chunk_id": 242,
    "doc_id": "The_Big_Bang.txt",
    "text": "distance and recessional velocitynow known as hubbles law.6364 independently deriving friedmanns equations in 1927, georges lematre, a belgian physicist and roman catholic priest, proposed that the recession of the nebulae was due to the expansion of the universe.6566 he inferred the relation that hubble would later observe, given the cosmological principle.11 in 1931, lematre went further and suggested that the evident expansion of the universe, if projected back in time, meant that the further in the past the smaller the universe was, until at some finite time in the past all the mass of the universe was concentrated into a single point, a primeval atom where and when the fabric of time and space came into existence.67 in the 1920s and 1930s, almost every major cosmologist preferred an eternal steady-state universe, and several complained that the beginning of time implied by an expanding universe imported religious concepts into physics; this objection was later repeated by supporters of the steady-state theory.68 this perception was enhanced by the fact that the originator of the expanding universe concept, lematre, was a roman catholic priest.69 arthur eddington agreed with aristotle that the universe did not have a beginning in time, viz., that matter is eternal. a beginning in time was repugnant to him.7071 lematre, however, disagreed: if the world has begun with a single quantum, the notions of space and time would altogether fail to have any meaning at the beginning; they would only begin to have a sensible meaning when the original quantum had been divided into a sufficient number of quanta. if this suggestion is correct, the beginning of the world happened a little before the beginning of space and time.72 during the 1930s, other ideas were proposed as non-standard cosmologies to explain hubbles observations, including the milne model,73 the oscillatory universe (originally suggested by friedmann, but advocated by albert einstein and richard c. tolman)74 and fritz zwickys tired light hypothesis.75 after world war ii, two distinct possibilities emerged. one was fred hoyles steady-state model, whereby new matter would be created as the universe seemed to expand. in this model the universe is roughly the same at any point in time.76 the other was lematres expanding universe theory, advocated and developed by george gamow, who used it to develop a theory for the abundance of chemical elements in the universe.77 and whose associates, ralph alpher and robert herman, predicted the cosmic background"
  },
  {
    "chunk_id": 243,
    "doc_id": "The_Big_Bang.txt",
    "text": "is roughly the same at any point in time.76 the other was lematres expanding universe theory, advocated and developed by george gamow, who used it to develop a theory for the abundance of chemical elements in the universe.77 and whose associates, ralph alpher and robert herman, predicted the cosmic background radiation.78 as a named model ironically, it was hoyle who coined the phrase that came to be applied to lematres theory, referring to it as this big bang idea during a bbc radio broadcast in march 1949.4944notes 1 for a while, support was split between these two theories. eventually, the observational evidence, most notably from radio source counts, began to favor big bang over steady state. the discovery and confirmation of the cmb in 1964 secured the big bang as the best theory of the origin and evolution of the universe.79 in 1968 and 1970, roger penrose, stephen hawking, and george f. r. ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the big bang.8081 then, from the 1970s to the 1990s, cosmologists worked on characterizing the features of the big bang universe and resolving outstanding problems. in 1981, alan guth made a breakthrough in theoretical work on resolving certain outstanding theoretical problems in the big bang models with the introduction of an epoch of rapid expansion in the early universe he called inflation.82 meanwhile, during these decades, two questions in observational cosmology that generated much discussion and disagreement were over the precise values of the hubble constant83 and the matter-density of the universe (before the discovery of dark energy, thought to be the key predictor for the eventual fate of the universe).84 significant progress in big bang cosmology has been made since the late 1990s as a result of advances in telescope technology as well as the analysis of data from satellites such as the cosmic background explorer (cobe),85 the hubble space telescope and wmap.86 cosmologists now have fairly precise and accurate measurements of many of the parameters of the big bang model, and have made the unexpected discovery that the expansion of the universe appears to be accelerating.8788 observational evidence the big bang picture is too firmly grounded in data from every area to be proved invalid in its general features. lawrence krauss89 the big bang models offer a comprehensive explanation for a broad range of observed phenomena, including"
  },
  {
    "chunk_id": 244,
    "doc_id": "The_Big_Bang.txt",
    "text": "the expansion of the universe appears to be accelerating.8788 observational evidence the big bang picture is too firmly grounded in data from every area to be proved invalid in its general features. lawrence krauss89 the big bang models offer a comprehensive explanation for a broad range of observed phenomena, including the abundances of the light elements, the cosmic microwave background, large-scale structure, and hubbles law.90 the earliest and most direct observational evidence of the validity of the theory are the expansion of the universe according to hubbles law (as indicated by the redshifts of galaxies), discovery and measurement of the cosmic microwave background and the relative abundances of light elements produced by big bang nucleosynthesis (bbn). more recent evidence includes observations of galaxy formation and evolution, and the distribution of large-scale cosmic structures.91 these are sometimes called the four pillars of the big bang models.9293 precise modern models of the big bang appeal to various exotic physical phenomena that have not been observed in terrestrial laboratory experiments or incorporated into the standard model of particle physics. of these features, dark matter is currently the subject of most active laboratory investigations.94 remaining issues include the cuspy halo problem95 and the dwarf galaxy problem96 of cold dark matter. dark energy is also an area of intense interest for scientists, but it is not clear whether direct detection of dark energy will be possible.97 inflation and baryogenesis remain more speculative features of current big bang models. viable, quantitative explanations for such phenomena are still being sought. these are unsolved problems in physics. hubbles law and the expansion of the universe main articles: hubbles law and expansion of the universe see also: distance measures (cosmology) and scale factor (cosmology) redshift of absorption lines due to recessional velocity observations of distant galaxies and quasars show that these objects are redshifted: the light emitted from them has been shifted to longer wavelengths. this can be seen by taking a frequency spectrum of an object and matching the spectroscopic pattern of emission or absorption lines corresponding to atoms of the chemical elements interacting with the light. these redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. if the redshift is interpreted as a doppler shift, the recessional velocity of the object can be calculated. for some galaxies, it is possible to estimate distances via the cosmic distance ladder. when the recessional velocities are"
  },
  {
    "chunk_id": 245,
    "doc_id": "The_Big_Bang.txt",
    "text": "redshifts are uniformly isotropic, distributed evenly among the observed objects in all directions. if the redshift is interpreted as a doppler shift, the recessional velocity of the object can be calculated. for some galaxies, it is possible to estimate distances via the cosmic distance ladder. when the recessional velocities are plotted against these distances, a linear relationship known as hubbles law is observed:63 displaystyle vh0d where displaystyle v is the recessional velocity of the galaxy or other distant object, displaystyle d is the proper distance to the object, and displaystyle h0 is hubbles constant, measured to be 70.41.3 1.4 kmsmpc by the wmap.40 hubbles law implies that the universe is uniformly expanding everywhere. this cosmic expansion was predicted from general relativity by friedmann in 192261 and lematre in 1927,65 well before hubble made his 1929 analysis and observations, and it remains the cornerstone of the big bang model as developed by friedmann, lematre, robertson, and walker. the theory requires the relation displaystyle vhd to hold at all times, where displaystyle d is the proper distance, displaystyle v is the recessional velocity, and displaystyle v, displaystyle h, and displaystyle d vary as the universe expands (hence we write displaystyle h0 to denote the present-day hubble constant). for distances much smaller than the size of the observable universe, the hubble redshift can be thought of as the doppler shift corresponding to the recession velocity displaystyle v. for distances comparable to the size of the observable universe, the attribution of the cosmological redshift becomes more ambiguous, although its interpretation as a kinematic doppler shift remains the most natural one.98 an unexplained discrepancy with the determination of the hubble constant is known as hubble tension. techniques based on observation of the cmb suggest a lower value of this constant compared to the quantity derived from measurements based on the cosmic distance ladder.99 cosmic microwave background radiation main article: cosmic microwave background the cosmic microwave background spectrum measured by the firas instrument on the cobe satellite is the most-precisely measured blackbody spectrum in nature.100 the data points and error bars on this graph are obscured by the theoretical curve. in 1964, arno penzias and robert wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band.79 their discovery provided substantial confirmation of the big-bang predictions by alpher, herman and gamow around 1950. through the 1970s, the radiation was found to be approximately"
  },
  {
    "chunk_id": 246,
    "doc_id": "The_Big_Bang.txt",
    "text": "by the theoretical curve. in 1964, arno penzias and robert wilson serendipitously discovered the cosmic background radiation, an omnidirectional signal in the microwave band.79 their discovery provided substantial confirmation of the big-bang predictions by alpher, herman and gamow around 1950. through the 1970s, the radiation was found to be approximately consistent with a blackbody spectrum in all directions; this spectrum has been redshifted by the expansion of the universe, and today corresponds to approximately 2.725 k. this tipped the balance of evidence in favor of the big bang model, and penzias and wilson were awarded the 1978 nobel prize in physics. the surface of last scattering corresponding to emission of the cmb occurs shortly after recombination, the epoch when neutral hydrogen becomes stable. prior to this, the universe comprised a hot dense photon-baryon plasma sea where photons were quickly scattered from free charged particles. peaking at around 37214 kyr,101 the mean free path for a photon becomes long enough to reach the present day and the universe becomes transparent. 9 year wmap image of the cosmic microwave background radiation (2012).102103 the radiation is isotropic to roughly one part in 100,000.104 in 1989, nasa launched cobe, which made two major advances: in 1990, high-precision spectrum measurements showed that the cmb frequency spectrum is an almost perfect blackbody with no deviations at a level of 1 part in 104, and measured a residual temperature of 2.726 k (more recent measurements have revised this figure down slightly to 2.7255 k); then in 1992, further cobe measurements discovered tiny fluctuations (anisotropies) in the cmb temperature across the sky, at a level of about one part in 105.85 john c. mather and george smoot were awarded the 2006 nobel prize in physics for their leadership in these results. during the following decade, cmb anisotropies were further investigated by a large number of ground-based and balloon experiments. in 20002001, several experiments, most notably boomerang, found the shape of the universe to be spatially almost flat by measuring the typical angular size (the size on the sky) of the anisotropies.105106107 in early 2003, the first results of the wilkinson microwave anisotropy probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. the results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general.86 the planck space probe was launched in may 2009. other"
  },
  {
    "chunk_id": 247,
    "doc_id": "The_Big_Bang.txt",
    "text": "the wilkinson microwave anisotropy probe were released, yielding what were at the time the most accurate values for some of the cosmological parameters. the results disproved several specific cosmic inflation models, but are consistent with the inflation theory in general.86 the planck space probe was launched in may 2009. other ground and balloon-based cosmic microwave background experiments are ongoing. abundance of primordial elements main article: big bang nucleosynthesis time evolution of light element abundances during big bang nucleosynthesis using big bang models, it is possible to calculate the expected concentration of the isotopes helium-4 (4he), helium-3 (3he), deuterium (2h), and lithium-7 (7li) in the universe as ratios to the amount of ordinary hydrogen.37 the relative abundances depend on a single parameter, the ratio of photons to baryons. this value can be calculated independently from the detailed structure of cmb fluctuations. the ratios predicted (by mass, not by abundance) are about 0.25 for 4he:h, about 103 for 2h:h, about 104 for 3he:h, and about 109 for 7li:h.37 the measured abundances all agree at least roughly with those predicted from a single value of the baryon-to-photon ratio. the agreement is excellent for deuterium, close but formally discrepant for 4he, and off by a factor of two for 7li (this anomaly is known as the cosmological lithium problem); in the latter two cases, there are substantial systematic uncertainties. nonetheless, the general consistency with abundances predicted by bbn is strong evidence for the big bang, as the theory is the only known explanation for the relative abundances of light elements, and it is virtually impossible to tune the big bang to produce much more or less than 2030 helium.108 indeed, there is no obvious reason outside of the big bang that, for example, the young universe before star formation, as determined by studying matter supposedly free of stellar nucleosynthesis products, should have more helium than deuterium or more deuterium than 3he, and in constant ratios, too.109: 182185 galactic evolution and distribution main articles: galaxy formation and evolution and structure formation detailed observations of the morphology and distribution of galaxies and quasars are in agreement with the current big bang models. a combination of observations and theory suggest that the first quasars and galaxies formed within a billion years after the big bang,110 and since then, larger structures have been forming, such as galaxy clusters and superclusters.111 populations of stars have been aging and evolving,"
  },
  {
    "chunk_id": 248,
    "doc_id": "The_Big_Bang.txt",
    "text": "with the current big bang models. a combination of observations and theory suggest that the first quasars and galaxies formed within a billion years after the big bang,110 and since then, larger structures have been forming, such as galaxy clusters and superclusters.111 populations of stars have been aging and evolving, so that distant galaxies (which are observed as they were in the early universe) appear very different from nearby galaxies (observed in a more recent state). moreover, galaxies that formed relatively recently appear markedly different from galaxies formed at similar distances but shortly after the big bang. these observations are strong arguments against the steady-state model. observations of star formation, galaxy and quasar distributions and larger structures, agree well with big bang simulations of the formation of structure in the universe, and are helping to complete details of the theory.111112 primordial gas clouds focal plane of bicep2 telescope under a microscope used to search for polarization in the cmb113114115116 in 2011, astronomers found what they believe to be pristine clouds of primordial gas by analyzing absorption lines in the spectra of distant quasars. before this discovery, all other astronomical objects have been observed to contain heavy elements that are formed in stars. despite being sensitive to carbon, oxygen, and silicon, these three elements were not detected in these two clouds.117118 since the clouds of gas have no detectable levels of heavy elements, they likely formed in the first few minutes after the big bang, during bbn. other lines of evidence the age of the universe as estimated from the hubble expansion and the cmb is now in agreement with other estimates using the ages of the oldest stars, both as measured by applying the theory of stellar evolution to globular clusters and through radiometric dating of individual population ii stars.119 it is also in agreement with age estimates based on measurements of the expansion using type ia supernovae and measurements of temperature fluctuations in the cosmic microwave background.120 the agreement of independent measurements of this age supports the lambda-cdm (cdm) model, since the model is used to relate some of the measurements to an age estimate, and all estimates turn agree. still, some observations of objects from the relatively early universe (in particular quasar apm 082795255) raise concern as to whether these objects had enough time to form so early in the cdm model.121122 the prediction that the cmb temperature"
  },
  {
    "chunk_id": 249,
    "doc_id": "The_Big_Bang.txt",
    "text": "the measurements to an age estimate, and all estimates turn agree. still, some observations of objects from the relatively early universe (in particular quasar apm 082795255) raise concern as to whether these objects had enough time to form so early in the cdm model.121122 the prediction that the cmb temperature was higher in the past has been experimentally supported by observations of very low temperature absorption lines in gas clouds at high redshift.123 this prediction also implies that the amplitude of the sunyaevzeldovich effect in clusters of galaxies does not depend directly on redshift. observations have found this to be roughly true, but this effect depends on cluster properties that do change with cosmic time, making precise measurements difficult.124125 future observations future gravitational-wave observatories might be able to detect primordial gravitational waves, relics of the early universe, up to less than a second after the big bang.126127 problems and related issues in physics see also: list of unsolved problems in physics as with any theory, a number of mysteries and problems have arisen as a result of the development of the big bang models. some of these mysteries and problems have been resolved while others are still outstanding. proposed solutions to some of the problems in the big bang model have revealed new mysteries of their own. for example, the horizon problem, the magnetic monopole problem, and the flatness problem are most commonly resolved with inflation theory, but the details of the inflationary universe are still left unresolved and many, including some founders of the theory, say it has been disproven.128129130131 what follows are a list of the mysterious aspects of the big bang concept still under intense investigation by cosmologists and astrophysicists. baryon asymmetry main article: baryon asymmetry it is not yet understood why the universe has more matter than antimatter.34 it is generally assumed that when the universe was young and very hot it was in statistical equilibrium and contained equal numbers of baryons and antibaryons. both matter and antimatter were much more abundant than today, with a tiny asymmetry of only one part in 10 billion. the matter and antimatter collided and annihilated, leaving only the residual amount of matter. today observations suggest that the universe, including its most distant parts, is made almost entirely of normal matter with very little antimatter.132 if matter and antimatter were in complete symmetry, then annihilation would result in only photons"
  },
  {
    "chunk_id": 250,
    "doc_id": "The_Big_Bang.txt",
    "text": "matter and antimatter collided and annihilated, leaving only the residual amount of matter. today observations suggest that the universe, including its most distant parts, is made almost entirely of normal matter with very little antimatter.132 if matter and antimatter were in complete symmetry, then annihilation would result in only photons and virtually no matter at all, which is obviously not what is observed. a process called baryogenesis was hypothesized to account for the asymmetry. for baryogenesis to occur, the sakharov conditions must be satisfied. these require that baryon number is not conserved, that c-symmetry and cp-symmetry are violated and that the universe depart from thermodynamic equilibrium.133134 all these conditions occur in the standard model, but the effects are not strong enough to explain the present baryon asymmetry. dark energy main article: dark energy measurements of the redshiftmagnitude relation for type ia supernovae indicate that the expansion of the universe has been accelerating since the universe was about half its present age. to explain this acceleration, cosmological models require that much of the energy in the universe consists of a component with large negative pressure, dubbed dark energy.11 dark energy, though speculative, solves numerous problems. measurements of the cosmic microwave background indicate that the universe is very nearly spatially flat, and therefore according to general relativity the universe must have almost exactly the critical density of massenergy. but the mass density of the universe can be measured from its gravitational clustering, and is found to have only about 30 of the critical density.11 since theory suggests that dark energy does not cluster in the usual way it is the best explanation for the missing energy density. dark energy also helps to explain two geometrical measures of the overall curvature of the universe, one using the frequency of gravitational lenses,135 and the other using the characteristic pattern of the large-scale structure--baryon acoustic oscillations--as a cosmic ruler.136137 negative pressure is believed to be a property of vacuum energy, but the exact nature and existence of dark energy remains one of the great mysteries of the big bang. results from the wmap team in 2008 are in accordance with a universe that consists of 73 dark energy, 23 dark matter, 4.6 regular matter and less than 1 neutrinos.40 according to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as"
  },
  {
    "chunk_id": 251,
    "doc_id": "The_Big_Bang.txt",
    "text": "2008 are in accordance with a universe that consists of 73 dark energy, 23 dark matter, 4.6 regular matter and less than 1 neutrinos.40 according to theory, the energy density in matter decreases with the expansion of the universe, but the dark energy density remains constant (or nearly so) as the universe expands. therefore, matter made up a larger fraction of the total energy of the universe in the past than it does today, but its fractional contribution will fall in the far future as dark energy becomes even more dominant.citation needed the dark energy component of the universe has been explained by theorists using a variety of competing theories including einsteins cosmological constant but also extending to more exotic forms of quintessence or other modified gravity schemes.138 a cosmological constant problem, sometimes called the most embarrassing problem in physics, results from the apparent discrepancy between the measured energy density of dark energy, and the one naively predicted from planck units.139 dark matter main article: dark matter chart shows the proportion of different components of the universe about 95 is dark matter and dark energy. during the 1970s and the 1980s, various observations showed that there is not sufficient visible matter in the universe to account for the apparent strength of gravitational forces within and between galaxies. this led to the idea that up to 90 of the matter in the universe is dark matter that does not emit light or interact with normal baryonic matter. in addition, the assumption that the universe is mostly normal matter led to predictions that were strongly inconsistent with observations. in particular, the universe today is far more lumpy and contains far less deuterium than can be accounted for without dark matter. while dark matter has always been controversial, it is inferred by various observations: the anisotropies in the cmb, the galaxy rotation problem, galaxy cluster velocity dispersions, large-scale structure distributions, gravitational lensing studies, and x-ray measurements of galaxy clusters.140 indirect evidence for dark matter comes from its gravitational influence on other matter, as no dark matter particles have been observed in laboratories. many particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.141 additionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem96 and the cuspy halo problem.95 alternative theories have been proposed that do"
  },
  {
    "chunk_id": 252,
    "doc_id": "The_Big_Bang.txt",
    "text": "particle physics candidates for dark matter have been proposed, and several projects to detect them directly are underway.141 additionally, there are outstanding problems associated with the currently favored cold dark matter model which include the dwarf galaxy problem96 and the cuspy halo problem.95 alternative theories have been proposed that do not require a large amount of undetected matter, but instead modify the laws of gravity established by newton and einstein; yet no alternative theory has been as successful as the cold dark matter proposal in explaining all extant observations.142 horizon problem main article: horizon problem the horizon problem results from the premise that information cannot travel faster than light. in a universe of finite age this sets a limitthe particle horizonon the separation of any two regions of space that are in causal contact.143 the observed isotropy of the cmb is problematic in this regard: if the universe had been dominated by radiation or matter at all times up to the epoch of last scattering, the particle horizon at that time would correspond to about 2 degrees on the sky. there would then be no mechanism to cause wider regions to have the same temperature.109: 191 a resolution to this apparent inconsistency is offered by inflation theory in which a homogeneous and isotropic scalar energy field dominates the universe at some very early period (before baryogenesis). during inflation, the universe undergoes exponential expansion, and the particle horizon expands much more rapidly than previously assumed, so that regions presently on opposite sides of the observable universe are well inside each others particle horizon. the observed isotropy of the cmb then follows from the fact that this larger region was in causal contact before the beginning of inflation.28: 180 heisenbergs uncertainty principle predicts that during the inflationary phase there would be quantum thermal fluctuations, which would be magnified to a cosmic scale. these fluctuations served as the seeds for all the current structures in the universe.109: 207 inflation predicts that the primordial fluctuations are nearly scale invariant and gaussian, which has been confirmed by measurements of the cmb.86: sec 6 a related issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch"
  },
  {
    "chunk_id": 253,
    "doc_id": "The_Big_Bang.txt",
    "text": "issue to the classic horizon problem arises because in most standard cosmological inflation models, inflation ceases well before electroweak symmetry breaking occurs, so inflation should not be able to prevent large-scale discontinuities in the electroweak vacuum since distant parts of the observable universe were causally separate when the electroweak epoch ended.144 magnetic monopoles the magnetic monopole objection was raised in the late 1970s. grand unified theories (guts) predicted topological defects in space that would manifest as magnetic monopoles. these objects would be produced efficiently in the hot early universe, resulting in a density much higher than is consistent with observations, given that no monopoles have been found. this problem is resolved by cosmic inflation, which removes all point defects from the observable universe, in the same way that it drives the geometry to flatness.143 flatness problem the overall geometry of the universe is determined by whether the omega cosmological parameter is less than, equal to or greater than 1. shown from top to bottom are a closed universe with positive curvature, a hyperbolic universe with negative curvature and a flat universe with zero curvature. the flatness problem (also known as the oldness problem) is an observational problem associated with a flrw.143 the universe may have positive, negative, or zero spatial curvature depending on its total energy density. curvature is negative if its density is less than the critical density; positive if greater; and zero at the critical density, in which case space is said to be flat. observations indicate the universe is consistent with being flat.145146 the problem is that any small departure from the critical density grows with time, and yet the universe today remains very close to flat.notes 2 given that a natural timescale for departure from flatness might be the planck time, 1043 seconds,33 the fact that the universe has reached neither a heat death nor a big crunch after billions of years requires an explanation. for instance, even at the relatively late age of a few minutes (the time of nucleosynthesis), the density of the universe must have been within one part in 1014 of its critical value, or it would not exist as it does today.147 misconceptions in addition to confusion about the nature of cosmic expansion, the big bang model itself is sometimes misunderstood. one of the common misconceptions about the big bang model is that it fully explains the origin of the universe."
  },
  {
    "chunk_id": 254,
    "doc_id": "The_Big_Bang.txt",
    "text": "critical value, or it would not exist as it does today.147 misconceptions in addition to confusion about the nature of cosmic expansion, the big bang model itself is sometimes misunderstood. one of the common misconceptions about the big bang model is that it fully explains the origin of the universe. however, the big bang model does not describe how energy, time, and space were caused, but rather it describes the emergence of the present universe from an ultra-dense and high-temperature initial state.148 another common misconception relates to the recession speeds associated with hubbles law. these are not velocities in a relativistic sense (for example, they are not related to the spatial components of 4-velocities). therefore, it is not remarkable that according to hubbles law, galaxies farther than the hubble distance recede faster than the speed of light. such recession speeds do not correspond to faster-than-light travel.149 implications given current understanding, scientific extrapolations about the future of the universe are only possible for finite durations, albeit for much longer periods than the current age of the universe. anything beyond that becomes increasingly speculative. likewise, at present, a proper understanding of the origin of the universe can only be subject to conjecture.150 prebig bang cosmology the big bang explains the evolution of the universe from a starting density and temperature that is well beyond humanitys capability to replicate, so extrapolations to the most extreme conditions and earliest times are necessarily more speculative. lematre called this initial state the primeval atom while gamow called the material ylem. how the initial state of the universe originated is still an open question, but the big bang model does constrain some of its characteristics. for example, if specific laws of nature were to come to existence in a random way, inflation models show, some combinations of these are far more probable,151 partly explaining why our universe is rather stable. another possible explanation for the stability of the universe could be a hypothetical multiverse, which assumes every possible universe to exist, and thinking species could only emerge in those stable enough.152 a flat universe implies a balance between gravitational potential energy and other energy forms, requiring no additional energy to be created.145146 the big bang theory is built upon the equations of classical general relativity, which are not expected to be valid at the origin of cosmic time, as the temperature of the universe approaches the planck"
  },
  {
    "chunk_id": 255,
    "doc_id": "The_Big_Bang.txt",
    "text": "between gravitational potential energy and other energy forms, requiring no additional energy to be created.145146 the big bang theory is built upon the equations of classical general relativity, which are not expected to be valid at the origin of cosmic time, as the temperature of the universe approaches the planck scale. correcting this will require the development of a correct treatment of quantum gravity.23 certain quantum gravity treatments, such as the wheelerdewitt equation, imply that time itself could be an emergent property.153 as such, physics may conclude that time did not exist before the big bang.154155156157158 while it is not known what could have preceded the hot dense state of the early universe or how and why it originated, or even whether such questions are sensible, speculation abounds on the subject of cosmogony. some speculative proposals in this regard, each of which entails untested hypotheses, are: the simplest models, in which the big bang was caused by quantum fluctuations. that scenario had very little chance of happening, but, according to the totalitarian principle, even the most improbable event will eventually happen. it took place instantly, in our perspective, due to the absence of perceived time before the big bang.159160161162 emergent universe models, which feature a low-activity past-eternal era before the big bang, resembling ancient ideas of a cosmic egg and birth of the world out of primordial chaos. models in which the whole of spacetime is finite, including the hartlehawking no-boundary condition. for these cases, the big bang does represent the limit of time but without a singularity.163 in such a case, the universe is self-sufficient.164 brane cosmology models, in which inflation is due to the movement of branes in string theory; the pre-big bang model; the ekpyrotic model, in which the big bang is the result of a collision between branes; and the cyclic model, a variant of the ekpyrotic model in which collisions occur periodically. in the latter model the big bang was preceded by a big crunch and the universe cycles from one process to the other.165166167168169 eternal inflation, in which universal inflation ends locally here and there in a random fashion, each end-point leading to a bubble universe, expanding from its own big bang.170171 this is sometimes referred to as pre-big bang inflation.172 proposals in the last two categories see the big bang as an event in either a much larger and older universe or in"
  },
  {
    "chunk_id": 256,
    "doc_id": "The_Big_Bang.txt",
    "text": "there in a random fashion, each end-point leading to a bubble universe, expanding from its own big bang.170171 this is sometimes referred to as pre-big bang inflation.172 proposals in the last two categories see the big bang as an event in either a much larger and older universe or in a multiverse. ultimate fate of the universe main article: ultimate fate of the universe before observations of dark energy, cosmologists considered two scenarios for the future of the universe. if the mass density of the universe were greater than the critical density, then the universe would reach a maximum size and then begin to collapse. it would become denser and hotter again, ending with a state similar to that in which it starteda big crunch.21 alternatively, if the density in the universe were equal to or below the critical density, the expansion would slow down but never stop. star formation would cease with the consumption of interstellar gas in each galaxy; stars would burn out, leaving white dwarfs, neutron stars, and black holes. collisions between these would result in mass accumulating into larger and larger black holes. the average temperature of the universe would very gradually asymptotically approach absolute zeroa big freeze.173 moreover, if protons are unstable, then baryonic matter would disappear, leaving only radiation and black holes. eventually, black holes would evaporate by emitting hawking radiation. the entropy of the universe would increase to the point where no organized form of energy could be extracted from it, a scenario known as heat death.174 modern observations of accelerating expansion imply that more and more of the currently visible universe will pass beyond our event horizon and out of contact with us. the eventual result is not known. the lambda-cdm model of the universe contains dark energy in the form of a cosmological constant. this theory suggests that only gravitationally bound systems, such as galaxies, will remain together, and they too will be subject to heat death as the universe expands and cools. other explanations of dark energy, called phantom dark energy theories, suggest that ultimately galaxy clusters, stars, planets, atoms, nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called big rip.175"
  },
  {
    "chunk_id": 257,
    "doc_id": "The_Big_Bang.txt",
    "text": "nuclei, and matter itself will be torn apart by the ever-increasing expansion in a so-called big rip.175"
  },
  {
    "chunk_id": 258,
    "doc_id": "Machine_learning.txt",
    "text": "machine learning machine learning (ml) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalise to unseen data, and thus perform tasks without explicit instructions. 1 within a subdiscipline in machine learning, advances in the field of deep learning have allowed neural networks, a class of statistical algorithms, to surpass many previous machine learning approaches in performance. ml finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine. the application of ml to business problems is known as predictive analytics. statistics and mathematical optimisation (mathematical programming) methods comprise the foundations of machine learning. data mining is a related field of study, focusing on exploratory data analysis (eda) through unsupervised learning. 34 from a theoretical viewpoint, probably approximately correct learning provides a mathematical and statistical framework for describing machine learning. most traditional machine learning and deep learning algorithms can be described as empirical risk minimisation under this framework. the term machine learning was coined in 1959 by arthur samuel, an ibm employee and pioneer in the field of computer gaming and artificial intelligence. 56 the synonym self-teaching computers was also used during this time period.78 the earliest machine learning program was introduced in the 1950s when arthur samuel invented a computer program that calculated the winning chance in checkers for each side, but the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.9 in 1949, canadian psychologist donald hebb published the book the organization of behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells. 10 hebbs model of neurons interacting with one another set a groundwork for how ais and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.9 other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician walter pitts and warren mcculloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.9 by the early 1960s, an experimental learning machine with punched tape memory, called cybertron, had been developed by raytheon company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. it was repetitively trained by a human operatorteacher to recognise patterns and equipped"
  },
  {
    "chunk_id": 259,
    "doc_id": "Machine_learning.txt",
    "text": "that mirror human thought processes.9 by the early 1960s, an experimental learning machine with punched tape memory, called cybertron, had been developed by raytheon company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. it was repetitively trained by a human operatorteacher to recognise patterns and equipped with a goof button to cause it to reevaluate incorrect decisions.11 a representative book on research into machine learning during the 1960s was nils nilssons book on learning machines, dealing mostly with machine learning for pattern classification.12 interest related to pattern recognition history 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 140 deep learning is a subset of machine learning, which is itself a subset of artificial intelligence. 20 continued into the 1970s, as described by duda and hart in 1973.13 in 1981, a report was given on using teaching strategies so that an artificial neural network learns to recognise 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.14 tom m. mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: a computer program is said to learn from experience e with respect to some class of tasks t and performance measure p if its performance at tasks in t, as measured by p, improves with experience e.15 this definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. this follows alan turings proposal in his paper computing machinery and intelligence, in which the question, can machines think?, is replaced with the question, can machines do what we (as thinking entities) can do?.16 modern-day machine learning algorithms are broken into 3 algorithm types: supervised learning algorithms, unsupervised learning algorithms, and reinforcement learning algorithms.17 current supervised learning algorithms have objectives of classification and regression. current unsupervised learning algorithms have objectives of clustering, dimensionality reduction, and association rule. current reinforcement learning algorithms focus on decisions that must be made with respect to some previous, unknown time and are broken down to either be studies of model-based methods or model-free methods. in 2014 ian goodfellow and others introduced generative adversarial networks (gans) with realistic data synthesis.18 by 2016 alphago obtained victory against top human players using reinforcement learning techniques.19 as a scientific endeavour, machine learning grew out of the quest for artificial intelligence (ai). in the early days of"
  },
  {
    "chunk_id": 260,
    "doc_id": "Machine_learning.txt",
    "text": "model-free methods. in 2014 ian goodfellow and others introduced generative adversarial networks (gans) with realistic data synthesis.18 by 2016 alphago obtained victory against top human players using reinforcement learning techniques.19 as a scientific endeavour, machine learning grew out of the quest for artificial intelligence (ai). in the early days of ai as an academic discipline, some researchers were interested in having machines learn from data. they attempted to approach the problem with various symbolic methods, as well as what were then termed neural networks; these were mostly perceptrons and other models that were later found to be reinventions of the generalised linear models of statistics.21 probabilistic reasoning was also employed, especially in automated medical diagnosis. 22:488 however, an increasing emphasis on the logical, knowledge-based approach caused a rift between ai and machine learning. probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.22:488 by 1980, expert systems had come to dominate ai, and statistics was out of favour.23 work on symbolicknowledge-based learning relationships to other fields artificial intelligence 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 240 did continue within ai, leading to inductive logic programming(ilp), but the more statistical line of research was now outside the field of ai proper, in pattern recognition and information retrieval. 22:708710,755 neural networks research had been abandoned by ai and computer science around the same time. this line, too, was continued outside the aics field, as connectionism, by researchers from other disciplines, including john hopfield, david rumelhart, and geoffrey hinton. their main success came in the mid-1980s with the reinvention of backpropagation. 22:25 machine learning (ml), reorganised and recognised as its own field, started to flourish in the 1990s. the field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. it shifted focus away from the symbolic approaches it had inherited from ai, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory. 23 there is a close connection between machine learning and compression. a system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). this equivalence has been used as a justification for using data compression as a benchmark for general intelligence.242526"
  },
  {
    "chunk_id": 261,
    "doc_id": "Machine_learning.txt",
    "text": "for optimal data compression (by using arithmetic coding on the output distribution). conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). this equivalence has been used as a justification for using data compression as a benchmark for general intelligence.242526 an alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. for each compressor c(.) we define an associated vector space , such that c(.) maps an input string x, corresponding to the vector norm x. an exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, lzw, lz77, and ppm.27 according to aixi theory, a connection more directly explained in hutter prize, the best possible compression of x is the smallest possible software that generates x. for example, in that model, a zip files compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form. examples of ai-powered audiovideo compression software include nvidia maxine, aivc.28 examples of software that can perform ai-powered image compression include opencv, tensorflow, matlabs image processing toolbox (ipt) and high-fidelity generative image compression.29 in unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. this technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression. 30 data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. k-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. this process condenses extensive datasets into a more compact set of representative points. particularly beneficial in image and signal processing, k-means clustering data compression 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 340 aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.31 large language models (llms) are also efficient lossless data compressors on some data sets, as demonstrated by deepminds research with the chinchilla 70b model. developed by deepmind, chinchilla 70b"
  },
  {
    "chunk_id": 262,
    "doc_id": "Machine_learning.txt",
    "text": "data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.31 large language models (llms) are also efficient lossless data compressors on some data sets, as demonstrated by deepminds research with the chinchilla 70b model. developed by deepmind, chinchilla 70b effectively compressed data, outperforming conventional methods such as portable network graphics (png) for images and free lossless audio codec (flac) for audio. it achieved compression of image and audio data to 43.4 and 16.4 of their original sizes, respectively. there is, however, some reason to be concerned that the data set used for testing overlaps the llm training data set, making it possible that the chinchilla 70b model is only an efficient compression tool on data it has already been trained on.3233 machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as unsupervised learning or as a preprocessing step to improve learner accuracy. much of the confusion between these two research communities (which do often have separate conferences and separate journals, ecml pkdd being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (kdd) the key task is the discovery of previously unknown knowledge. evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical kdd task, supervised methods cannot be used due to the unavailability of training data. machine learning also has intimate ties to optimisation: many learning problems are formulated as minimisation of some loss function on a training set of examples. loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).34 characterizing the generalisation of various learning algorithms is an active"
  },
  {
    "chunk_id": 263,
    "doc_id": "Machine_learning.txt",
    "text": "between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).34 characterizing the generalisation of various learning algorithms is an active topic of current research, especially for deep learning algorithms. machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalisable predictive patterns.35 conventional statistical analyses require the a priori selection of a model most suitable for the study data set. in addition, only significant or theoretically relevant variables based on previous experience are included for analysis. in contrast, machine learning is not built on a pre-structured data mining generalization statistics 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 440 model; rather, the data shape the model by detecting underlying patterns. the more variables (input) used to train the model, the more accurate the ultimate model will be.36 leo breiman distinguished two statistical modelling paradigms: data model and algorithmic model,37 wherein algorithmic model means more or less the machine learning algorithms like random forest. some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning. 38 analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks. 39 statistical physics is thus finding applications in the area of medical diagnostics. 40 a core objective of a learner is to generalise from its experience.241 generalization in this context is the ability of a learning machine to perform accurately on new, unseen examplestasks after having experienced a learning data set. the training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases. the computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. instead, probabilistic bounds on the performance are quite"
  },
  {
    "chunk_id": 264,
    "doc_id": "Machine_learning.txt",
    "text": "performance is a branch of theoretical computer science known as computational learning theory via the probably approximately correct learning model. because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. instead, probabilistic bounds on the performance are quite common. the biasvariance decomposition is one way to quantify generalisation error. for the best performance in the context of generalisation, the complexity of the hypothesis should match the complexity of the function underlying the data. if the hypothesis is less complex than the function, then the model has underfitted the data. if the complexity of the model is increased in response, then the training error decreases. but if the hypothesis is too complex, then the model is subject to overfitting and generalisation will be poorer.42 in addition to performance bounds, learning theorists study the time complexity and feasibility of learning. in computational learning theory, a computation is considered feasible if it can be done in polynomial time. there are two kinds of time complexity results: positive results show that a certain class of functions can be learned in polynomial time. negative results show that certain classes cannot be learned in polynomial time. statistical physics theory approaches 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 540 in supervised learning, the training data is labelled with the expected answers, while in unsupervised learning, the model identifies patterns or structures in unlabelled data. a support-vector machine is a supervised learning model that divides the data into regions separated by a linear boundary. here, the linear boundary divides the black circles from the white. machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the signal or feedback available to the learning system: supervised learning: the computer is presented with example inputs and their desired outputs, given by a teacher, and the goal is to learn a general rule that maps inputs to outputs. unsupervised learning: no labels are given to the learning algorithm, leaving it on its own to find structure in its input. unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). reinforcement learning: a computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an"
  },
  {
    "chunk_id": 265,
    "doc_id": "Machine_learning.txt",
    "text": "unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning). reinforcement learning: a computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent). as it navigates its problem space, the program is provided feedback thats analogous to rewards, which it tries to maximise.2 although each algorithm has advantages and limitations, no single algorithm works for all problems.434445 supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.46 the data, known as training data, consists of a set of training examples. each training example has one or more inputs and the desired output, also known as a supervisory signal. in the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. through iterative optimisation of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.47 an optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. an algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.15 types of supervised-learning algorithms include active learning, classification and regression. 48 classification algorithms are used when the outputs are restricted to a limited set of values, while regression algorithms are used when the outputs can take any numerical value within a range. for example, in a classification algorithm that filters emails, the input is an incoming email, and the output is the folder in which to file the email. in contrast, regression is used for tasks such as predicting a persons height based on factors like age and genetics or forecasting future temperatures based on historical data.49 supervised learning 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 640 similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. it has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. unsupervised learning algorithms find structures in data that"
  },
  {
    "chunk_id": 266,
    "doc_id": "Machine_learning.txt",
    "text": "closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. it has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification. unsupervised learning algorithms find structures in data that has not been labelled, classified or categorised. instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. central applications of unsupervised machine learning include clustering, dimensionality reduction, 4 and density estimation. 50 cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. other methods are based on estimated density and graph connectivity. a special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.5152 semi-supervised learning falls between unsupervised learning (without any labelled training data) and supervised learning (with completely labelled training data). some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabelled data, when used in conjunction with a small amount of labelled data, can produce a considerable improvement in learning accuracy. in weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.53 reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment to maximise some notion of cumulative reward. due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimisation, multi-agent systems, swarm intelligence, statistics and genetic algorithms. in reinforcement learning, the environment is typically represented as a markov decision process (mdp). many reinforcement learning algorithms use dynamic programming techniques.54 reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the mdp and are used when exact models are infeasible. reinforcement learning"
  },
  {
    "chunk_id": 267,
    "doc_id": "Machine_learning.txt",
    "text": "and genetic algorithms. in reinforcement learning, the environment is typically represented as a markov decision process (mdp). many reinforcement learning algorithms use dynamic programming techniques.54 reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the mdp and are used when exact models are infeasible. reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent. unsupervised learning semi-supervised learning reinforcement learning 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 740 in reinforcement learning, an agent takes actions in an environment: these produce a reward andor a representation of the state, which is fed back to the agent. dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables. 55 in other words, it is a process of reducing the dimension of the feature set, also called the number of features. most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. one of the popular methods of dimensionality reduction is principal component analysis (pca). pca involves changing higherdimensional data (e.g., 3d) to a smaller space (e.g., 2d). the manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the areas of manifold learning and manifold regularisation. other approaches have been developed which do not fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system. for example, topic modelling, meta-learning. 56 self-learning, as a machine learning paradigm, was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (caa).5758 it gives a solution to the problem learning without any external reward, by introducing emotion as an internal reward. emotion is used as a state evaluation of a self-learning agent. the caa self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. the system is driven by the interaction between cognition and emotion.59 the self-learning algorithm updates a memory matrix w w(a,s) such that in each iteration executes the following machine learning routine: 1. in situation s act a 2. receive a consequence situation s 3. compute emotion of being in the consequence situation v(s) 4. update crossbar memory w(a,s) w(a,s) v(s) it is a system with only one input, situation, and only one output,"
  },
  {
    "chunk_id": 268,
    "doc_id": "Machine_learning.txt",
    "text": "each iteration executes the following machine learning routine: 1. in situation s act a 2. receive a consequence situation s 3. compute emotion of being in the consequence situation v(s) 4. update crossbar memory w(a,s) w(a,s) v(s) it is a system with only one input, situation, and only one output, action (or behaviour) a. there is neither a separate reinforcement input nor an advice input from the environment. the backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. the caa exists in two environments, one is the behavioural environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioural environment. after receiving the genome (species) vector from the genetic environment, the caa learns a goal-seeking behaviour in an environment that contains both desirable and undesirable situations.60 dimensionality reduction other types self-learning 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 840 several learning algorithms aim at discovering better representations of the inputs provided during training.61 classic examples include principal component analysis and cluster analysis. feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a preprocessing step before performing classification or predictions. this technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. this replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task. feature learning can be either supervised or unsupervised. in supervised feature learning, features are learned using labelled input data. examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. in unsupervised feature learning, features are learned with unlabelled input data. examples include dictionary learning, independent component analysis, autoencoders, matrix factorisation62 and various forms of clustering. 636465 manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higherdimensional vectors.66 deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with"
  },
  {
    "chunk_id": 269,
    "doc_id": "Machine_learning.txt",
    "text": "that the learned representation is sparse, meaning that the mathematical model has many zeros. multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higherdimensional vectors.66 deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. it has been argued that an intelligent machine learns a representation that disentangles the underlying factors of variation that explain the observed data.67 feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. however, realworld data such as images, video, and sensory data have not yielded attempts to algorithmically define specific features. an alternative is to discover such features or representations through examination, without relying on explicit algorithms. sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. the method is strongly np-hard and difficult to solve approximately.68 a popular heuristic method for sparse dictionary learning is the k-svd algorithm. sparse dictionary learning has been applied in several contexts. in classification, the problem is to determine the class to which a previously unseen training example belongs. for a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. sparse dictionary learning has also been applied in image denoising. the key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.69 in data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations that raise suspicions by differing significantly from the majority of the data.70 typically, the anomalous items represent an issue such as bank fraud, a structural feature learning sparse dictionary learning anomaly detection 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 940 defect, medical problems or errors in a text. anomalies are referred to as outliers, novelties, noise, deviations and exceptions.71 in particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. this pattern does not adhere to the common statistical definition of an outlier as a rare object. many outlier"
  },
  {
    "chunk_id": 270,
    "doc_id": "Machine_learning.txt",
    "text": "to as outliers, novelties, noise, deviations and exceptions.71 in particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare, but unexpected bursts of inactivity. this pattern does not adhere to the common statistical definition of an outlier as a rare object. many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.72 three broad categories of anomaly detection techniques exist.73 unsupervised anomaly detection techniques detect anomalies in an unlabelled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. supervised anomaly detection techniques require a data set that has been labelled as normal and abnormal and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). semi-supervised anomaly detection techniques construct a model representing normal behaviour from a given normal training data set and then test the likelihood of a test instance being generated by the model. robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,7475 and finally meta-learning (e.g. maml). association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. it is intended to identify strong rules discovered in databases using some measure of interestingness.76 rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves rules to store, manipulate or apply knowledge. the defining characteristic of a rule-based machine learning algorithm is the identification and utilisation of a set of relational rules that collectively represent the knowledge captured by the system. this is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.77 rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems. based on the concept of strong rules, rakesh agrawal, tomasz imieliski and arun swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (pos) systems in supermarkets.78 for example, the rule found in the sales data of a supermarket would indicate that"
  },
  {
    "chunk_id": 271,
    "doc_id": "Machine_learning.txt",
    "text": "immune systems. based on the concept of strong rules, rakesh agrawal, tomasz imieliski and arun swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (pos) systems in supermarkets.78 for example, the rule found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. in addition to market basket analysis, association rules are employed today in application areas including web usage mining, intrusion detection, continuous production, and bioinformatics. in contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions. robot learning association rules 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1040 learning classifier systems (lcs) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. they seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner to make predictions.79 inductive logic programming (ilp) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ilp system will derive a hypothesized logic program that entails all positive and no negative examples. inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs. inductive logic programming is particularly useful in bioinformatics and natural language processing. gordon plotkin and ehud shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.808182 shapiro built their first implementation (model inference system) in 1981: a prolog program that inductively inferred logic programs from positive and negative examples.83 the term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. a machine learning model is a type of mathematical model that, once trained on a given dataset, can be used to make predictions or classifications on new data. during training, a"
  },
  {
    "chunk_id": 272,
    "doc_id": "Machine_learning.txt",
    "text": "theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set. a machine learning model is a type of mathematical model that, once trained on a given dataset, can be used to make predictions or classifications on new data. during training, a learning algorithm iteratively adjusts the models internal parameters to minimise errors in its predictions.84 by extension, the term model can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.85 various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection. artificial neural networks (anns), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. such systems learn to perform tasks by considering examples, generally without being programmed with any taskspecific rules. an ann is a model based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. each connection, like the synapses in a biological brain, can transmit information, a signal, from one artificial neuron to another. an artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. in common ann implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. the connections between artificial neurons are called edges. artificial neurons and edges typically have a weight that adjusts as learning proceeds. the weight models artificial neural networks 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1140 an artificial neural network is an interconnected group of nodes, akin to the vast network of neurons in a brain. here, each circular node represents an artificial neuron and an arrow represents a connection from the output of one artificial neuron to the input of another. a decision tree showing survival probability of passengers on the titanic increases or decreases the strength of the signal at a connection. artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. typically, artificial neurons are aggregated into layers. different layers may perform different kinds of transformations on their"
  },
  {
    "chunk_id": 273,
    "doc_id": "Machine_learning.txt",
    "text": "the titanic increases or decreases the strength of the signal at a connection. artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. typically, artificial neurons are aggregated into layers. different layers may perform different kinds of transformations on their inputs. signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times. the original goal of the ann approach was to solve problems in the same way that a human brain would. however, over time, attention moved to performing specific tasks, leading to deviations from biology. artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis. deep learning consists of multiple hidden layers in an artificial neural network. this approach tries to model the way the human brain processes light and sound into vision and hearing. some successful applications of deep learning are computer vision and speech recognition.86 decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the items target value (represented in the leaves). it is one of the predictive modelling approaches used in statistics, data mining, and machine learning. tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. in decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. in data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making. decision trees 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1240 illustration of linear regression on a data set random forest regression (rfr) falls under the umbrella of decision tree-based models. rfr is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. to build decision trees, rfr uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. this random selection of rfr for training"
  },
  {
    "chunk_id": 274,
    "doc_id": "Machine_learning.txt",
    "text": "is an ensemble learning method that builds multiple decision trees and averages their predictions to improve accuracy and to avoid overfitting. to build decision trees, rfr uses bootstrapped sampling; for instance, each decision tree is trained on random data from the training set. this random selection of rfr for training enables the model to reduce biased predictions and achieve a higher degree of accuracy. rfr generates independent decision trees, and it can work on singleoutput data as well as multiple regressor tasks. this makes rfr compatible to be use in various applications.8788 support-vector machines (svms), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. given a set of training examples, each marked as belonging to one of two categories, an svm training algorithm builds a model that predicts whether a new example falls into one category.89 an svm training algorithm is a non-probabilistic, binary, linear classifier, although methods such as platt scaling exist to use svm in a probabilistic classification setting. in addition to performing linear classification, svms can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. the latter is often extended by regularisation methods to mitigate overfitting and bias, as in ridge regression. when dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in microsoft excel90 ), logistic regression (often used in statistical classification) or even kernel regression, which introduces nonlinearity by taking advantage of the kernel trick to implicitly map input variables to higherdimensional space. multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. this approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. it is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,91 which are inherently multi-dimensional. random forest regression support-vector machines regression analysis 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1340 a simple bayesian network. rain influences whether the sprinkler"
  },
  {
    "chunk_id": 275,
    "doc_id": "Machine_learning.txt",
    "text": "is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,91 which are inherently multi-dimensional. random forest regression support-vector machines regression analysis 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1340 a simple bayesian network. rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet. an example of gaussian process regression (prediction) compared with other regression models 92 a bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (dag). for example, a bayesian network could represent the probabilistic relationships between diseases and symptoms. given symptoms, the network can be used to compute the probabilities of the presence of various diseases. efficient algorithms exist that perform inference and learning. bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic bayesian networks. generalisations of bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams. a gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a predefined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations. given a set of observed points, or inputoutput examples, the distribution of the (unobserved) output of a new point as a function of its input data can be directly computed by looking at the observed points and the covariances between those points and the new, unobserved point. gaussian processes are popular surrogate models in bayesian optimisation used to do hyperparameter optimisation. a genetic algorithm (ga) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. in machine learning, genetic algorithms were used in the 1980s and 1990s.9394 conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms. 95 the theory of belief functions, also referred to as evidence theory or dempstershafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. these theoretical frameworks"
  },
  {
    "chunk_id": 276,
    "doc_id": "Machine_learning.txt",
    "text": "used to improve the performance of genetic and evolutionary algorithms. 95 the theory of belief functions, also referred to as evidence theory or dempstershafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and imprecise probability theories. these theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g., dempsters rule of combination), just like how in a pmf-based bayesian bayesian networks gaussian processes genetic algorithms belief functions 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1440 approach would combine probabilities.96 however, there are many caveats to these beliefs functions when compared to bayesian approaches to incorporate ignorance and uncertainty quantification. these belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learners decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.976 however, the computational complexity of these algorithms is dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches. rule-based machine learning (rbml) is a branch of machine learning that automatically discovers and learns rules from data. it provides interpretable models, making it useful for decision-making in fields like healthcare, fraud detection, and cybersecurity. key rbml techniques includes learning classifier systems, 98 association rule learning, 99 artificial immune systems, 100 and other similar models. these methods extract patterns from data and evolve rules over time. typically, machine learning models require a high quantity of reliable data to perform accurate predictions. when training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. overfitting is something to watch out for when training a machine learning model. trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. algorithmic bias is a potential result of data not being fully prepared for training. machine learning ethics is becoming a field of study and, notably, becoming integrated within machine"
  },
  {
    "chunk_id": 277,
    "doc_id": "Machine_learning.txt",
    "text": "in skewed or undesired predictions. biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. algorithmic bias is a potential result of data not being fully prepared for training. machine learning ethics is becoming a field of study and, notably, becoming integrated within machine learning engineering teams. federated learning is an adapted form of distributed artificial intelligence to train machine learning models that decentralises the training process, allowing for users privacy to be maintained by not needing to send their data to a centralised server. this also increases efficiency by decentralising the training process to many devices. for example, gboard uses federated machine learning to train search query prediction models on users mobile phones without having to send individual searches back to google. 101 there are many applications for machine learning, including: agriculture anatomy adaptive website affective computing astronomy automated decision-making banking behaviorism bioinformatics brainmachine interfaces rule-based models training models federated learning applications 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1540 cheminformatics citizen science climate science computer networks computer vision credit-card fraud detection data quality dna sequence classification economics financial data analysis102 general game playing handwriting recognition healthcare information retrieval insurance internet fraud detection investment management103 knowledge graph embedding linguistics machine learning control machine perception machine translation material engineering marketing medical diagnosis natural language processing natural language understanding online advertising optimisation recommender systems robot locomotion search engines sentiment analysis sequence mining software engineering speech recognition structural health monitoring syntactic pattern recognition telecommunications theorem proving time-series forecasting tomographic reconstruction104 user behaviour analytics in 2006, the media-services provider netflix held the first netflix prize competition to find a program to better predict user preferences and improve the accuracy of its existing cinematch movie recommendation algorithm by at least 10. a joint team made up of researchers from att labs-research in collaboration with the teams big chaos and pragmatic theory built an ensemble model to win the grand prize in 2009 for 1 million.105 shortly after the prize was awarded, netflix realised that viewers ratings were not the best indicators of their viewing patterns (everything is a recommendation) and they changed their recommendation engine accordingly.106 in 2010, an article in the wall street journal noted the use of machine learning by rebellion research to predict the 2008 financial crisis. 107 in 2012, co-founder of sun microsystems, vinod khosla, predicted that 80 of medical doctors jobs would be"
  },
  {
    "chunk_id": 278,
    "doc_id": "Machine_learning.txt",
    "text": "recommendation) and they changed their recommendation engine accordingly.106 in 2010, an article in the wall street journal noted the use of machine learning by rebellion research to predict the 2008 financial crisis. 107 in 2012, co-founder of sun microsystems, vinod khosla, predicted that 80 of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.108 in 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognised influences among artists.109 in 2019 springer nature published the first research book created using machine learning.110 in 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for covid-19.111 machine learning was recently applied to predict the proenvironmental behaviour of travellers.112 recently, machine learning technology was also applied to optimise smartphones performance and thermal behaviour based on the users interaction with the phone.113114115 when applied correctly, machine learning algorithms (mlas) can utilise a wide range of company characteristics to predict stock returns without overfitting. by employing effective feature engineering and combining forecasts, mlas can generate results that far surpass those obtained from basic linear techniques like ols. 116 recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.117 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1640 machine learning is becoming a useful tool to investigate and predict evacuation decision-making in large-scale and small-scale disasters. different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.118119120 other applications have been focusing on pre evacuation decisions in building fires.121122 although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.123124125 reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.126 the black box theory poses another yet significant challenge. black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted"
  },
  {
    "chunk_id": 279,
    "doc_id": "Machine_learning.txt",
    "text": "lack of resources, and evaluation problems.126 the black box theory poses another yet significant challenge. black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted from the data.127 the house of lords select committee, which claimed that such an intelligence system that could have a substantial impact on an individuals life would not be considered acceptable unless it provided a full and satisfactory explanation for the decisions it makes.127 in 2018, a self-driving car from uber failed to detect a pedestrian, who was killed after a collision.128 attempts to use machine learning in healthcare with the ibm watson system failed to deliver even after years of time and billions of dollars invested.129130 microsofts bing chat chatbot has been reported to produce hostile and offensive response against its users.131 machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. while it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research itself.132 explainable ai (xai), or interpretable ai, or explainable machine learning (xml), is artificial intelligence (ai) in which humans can understand the decisions or predictions made by the ai.133 it contrasts with the black box concept in machine learning where even its designers cannot explain why an ai arrived at a specific decision.134 by refining the mental models of users of aipowered systems and dismantling their misconceptions, xai promises to help users perform more effectively. xai may be an implementation of the social right to explanation. settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalising the theory in accordance with how complex the theory is.135 limitations explainability overfitting 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1740 the blue line could be an example of overfitting a linear function due to random noise. learners can also be disappointed by learning the wrong lesson. a toy example is that an image classifier trained only on pictures of brown horses and black cats might"
  },
  {
    "chunk_id": 280,
    "doc_id": "Machine_learning.txt",
    "text": "learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1740 the blue line could be an example of overfitting a linear function due to random noise. learners can also be disappointed by learning the wrong lesson. a toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.136 a real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. modifying these patterns on a legitimate image can result in adversarial images that the system misclassifies.137138 adversarial vulnerabilities can also result in nonlinear systems or from non-pattern perturbations. for some systems, it is possible to change the output by only changing a single adversarially chosen pixel.139 machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning. 140 researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories spam and not spam of posts) machine learning models that are often developed or trained by third parties. parties can change the classification of any input, including in cases for which a type of datasoftware transparency is provided, possibly including white-box access. 141142143 classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data into a training and test set (conventionally 23 training set and 13 test set designation) and evaluates the performance of the training model on the test set. in comparison, the k-fold-cross-validation method randomly partitions the data into k subsets and then k experiments are performed each considering 1 subset for evaluation and the remaining k-1 subsets for training the model. in addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.144 in addition to overall accuracy, investigators frequently report sensitivity and specificity, meaning true positive rate (tpr) and true negative rate (tnr), respectively. similarly, investigators sometimes report the false positive rate (fpr) as well as the false negative rate (fnr). however, these rates are ratios that fail to reveal their numerators and denominators. receiver operating characteristic (roc), along with the accompanying area under the roc curve (auc), offer additional tools"
  },
  {
    "chunk_id": 281,
    "doc_id": "Machine_learning.txt",
    "text": "rate (tnr), respectively. similarly, investigators sometimes report the false positive rate (fpr) as well as the false negative rate (fnr). however, these rates are ratios that fail to reveal their numerators and denominators. receiver operating characteristic (roc), along with the accompanying area under the roc curve (auc), offer additional tools for classification model assessment. higher auc is associated with a better performing model.145 other limitations and vulnerabilities model assessments 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1840 the ethics of artificial intelligence covers a broad range of topics within ai that are considered to have particular ethical stakes.146 this includes algorithmic biases, fairness, accountability, transparency, privacy, and regulation, particularly where systems influence or automate human decision-making.147 it also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, ai safety and alignment, technological unemployment, ai-enabled misinformation, 148 how to treat certain ai systems if they have a moral status (ai welfare and rights), artificial superintelligence and existential risks. 146 some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military. different machine learning approaches can suffer from different data biases. a machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. when trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.149 systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitising cultural prejudices.150 for example, in 1988, the uks commission for racial equality found that st. georges medical school had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to either be women or have non-european-sounding names.149 using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.151152 another example includes predictive policing company geoliticas predictive algorithm that resulted in disproportionately high levels of over-policing in low-income and minority communities after being trained with historical crime data.153 while responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part"
  },
  {
    "chunk_id": 282,
    "doc_id": "Machine_learning.txt",
    "text": "successful applicants.151152 another example includes predictive policing company geoliticas predictive algorithm that resulted in disproportionately high levels of over-policing in low-income and minority communities after being trained with historical crime data.153 while responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame the lack of participation and representation of minority populations in the field of ai for machine learnings vulnerability to biases.154 in fact, according to research carried out by the computing research association in 2021, female faculty make up just 16.1 of all faculty members who focus on ai among several universities around the world.155 furthermore, among the group of new u.s. resident ai phd graduates, 45 identified as white, 22.4 as asian, 3.2 as hispanic, and 2.4 as african american, which further demonstrates a lack of diversity in the field of ai.155 language models learned from data have been shown to contain human-like biases.156157 because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.158159 in 2016, microsoft tested tay, a chatbot that learned from twitter, and it quickly picked up racist and sexist language.160 ethics bias 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 1940 in an experiment carried out by propublica, an investigative journalism organisation, a machine learning algorithms insight into the recidivism rates among prisoners falsely flagged black defendants high risk twice as often as white defendants.153 in 2015, google photos once tagged a couple of black people as gorillas, which caused controversy. the gorilla label was subsequently removed, and in 2023, it still cannot recognise gorillas.161 similar issues with recognising nonwhite people have been found in many other systems.162 because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.163 concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including fei-fei li, who said that theres nothing artificial about ai. its inspired by people, its created by people, andmost importantlyit impacts people. it is a powerful tool we are only just beginning to understand, and that is a profound responsibility.164 there are concerns among health care professionals that these systems might not be designed in the publics interest but as income-generating machines. this is especially true in the united states,"
  },
  {
    "chunk_id": 283,
    "doc_id": "Machine_learning.txt",
    "text": "impacts people. it is a powerful tool we are only just beginning to understand, and that is a profound responsibility.164 there are concerns among health care professionals that these systems might not be designed in the publics interest but as income-generating machines. this is especially true in the united states, where there is a long-standing ethical dilemma of improving health care, but also increasing profits. for example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithms proprietary owners hold stakes. there is potential for machine learning in health care to provide professionals with an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.165 since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.166 by 2019, graphics processing units (gpus), often with ai-specific enhancements, had displaced cpus as the dominant method of training large-scale commercial cloud ai.167 openai estimated the hardware compute used in the largest deep learning projects from alexnet (2012) to alphazero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.168169 tensor processing units (tpus) are specialised hardware accelerators developed by google specifically for machine learning workloads. unlike general-purpose gpus and fpgas, tpus are optimised for tensor computations, making them particularly efficient for deep learning tasks such as training and inference. they are widely used in google cloud ai services and large-scale machine learning models like googles deepmind alphafold and large language models. tpus leverage matrix multiplication units and high-bandwidth memory to accelerate computations while maintaining energy efficiency.170 since their introduction in 2016, tpus have become a key component of ai infrastructure, especially in cloud-based environments. financial incentives hardware tensor processing units (tpus) 28122025 23:11 machine learning - wikipedia https:en.wikipedia.orgwikimachinelearning 2040 neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. these systems may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.171 a physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. the term physical neural network highlights the use of"
  },
  {
    "chunk_id": 284,
    "doc_id": "Machine_learning.txt",
    "text": "may be implemented through software-based simulations on conventional hardware or through specialised hardware architectures.171 a physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. the term physical neural network highlights the use of physical hardware for computation, as opposed to software-based implementations. it broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.172173 embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers. 174175176177 running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. embedded machine learning can be achieved through various techniques, such as hardware acceleration, 178179 approximate computing, 180 and model optimisation.181182 common optimisation techniques include pruning, quantisation, knowledge distillation, low-rank factorisation, network architecture search, and parameter sharing. software suites containing a variety of machine learning algorithms include the following: caffe deeplearning4j deepspeed elki google jax infer.net jasp jubatus keras kubeflow lightgbm mahout mallet microsoft cognitive toolkit ml.net mlpack mxnet opennn orange pandas (software) root (tmva with root) scikit-learn shogun spark mllib systemml theano tensorflow torch pytorch weka moa xgboost neuromorphic computing physical neural networks embedded machine learning software"
  },
  {
    "chunk_id": 285,
    "doc_id": "curved_spacetime.txt",
    "text": "in physics, curved spacetime is the mathematical model in which, with einsteins theory of general relativity, gravity naturally arises, as opposed to being described as a fundamental force in newtons static euclidean reference frame. objects move along geodesicscurved paths determined by the local geometry of spacetimerather than being influenced directly by distant bodies. this framework led to two fundamental principles: coordinate independence, which asserts that the laws of physics are the same regardless of the coordinate system used, and the equivalence principle, which states that the effects of gravity are indistinguishable from those of acceleration in sufficiently small regions of space. these principles laid the groundwork for a deeper understanding of gravity through the geometry of spacetime, as formalized in einsteins field equations. introduction newtons theories assumed that motion takes place against the backdrop of a rigid euclidean reference frame that extends throughout all space and all time. gravity is mediated by a mysterious force, acting instantaneously across a distance, whose actions are independent of the intervening space.note 1 in contrast, einstein denied that there is any background euclidean reference frame that extends throughout space. nor is there any such thing as a force of gravitation, only the structure of spacetime itself.1: 175190 figure 51. tidal effects. in spacetime terms, the path of a satellite orbiting the earth is not dictated by the distant influences of the earth, moon and sun. instead, the satellite moves through space only in response to local conditions. since spacetime is everywhere locally flat when considered on a sufficiently small scale, the satellite is always following a straight line in its local inertial frame. we say that the satellite always follows along the path of a geodesic. no evidence of gravitation can be discovered following alongside the motions of a single particle.1: 175190 in any analysis of spacetime, evidence of gravitation requires that one observe the relative accelerations of two bodies or two separated particles. in fig. 5-1, two separated particles, free-falling in the gravitational field of the earth, exhibit tidal accelerations due to local inhomogeneities in the gravitational field such that each particle follows a different path through spacetime. the tidal accelerations that these particles exhibit with respect to each other do not require forces for their explanation. rather, einstein described them in terms of the geometry of spacetime, i.e. the curvature of spacetime. these tidal accelerations are strictly local. it is the cumulative"
  },
  {
    "chunk_id": 286,
    "doc_id": "curved_spacetime.txt",
    "text": "a different path through spacetime. the tidal accelerations that these particles exhibit with respect to each other do not require forces for their explanation. rather, einstein described them in terms of the geometry of spacetime, i.e. the curvature of spacetime. these tidal accelerations are strictly local. it is the cumulative total effect of many local manifestations of curvature that result in the appearance of a gravitational force acting at a long range from earth.1: 175190 different observers viewing the scenarios presented in this figure interpret the scenarios differently depending on their knowledge of the situation. (i) a first observer, at the center of mass of particles 2 and 3 but unaware of the large mass 1, concludes that a force of repulsion exists between the particles in scenario a while a force of attraction exists between the particles in scenario b. (ii) a second observer, aware of the large mass 1, smiles at the first reporters naivet. this second observer knows that in reality, the apparent forces between particles 2 and 3 really represent tidal effects resulting from their differential attraction by mass 1. (iii) a third observer, trained in general relativity, knows that there are, in fact, no forces at all acting between the three objects. rather, all three objects move along geodesics in spacetime. two central propositions underlie general relativity. the first crucial concept is coordinate independence: the laws of physics cannot depend on what coordinate system one uses. this is a major extension of the principle of relativity from the version used in special relativity, which states that the laws of physics must be the same for every observer moving in non-accelerated (inertial) reference frames. in general relativity, to use einsteins own (translated) words, the laws of physics must be of such a nature that they apply to systems of reference in any kind of motion.2: 113 this leads to an immediate issue: in accelerated frames, one feels forces that seemingly would enable one to assess ones state of acceleration in an absolute sense. einstein resolved this problem through the principle of equivalence.3: 137149 figure 52. equivalence principle the equivalence principle states that in any sufficiently small region of space, the effects of gravitation are the same as those from acceleration. in fig. 5-2, person a is in a spaceship, far from any massive objects, that undergoes a uniform acceleration of g. person b is in a"
  },
  {
    "chunk_id": 287,
    "doc_id": "curved_spacetime.txt",
    "text": "principle the equivalence principle states that in any sufficiently small region of space, the effects of gravitation are the same as those from acceleration. in fig. 5-2, person a is in a spaceship, far from any massive objects, that undergoes a uniform acceleration of g. person b is in a box resting on earth. provided that the spaceship is sufficiently small so that tidal effects are non-measurable (given the sensitivity of current gravity measurement instrumentation, a and b presumably should be lilliputians), there are no experiments that a and b can perform which will enable them to tell which setting they are in.3: 141149 an alternative expression of the equivalence principle is to note that in newtons universal law of gravitation, f gmmgr2 mgg and in newtons second law, f mia, there is no a priori reason why the gravitational mass mg should be equal to the inertial mass mi. the equivalence principle states that these two masses are identical.3: 141149 to go from the elementary description above of curved spacetime to a complete description of gravitation requires tensor calculus and differential geometry, topics both requiring considerable study. without these mathematical tools, it is possible to write about general relativity, but it is not possible to demonstrate any non-trivial derivations. gravitational time dilation main article: gravitational time dilation figure 53. einsteins argument suggesting gravitational redshift in the discussion of special relativity, forces played no more than a background role. special relativity assumes the ability to define inertial frames that fill all of spacetime, all of whose clocks run at the same rate as the clock at the origin. is this really possible? in a nonuniform gravitational field, experiment dictates that the answer is no. gravitational fields make it impossible to construct a global inertial frame. in small enough regions of spacetime, local inertial frames are still possible. general relativity involves the systematic stitching together of these local frames into a more general picture of spacetime.4: 118126 years before publication of the general theory in 1916, einstein used the equivalence principle to predict the existence of gravitational redshift in the following thought experiment: (i) assume that a tower of height h (fig. 5-3) has been constructed. (ii) drop a particle of rest mass m from the top of the tower. it falls freely with acceleration g, reaching the ground with velocity v (2gh)12, so that its total energy e, as measured"
  },
  {
    "chunk_id": 288,
    "doc_id": "curved_spacetime.txt",
    "text": "thought experiment: (i) assume that a tower of height h (fig. 5-3) has been constructed. (ii) drop a particle of rest mass m from the top of the tower. it falls freely with acceleration g, reaching the ground with velocity v (2gh)12, so that its total energy e, as measured by an observer on the ground, is displaystyle mtfrac 12mv2c2mmghc2 (iii) a mass-energy converter transforms the total energy of the particle into a single high energy photon, which it directs upward. (iv) at the top of the tower, an energy-mass converter transforms the energy of the photon e back into a particle of rest mass m.4: 118126 it must be that m m, since otherwise one would be able to construct a perpetual motion device. we therefore predict that e m, so that displaystyle frac eefrac hnu ,hnu frac mmfrac mghc21-frac ghc2 a photon climbing in earths gravitational field loses energy and is redshifted. early attempts to measure this redshift through astronomical observations were somewhat inconclusive, but definitive laboratory observations were performed by pound rebka (1959) and later by pound snider (1964).5 light has an associated frequency, and this frequency may be used to drive the workings of a clock. the gravitational redshift leads to an important conclusion about time itself: gravity makes time run slower. suppose we build two identical clocks whose rates are controlled by some stable atomic transition. place one clock on top of the tower, while the other clock remains on the ground. an experimenter on top of the tower observes that signals from the ground clock are lower in frequency than those of the clock next to her on the tower. light going up the tower is just a wave, and it is impossible for wave crests to disappear on the way up. exactly as many oscillations of light arrive at the top of the tower as were emitted at the bottom. the experimenter concludes that the ground clock is running slow, and can confirm this by bringing the tower clock down to compare side by side with the ground clock.6: 1618 for a 1 km tower, the discrepancy would amount to about 9.4 nanoseconds per day, easily measurable with modern instrumentation. clocks in a gravitational field do not all run at the same rate. experiments such as the poundrebka experiment have firmly established the distortion of time component of spacetime. the poundrebka experiment says"
  },
  {
    "chunk_id": 289,
    "doc_id": "curved_spacetime.txt",
    "text": "km tower, the discrepancy would amount to about 9.4 nanoseconds per day, easily measurable with modern instrumentation. clocks in a gravitational field do not all run at the same rate. experiments such as the poundrebka experiment have firmly established the distortion of time component of spacetime. the poundrebka experiment says nothing about curvature of the space component of spacetime. but the theoretical arguments predicting gravitational time dilation do not depend on the details of general relativity at all. any theory of gravity will predict gravitational time dilation if it respects the principle of equivalence.6: 16 this includes newtonian gravitation. a standard demonstration in general relativity is to show how, in the newtonian limit (i.e. the particles are moving slowly, the gravitational field is weak, and the field is static), time component of the christoffel symbols describing the geometry of spacetime alone is sufficient to derive newtons law of gravity.7: 101106 newtonian gravitation is a theory of distorted time. general relativity is a theory of distorted spacetime. given g as the gravitational constant, m as the mass of a newtonian star, and orbiting bodies of insignificant mass at distance r from the star, the spacetime interval for newtonian gravitation is one for which only the time coefficient is variable:6: 229232 displaystyle delta s2left(1-frac 2gmc2rright)(cdelta t)2-,(delta x)2-(delta y)2-(delta z)2 distortion of space the displaystyle (1-2gm(c2r)) coefficient in front of displaystyle (cdelta t)2 describes the distortion of time in newtonian gravitation, and this distortion completely accounts for all newtonian gravitational effects. as expected, this correction factor is directly proportional to displaystyle g and displaystyle m, and because of the displaystyle r in the denominator, the correction factor increases as one approaches the gravitating body, meaning that time is distorted. but general relativity is a theory of distorted space and distorted time, so if there are terms modifying the spatial components of the spacetime interval presented above, should not their effects be seen on, say, planetary and satellite orbits due to distortion correction factors applied to the spatial terms? the answer is that they are seen, but the effects are tiny. the reason is that planetary velocities are extremely small compared to the speed of light, so that for planets and satellites of the solar system, the displaystyle (cdelta t)2 term dwarfs the spatial terms.6: 234238 despite the minuteness of the spatial terms, the first indications that something was wrong with newtonian gravitation were"
  },
  {
    "chunk_id": 290,
    "doc_id": "curved_spacetime.txt",
    "text": "that planetary velocities are extremely small compared to the speed of light, so that for planets and satellites of the solar system, the displaystyle (cdelta t)2 term dwarfs the spatial terms.6: 234238 despite the minuteness of the spatial terms, the first indications that something was wrong with newtonian gravitation were discovered over a century-and-a-half ago. in 1859, urbain le verrier, in an analysis of available timed observations of transits of mercury over the suns disk from 1697 to 1848, reported that known physics could not explain the orbit of mercury, unless there possibly existed a planet or asteroid belt within the orbit of mercury. the perihelion of mercurys orbit exhibited an excess rate of precession over that which could be explained by the tugs of the other planets.8 the ability to detect and accurately measure the minute value of this anomalous precession (only 43 arc seconds per tropical century) is testimony to the sophistication of 19th century astrometry. figure 54. general relativity is a theory of distorted time and distorted space. click here to animate. as the astronomer who had earlier discovered the existence of neptune at the tip of his pen by analyzing irregularities in the orbit of uranus, le verriers announcement triggered a two-decades long period of vulcan-mania, as professional and amateur astronomers alike hunted for the hypothetical new planet. this search included several false sightings of vulcan. it was ultimately established that no such planet or asteroid belt existed.9 in 1916, einstein was to show that this anomalous precession of mercury is explained by the spatial terms in the distortion of spacetime. distortion in the temporal term, being simply an expression of newtonian gravitation, has no part in explaining this anomalous precession. the success of his calculation was a powerful indication to einsteins peers that the general theory of relativity could be correct. the most spectacular of einsteins predictions was his calculation that the distortion terms in the spatial components of the spacetime interval could be measured in the bending of light around a massive body. light has a slope of 1 on a spacetime diagram. its movement in space is equal to its movement in time. for the weak field expression of the invariant interval, einstein calculated an exactly equal but opposite sign distortion in its spatial components.6: 234238 displaystyle delta s2left(1-frac 2gmc2rright)(cdelta t)2displaystyle -,left(1frac 2gmc2rright)left(delta x)2(delta y)2(delta z)2right in newtons gravitation, the displaystyle (1-2gm(c2r)) coefficient"
  },
  {
    "chunk_id": 291,
    "doc_id": "curved_spacetime.txt",
    "text": "movement in space is equal to its movement in time. for the weak field expression of the invariant interval, einstein calculated an exactly equal but opposite sign distortion in its spatial components.6: 234238 displaystyle delta s2left(1-frac 2gmc2rright)(cdelta t)2displaystyle -,left(1frac 2gmc2rright)left(delta x)2(delta y)2(delta z)2right in newtons gravitation, the displaystyle (1-2gm(c2r)) coefficient in front of displaystyle (cdelta t)2 predicts bending of light around a star. in general relativity, the displaystyle (12gm(c2r)) coefficient in front of displaystyle left(delta x)2(delta y)2(delta z)2right predicts a doubling of the total bending.6: 234238 the story of the 1919 eddington eclipse expedition and einsteins rise to fame is well told elsewhere.10 sources of spacetime curvature figure 5-5. contravariant components of the stressenergy tensor in newtons theory of gravitation, the only source of gravitational force is mass. in contrast, general relativity identifies several sources of spacetime curvature in addition to mass. in the einstein field equations, the sources of gravity are presented on the right-hand side in displaystyle tmu nu , the stressenergy tensor.11 fig. 5-5 classifies the various sources of gravity in the stressenergy tensor: displaystyle t00 (red): the total massenergy density, including any contributions to the potential energy from forces between the particles, as well as kinetic energy from random thermal motions. displaystyle t0i and displaystyle ti0 (orange): these are momentum density terms. even if there is no bulk motion, energy may be transmitted by heat conduction, and the conducted energy will carry momentum. displaystyle tij are the rates of flow of the i-component of momentum per unit area in the j-direction. even if there is no bulk motion, random thermal motions of the particles will give rise to momentum flow, so the i j terms (green) represent isotropic pressure, and the i j terms (blue) represent shear stresses.11 one important conclusion to be derived from the equations is that, colloquially speaking, gravity itself creates gravity.note 2 energy has mass. even in newtonian gravity, the gravitational field is associated with an energy, displaystyle emgh, called the gravitational potential energy. in general relativity, the energy of the gravitational field feeds back into creation of the gravitational field. this makes the equations nonlinear and hard to solve in anything other than weak field cases.6: 240 numerical relativity is a branch of general relativity using numerical methods to solve and analyze problems, often employing supercomputers to study black holes, gravitational waves, neutron stars and other phenomena in the strong field"
  },
  {
    "chunk_id": 292,
    "doc_id": "curved_spacetime.txt",
    "text": "makes the equations nonlinear and hard to solve in anything other than weak field cases.6: 240 numerical relativity is a branch of general relativity using numerical methods to solve and analyze problems, often employing supercomputers to study black holes, gravitational waves, neutron stars and other phenomena in the strong field regime. energy-momentum figure 5-6. (left) mass-energy warps spacetime. (right) rotating massenergy distributions with angular momentum j generate gravitomagnetic fields h. in special relativity, mass-energy is closely connected to momentum. just as space and time are different aspects of a more comprehensive entity called spacetime, massenergy and momentum are merely different aspects of a unified, four-dimensional quantity called four-momentum. in consequence, if massenergy is a source of gravity, momentum must also be a source. the inclusion of momentum as a source of gravity leads to the prediction that moving or rotating masses can generate fields analogous to the magnetic fields generated by moving charges, a phenomenon known as gravitomagnetism.12 figure 57. origin of gravitomagnetism it is well known that the force of magnetism can be deduced by applying the rules of special relativity to moving charges. (an eloquent demonstration of this was presented by feynman in volume ii, chapter 136 of his lectures on physics, available online.)13 analogous logic can be used to demonstrate the origin of gravitomagnetism.6: 245253 in fig. 5-7a, two parallel, infinitely long streams of massive particles have equal and opposite velocities v and v relative to a test particle at rest and centered between the two. because of the symmetry of the setup, the net force on the central particle is zero. assume displaystyle vll c so that velocities are simply additive. fig. 5-7b shows exactly the same setup, but in the frame of the upper stream. the test particle has a velocity of v, and the bottom stream has a velocity of 2v. since the physical situation has not changed, only the frame in which things are observed, the test particle should not be attracted towards either stream.6: 245253 it is not at all clear that the forces exerted on the test particle are equal. (1) since the bottom stream is moving faster than the top, each particle in the bottom stream has a larger mass energy than a particle in the top. (2) because of lorentz contraction, there are more particles per unit length in the bottom stream than in the top stream. (3) another"
  },
  {
    "chunk_id": 293,
    "doc_id": "curved_spacetime.txt",
    "text": "since the bottom stream is moving faster than the top, each particle in the bottom stream has a larger mass energy than a particle in the top. (2) because of lorentz contraction, there are more particles per unit length in the bottom stream than in the top stream. (3) another contribution to the active gravitational mass of the bottom stream comes from an additional pressure term which, at this point, we do not have sufficient background to discuss. all of these effects together would seemingly demand that the test particle be drawn towards the bottom stream.6: 245253 the test particle is not drawn to the bottom stream because of a velocity-dependent force that serves to repel a particle that is moving in the same direction as the bottom stream. this velocity-dependent gravitational effect is gravitomagnetism.6: 245253 matter in motion through a gravitomagnetic field is hence subject to so-called frame-dragging effects analogous to electromagnetic induction. it has been proposed that such gravitomagnetic forces underlie the generation of the relativistic jets (fig. 5-8) ejected by some rotating supermassive black holes.1415 pressure and stress quantities that are directly related to energy and momentum should be sources of gravity as well, namely internal pressure and stress. taken together, mass-energy, momentum, pressure and stress all serve as sources of gravity: collectively, they are what tells spacetime how to curve. general relativity predicts that pressure acts as a gravitational source with exactly the same strength as massenergy density. the inclusion of pressure as a source of gravity leads to dramatic differences between the predictions of general relativity versus those of newtonian gravitation. for example, the pressure term sets a maximum limit to the mass of a neutron star. the more massive a neutron star, the more pressure is required to support its weight against gravity. the increased pressure, however, adds to the gravity acting on the stars mass. above a certain mass determined by the tolmanoppenheimervolkoff limit, the process becomes runaway and the neutron star collapses to a black hole.6: 243, 280 the stress terms become highly significant when performing calculations such as hydrodynamic simulations of core-collapse supernovae.16 these predictions for the roles of pressure, momentum and stress as sources of spacetime curvature are elegant and play an important role in theory. in regards to pressure, the early universe was radiation dominated,17 and it is highly unlikely that any of the relevant cosmological data (e.g. nucleosynthesis"
  },
  {
    "chunk_id": 294,
    "doc_id": "curved_spacetime.txt",
    "text": "core-collapse supernovae.16 these predictions for the roles of pressure, momentum and stress as sources of spacetime curvature are elegant and play an important role in theory. in regards to pressure, the early universe was radiation dominated,17 and it is highly unlikely that any of the relevant cosmological data (e.g. nucleosynthesis abundances, etc.) could be reproduced if pressure did not contribute to gravity, or if it did not have the same strength as a source of gravity as massenergy. likewise, the mathematical consistency of the einstein field equations would be broken if the stress terms did not contribute as a source of gravity. experimental test of the sources of spacetime curvature definitions: active, passive, and inertial mass bondi distinguishes between different possible types of mass: (1) active mass (displaystyle ma) is the mass which acts as the source of a gravitational field; (2)passive mass (displaystyle mp) is the mass which reacts to a gravitational field; (3) inertial mass (displaystyle mi) is the mass which reacts to acceleration.18 displaystyle mp is the same as gravitational mass (displaystyle mg) in the discussion of the equivalence principle. in newtonian theory, the third law of action and reaction dictates that displaystyle ma and displaystyle mp must be the same. on the other hand, whether displaystyle mp and displaystyle mi are equal is an empirical result. in general relativity, the equality of displaystyle mp and displaystyle mi is dictated by the equivalence principle. there is no action and reaction principle dictating any necessary relationship between displaystyle ma and displaystyle mp.18 pressure as a gravitational source figure 59. (a) cavendish experiment, (b) kreuzer experiment the classic experiment to measure the strength of a gravitational source (i.e. its active mass) was first conducted in 1797 by henry cavendish (fig. 5-9a). two small but dense balls are suspended on a fine wire, making a torsion balance. bringing two large test masses close to the balls introduces a detectable torque. given the dimensions of the apparatus and the measurable spring constant of the torsion wire, the gravitational constant g can be determined. to study pressure effects by compressing the test masses is hopeless, because attainable laboratory pressures are insignificant in comparison with the mass-energy of a metal ball. however, the repulsive electromagnetic pressures resulting from protons being tightly squeezed inside atomic nuclei are typically on the order of 1028 atm 1033 pa 1033 kgs2m1. this amounts to about 1 of"
  },
  {
    "chunk_id": 295,
    "doc_id": "curved_spacetime.txt",
    "text": "masses is hopeless, because attainable laboratory pressures are insignificant in comparison with the mass-energy of a metal ball. however, the repulsive electromagnetic pressures resulting from protons being tightly squeezed inside atomic nuclei are typically on the order of 1028 atm 1033 pa 1033 kgs2m1. this amounts to about 1 of the nuclear mass density of approximately 1018kgm3 (after factoring in c2 91016m2s2).19 figure 5-10. lunar laser ranging experiment. (left) this retroreflector was left on the moon by astronauts on the apollo 11 mission. (right) astronomers all over the world have bounced laser light off the retroreflectors left by apollo astronauts and russian lunar rovers to measure precisely the earth-moon distance. if pressure does not act as a gravitational source, then the ratio displaystyle mamp should be lower for nuclei with higher atomic number z, in which the electrostatic pressures are higher. l. b. kreuzer (1968) did a cavendish experiment using a teflon mass suspended in a mixture of the liquids trichloroethylene and dibromoethane having the same buoyant density as the teflon (fig. 5-9b). fluorine has atomic number z 9, while bromine has z 35. kreuzer found that repositioning the teflon mass caused no differential deflection of the torsion bar, hence establishing active mass and passive mass to be equivalent to a precision of 5105.20 although kreuzer originally considered this experiment merely to be a test of the ratio of active mass to passive mass, clifford will (1976) reinterpreted the experiment as a fundamental test of the coupling of sources to gravitational fields.21 in 1986, bartlett and van buren noted that lunar laser ranging had detected a 2 km offset between the moons center of figure and its center of mass. this indicates an asymmetry in the distribution of fe (abundant in the moons core) and al (abundant in its crust and mantle). if pressure did not contribute equally to spacetime curvature as does massenergy, the moon would not be in the orbit predicted by classical mechanics. they used their measurements to tighten the limits on any discrepancies between active and passive mass to about 1012.22 with decades of additional lunar laser ranging data, singh et al. (2023) reported improvement on these limits by a factor of about 100.23 gravitomagnetism figure 511. gravity probe b confirmed the existence of gravitomagnetism the existence of gravitomagnetism was proven by gravity probe b (gp-b), a satellite-based mission which launched on 20 april 2004.24 the"
  },
  {
    "chunk_id": 296,
    "doc_id": "curved_spacetime.txt",
    "text": "laser ranging data, singh et al. (2023) reported improvement on these limits by a factor of about 100.23 gravitomagnetism figure 511. gravity probe b confirmed the existence of gravitomagnetism the existence of gravitomagnetism was proven by gravity probe b (gp-b), a satellite-based mission which launched on 20 april 2004.24 the spaceflight phase lasted until 2005. the mission aim was to measure spacetime curvature near earth, with particular emphasis on gravitomagnetism. initial results confirmed the relatively large geodetic effect (which is due to simple spacetime curvature, and is also known as de sitter precession) to an accuracy of about 1. the much smaller frame-dragging effect (which is due to gravitomagnetism, and is also known as lensethirring precession) was difficult to measure because of unexpected charge effects causing variable drift in the gyroscopes. nevertheless, by august 2008, the frame-dragging effect had been confirmed to within 15 of the expected result,25 while the geodetic effect was confirmed to better than 0.5.2627 subsequent measurements of frame dragging by laser-ranging observations of the lares, lageos-1 and lageos-2 satellites has improved on the gp-b measurement, with results (as of 2016) demonstrating the effect to within 5 of its theoretical value,28 although there has been some disagreement on the accuracy of this result.29 another effort, the gyroscopes in general relativity (ginger) experiment, seeks to use three 6 m ring lasers mounted at right angles to each other 1400 m below the earths surface to measure this effect.3031 the first ten years of experience with a prototype ring laser gyroscope array, gingerino, established that the full scale experiment should be able to measure gravitomagnetism due to the earths rotation to within a 0.1 level or even better.32"
  },
  {
    "chunk_id": 297,
    "doc_id": "Large_language_model.txt",
    "text": "large language model a large language model (llm) is a language model trained with self-supervised machine learning on a vast amount of text, designed for natural language processing tasks, especially language generation. 12 the largest and most capable llms are generative pre-trained transformers (gpts) and provide the core capabilities of modern chatbots. llms can be fine-tuned for specific tasks or guided by prompt engineering. 3 these models acquire predictive power regarding syntax, semantics, and ontologies4 inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained on.5 they consist of billions to trillions of parameters and operate as general-purpose sequence models, generating, summarizing, translating, and reasoning over text. llms represent a significant new technology in their ability to generalize across tasks with minimal task-specific supervision, enabling capabilities like conversational agents, code generation, knowledge retrieval, and automated reasoning that previously required bespoke systems.6 llms evolved from earlier statistical and recurrent neural network approaches to language modeling. the transformer architecture, introduced in 2017, replaced recurrence with selfattention, allowing efficient parallelization, longer context handling, and scalable training on unprecedented data volumes.7 this innovation enabled models like gpt, bert, and their successors, which demonstrated emergent behaviors at scale, such as few-shot learning and compositional reasoning.8 reinforcement learning, particularly policy gradient algorithms, has been adapted to fine-tune llms for desired behaviors beyond raw next-token prediction.9 reinforcement learning from human feedback (rlhf) applies these methods to optimize a policy, the llms output distribution, against reward signals derived from human or automated preference judgments.10 this has been critical for aligning model outputs with user expectations, improving factuality, reducing harmful responses, and enhancing task performance. benchmark evaluations for llms have evolved from narrow linguistic assessments toward comprehensive, multi-task evaluations measuring reasoning, factual accuracy, alignment, and safety. 1112 hill climbing, iteratively optimizing models against benchmarks, has emerged as a dominant strategy, producing rapid incremental performance gains but raising concerns of overfitting to benchmarks rather than achieving genuine generalization or robust capability improvements.13 before the emergence of transformer-based models in 2017, some language models were considered large relative to the computational and data constraints of their time. in the early 1990s, ibms statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. in 2001, a smoothed n-gram model, such as those employing kneserney smoothing, trained on 300 million words, achieved state-ofhistory 28122025 23:02 large language model"
  },
  {
    "chunk_id": 298,
    "doc_id": "Large_language_model.txt",
    "text": "data constraints of their time. in the early 1990s, ibms statistical models pioneered word alignment techniques for machine translation, laying the groundwork for corpus-based language modeling. in 2001, a smoothed n-gram model, such as those employing kneserney smoothing, trained on 300 million words, achieved state-ofhistory 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 140 the number of publications about large language models by year grouped by publication types. the training compute of notable large models in flops vs publication date over the period 20102024. for overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). the majority of these models are language models. the training compute of notable large ai models in flops vs publication date over the period 20172024. the majority of large models are language models or multimodal models with language capacity. the-art perplexity on benchmark tests.14 during the 2000s, with the rise of widespread internet access, researchers began compiling massive text datasets from the web (web as corpus15 ) to train statistical language models.1617 moving beyond n-gram models, researchers started in 2000 to use neural networks to learn language models.18 following the breakthrough of deep neural networks in image classification around 2012,19 similar architectures were adapted for language tasks. this shift was marked by the development of word embeddings (eg, word2vec by mikolov in 2013) and sequence-to-sequence (seq2seq) models using lstm. in 2016, google transitioned its translation service to neural machine translation (nmt), replacing statistical phrase-based models with deep recurrent neural networks. these early nmt systems used lstm-based encoder-decoder architectures, as they preceded the invention of transformers. at the 2017 neurips conference, google researchers introduced the transformer architecture in their landmark paper attention is all you need.20 this papers goal was to improve upon 2014 seq2seq technology,21 and was based mainly on the attention mechanism developed by bahdanau et al. in 2014.22 the following year in 2018, bert was introduced and quickly became ubiquitous.23 though the original transformer has both encoder and decoder blocks, bert is an encoder-only model. academic and research usage of bert began to decline in 2023, following rapid improvements in the abilities of decoderonly models (such as gpt) to solve tasks via prompting. 24 although decoder-only gpt-1 was introduced in 2018, it was gpt-2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to"
  },
  {
    "chunk_id": 299,
    "doc_id": "Large_language_model.txt",
    "text": "began to decline in 2023, following rapid improvements in the abilities of decoderonly models (such as gpt) to solve tasks via prompting. 24 although decoder-only gpt-1 was introduced in 2018, it was gpt-2 in 2019 that caught widespread attention because openai claimed to have initially deemed it too powerful to release publicly, out of fear of malicious use.25 gpt-3 in 2020 went a step further and as of 2025 is available only via api with no offering of downloading the model to execute locally. but it was the 2022 consumer-facing chatbot chatgpt that received extensive media coverage and public attention.26 the 2023 gpt-4 was praised for its increased accuracy and as a holy grail for its multimodal capabilities.27 openai did not reveal the high-level architecture and the number of parameters of gpt-4. the release of chatgpt led to an uptick in llm usage across several research subfields of computer science, including robotics, software engineering, and societal impact work.24 in 2024 openai 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 240 an illustration of the main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention released the reasoning model openai o1, which generates long chains of thought before returning a final answer.28 many llms with parameter counts comparable to those of openais gpt series have been developed.29 since 2022, open-weight models have been gaining popularity, especially at first with bloom and llama, though both have restrictions on usage and deployment. mistral ais models mistral 7b and mixtral 8x7b have a more permissive apache license. in january 2025, deepseek released deepseek r1, a 671- billion-parameter open-weight model that performs comparably to openai o1 but at a much lower price per token for users.30 since 2023, many llms have been trained to be multimodal, having the ability to also process or generate other types of data, such as images, audio, or 3d meshes.31 these llms are also called large multimodal models (lmms),32 or multimodal large language models (mllms).3334 as of 2024, the largest and most capable models are all based on the transformer architecture. some recent implementations are based on other architectures, such as recurrent neural network variants and mamba (a state space model).353637 open-weight llms have increasingly shaped the field since 2023, contributing to broader participation in ai development and greater transparency in model evaluation. vake et al. (2025) demonstrated that community-driven"
  },
  {
    "chunk_id": 300,
    "doc_id": "Large_language_model.txt",
    "text": "architecture. some recent implementations are based on other architectures, such as recurrent neural network variants and mamba (a state space model).353637 open-weight llms have increasingly shaped the field since 2023, contributing to broader participation in ai development and greater transparency in model evaluation. vake et al. (2025) demonstrated that community-driven contributions to open-weight models measurably improve their efficiency and performance, with user participation growing rapidly on collaborative platforms such as hugging face.38 paris et al. (2025) further argued that openness in ai should extend beyond releasing model code or weights to encompass inclusiveness, accountability, and ethical responsibility in ai research and deployment.39 collectively, these studies highlight that open-weight llms can accelerate innovation and enhance scientific reproducibility, while fostering a more transparent and participatory ai ecosystem. as machine learning algorithms process numbers rather than text, the text must be converted to numbers. in the first step, a vocabulary is decided upon, then integer indices are arbitrarily but uniquely assigned to each vocabulary entry, and finally, an embedding is associated to the integer index. algorithms include byte-pair encoding (bpe) and wordpiece. there are also special tokens serving as control characters, such as mask for masked-out token (as used in bert), and unk dataset preprocessing tokenization 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 340 (unknown) for characters not appearing in the vocabulary. also, some special symbols are used to denote special text formatting. for example, denotes a preceding whitespace in roberta and gpt and denotes continuation of a preceding word in bert.40 for example, the bpe tokenizer used by the legacy version of gpt-3 would split tokenizer: texts - series of numerical tokens as tokenizer: texts -series of numerical tokens tokenization also compresses the datasets. because llms generally require input to be an array that is not jagged, the shorter texts must be padded until they match the length of the longest one. the average number of words per token depends on the language.4142 in english, the ratio is typically around 0.75 words per token, with 4 characters per token on average.43 as an example, consider a tokenizer based on byte-pair encoding. in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. all occurrences"
  },
  {
    "chunk_id": 301,
    "doc_id": "Large_language_model.txt",
    "text": "in the first step, all unique characters (including blanks and punctuation marks) are treated as an initial set of n-grams (i.e. initial set of uni-grams). successively the most frequent pair of adjacent characters is merged into a bi-gram and all instances of the pair are replaced by it. all occurrences of adjacent pairs of (previously merged) n-grams that most frequently occur together are then again merged into even lengthier n-gram, until a vocabulary of prescribed size is obtained. after a tokenizer is trained, any text can be tokenized by it, as long as it does not contain characters not appearing in the initial-set of uni-grams.44 a token vocabulary based on the frequencies extracted from mainly english corpora uses as few tokens as possible for an average english word. however, an average word in another language encoded by such an english-optimized tokenizer is split into a suboptimal amount of tokens. gpt2 tokenizer can use up to 15 times more tokens per word for some languages, for example for the shan language from myanmar. even more widespread languages such as portuguese and german have a premium of 50 compared to english.42 in the context of training llms, datasets are typically cleaned by removing low-quality, duplicated, or toxic data.45 cleaned datasets can increase training efficiency and lead to improved downstream performance.4647 a trained llm can be used to clean datasets for training a further llm.48 with the increasing proportion of llm-generated content on the web, data cleaning in the future may include filtering out such content. llm-generated content can pose a problem if the content is similar to human text (making filtering difficult) but of lower quality (degrading performance of models trained on it).3 training of largest language models might need more linguistic data than naturally available, or that the naturally occurring data is of insufficient quality. in these cases, synthetic data might be used. microsofts phi series of llms is trained on textbook-like data generated by another llm.49 byte-pair encoding problems dataset cleaning synthetic data 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 440 an llm is a type of foundation model (large x model) trained on language. llms can be trained in different ways. in particular, gpt models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.50 substantial infrastructure is necessary for training the largest models. the tendency towards larger models is visible"
  },
  {
    "chunk_id": 302,
    "doc_id": "Large_language_model.txt",
    "text": "(large x model) trained on language. llms can be trained in different ways. in particular, gpt models are first pretrained to predict the next word on a large amount of data, before being fine-tuned.50 substantial infrastructure is necessary for training the largest models. the tendency towards larger models is visible in the list of large language models. for example, the training of gpt-2 (i.e. a 1.5-billion-parameters model) in 2019 cost 50,000, while training of the palm (i.e. a 540-billionparameters model) in 2022 cost 8 million, and megatron-turing nlg 530b (in 2021) cost around 11 million. the qualifier large in large language model is inherently vague, as there is no definitive threshold for the number of parameters required to qualify as large. gpt-1 of 2018 has 117 million parameters. before being fine-tuned, most llms are next-token predictors. the fine-tuning shapes the llms behavior via techniques like reinforcement learning from human feedback (rlhf)51 or constitutional ai. 52 instruction fine-tuning is a form of supervised learning used to teach llms to follow user instructions. in 2022, openai demonstrated instructgpt, a version of gpt-3 similarly fine-tuned to follow instructions.53 reinforcement learning from human feedback (rlhf) involves training a reward model to predict which text humans prefer. then, the llm can be fine-tuned through reinforcement learning to better satisfy this reward model. since humans typically prefer truthful, helpful and harmless answers, rlhf favors such answers.54 llms are generally based on the transformer architecture, which leverages an attention mechanism that enables the model to process relationships between all elements in a sequence simultaneously, regardless of their distance from each other. training cost fine-tuning architecture 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 540 when each head calculates, according to its own criteria, how much other tokens are relevant for the it token, note that the second attention head, represented by the second column, is focusing most on the first two rows, i.e. the tokens the and animal, while the third column is focusing most on the bottom two rows, i.e. on tired, which has been tokenized into two tokens. 55 in order to find out which tokens are relevant to each other within the scope of the context window, the attention mechanism calculates soft weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own relevance for calculating its own soft weights. for example, the small (i.e. 117m parameter"
  },
  {
    "chunk_id": 303,
    "doc_id": "Large_language_model.txt",
    "text": "tokens are relevant to each other within the scope of the context window, the attention mechanism calculates soft weights for each token, more precisely for its embedding, by using multiple attention heads, each with its own relevance for calculating its own soft weights. for example, the small (i.e. 117m parameter sized) gpt-2 model has had twelve attention heads and a context window of only 1k tokens.56 in its medium version it has 345m parameters and contains 24 layers, each with 12 attention heads. for the training with gradient descent a batch size of 512 was utilized.44 googles gemini 1.5, introduced in february 2024, can have a context window of up to 1 million tokens.57 a model may be pre-trained either to predict how the segment continues, or what is missing in the segment, given a segment from its training dataset.58 it can be either autoregressive (i.e. predicting how the segment continues, as gpts do): for example given a segment i like to eat, the model predicts ice cream, or sushi. masked (i.e. filling in the parts missing from the segment, the way bert59 does it): for example, given a segment i like to cream, the model predicts that eat and ice are missing. models may be trained on auxiliary tasks which test their understanding of the data distribution, such as next sentence prediction (nsp), in which pairs of sentences are presented and the model must predict whether they appear consecutively in the training corpus.59 during training, regularization loss is also used to stabilize training. however regularization loss is usually not used during testing and evaluation. a mixture of experts (moe) is a machine learning architecture in which multiple specialized neural networks (experts) work together, with a gating mechanism that routes each input to the most appropriate expert(s). mixtures of experts can reduce inference costs, as only a fraction of the parameters are used for each input. the approach was introduced in 2017 by google researchers.606162 attention mechanism and context window mixture of experts 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 640 typically, llms are trained with single- or half-precision floating point numbers (float32 and float16). one float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. the largest models typically have more than 100 billion parameters, which places them outside the range of most consumer electronics.63 post-training quantization64 aims to decrease the space requirement"
  },
  {
    "chunk_id": 304,
    "doc_id": "Large_language_model.txt",
    "text": "floating point numbers (float32 and float16). one float16 has 16 bits, or 2 bytes, and so one billion parameters require 2 gigabytes. the largest models typically have more than 100 billion parameters, which places them outside the range of most consumer electronics.63 post-training quantization64 aims to decrease the space requirement by lowering precision of the parameters of a trained model, while preserving most of its performance. quantization can be further classified as static quantization if the quantization parameters are determined beforehand (typically during a calibration phase), and dynamic quantization if the quantization is applied during inference. the simplest form of quantization simply truncates all the parameters to a given number of bits: this is applicable to static as well as dynamic quantization, but loses much precision. dynamic quantization allows for the use of a different quantization codebook per layer, either a lookup table of values or a linear mapping (scaling factor and bias), at the cost of foregoing the possible speed improvements from using lower-precision arithmetic. quantized models are typically seen as frozen with modification of weights (e.g. fine-tuning) only applied to the original model. it is possible to fine-tune quantized models using low-rank adaptation. 65 beyond basic text generation, various techniques have been developed to extend llm capabilities, including the use of external tools and data sources, improved reasoning on complex problems, and enhanced instruction-following or autonomy through prompting methods. in 2020, openai researchers demonstrated that their new model gpt-3 could understand what format to use given a few rounds of q and a (or other type of task) in the input data as example, thanks in part due to the rlhf technique. this technique, called few-shot prompting, allows llms to be adapted to any task without requiring fine-tuning.3 also in 2022, it was found that the base gpt-3 model can generate an instruction based on user input. the generated instruction along with user input is then used as input to another instance of the model under a instruction: ..., input: ..., output: format. the other instance is able to complete the output and often produces the correct answer in doing so. the ability to self-instruct makes llms able to bootstrap themselves toward a correct answer.66 an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as q: or user: and the llm is asked to predict the"
  },
  {
    "chunk_id": 305,
    "doc_id": "Large_language_model.txt",
    "text": "in doing so. the ability to self-instruct makes llms able to bootstrap themselves toward a correct answer.66 an llm can be turned into a chatbot by specializing it for conversation. user input is prefixed with a marker such as q: or user: and the llm is asked to predict the output after a fixed a: or assistant:. this type of model became commercially available in 2022 with chatgpt, a sibling model of instructgpt fine-tuned to accept and produce dialog-formatted text based on gpt-3.5. it parameter size quantization extensibility prompt engineering dialogue processing (chatbot) 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 740 could similarly follow user instructions. before the stream of user and assistant lines, a chat context usually start with a few lines of overarching instructions, from a role called developer or system to convey a higher authority than the users input. this is called a system prompt. retrieval-augmented generation (rag) is an approach that integrates llms with document retrieval systems. given a query, a document retriever is called to retrieve the most relevant documents. this is usually done by encoding the query and the documents into vectors, then finding the documents with vectors (usually stored in a vector database) most similar to the vector of the query. the llm then generates an output based on both the query and context included from the retrieved documents.67 tool use is a mechanism that enables llms to interact with external systems, applications, or data sources. it can allow for example to fetch real-time information from an api or to execute code. a program separate from the llm watches the output stream of the llm for a special tool-calling syntax. when these special tokens appear, the program calls the tool accordingly and feeds its output back into the llms input stream.68 early tool-using llms were fine-tuned on the use of specific tools. but fine-tuning llms for the ability to read api documentation and call api correctly has greatly expanded the range of tools accessible to an llm.6970 describing available tools in the system prompt can also make an llm able to use tools. a system prompt instructing chatgpt (gpt-4) to use multiple types of tools can be found online.71 an llm is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions. but it can be transformed into"
  },
  {
    "chunk_id": 306,
    "doc_id": "Large_language_model.txt",
    "text": "tools. a system prompt instructing chatgpt (gpt-4) to use multiple types of tools can be found online.71 an llm is typically not an autonomous agent by itself, as it lacks the ability to interact with dynamic environments, recall past behaviors, and plan future actions. but it can be transformed into an agent by adding supporting elements: the role (profile) and the surrounding environment of an agent can be additional inputs to the llm, while memory can be integrated as a tool or provided as additional input. instructions and input patterns are used to make the llm plan actions and tool use is used to potentially carry out these actions.72 the react pattern, a portmanteau of reason and act, constructs an agent out of an llm, using the llm as a planner. the llm is prompted to think out loud. specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. it generates one or more thoughts before generating an action, which is then executed in the environment.73 in the deps (describe, explain, plan and select) method, an llm is first connected to the visual world via image descriptions. it is then prompted to produce plans for complex tasks and behaviors based on its pretrained knowledge and the environmental feedback it receives.74 the reflexion method constructs an agent that learns over multiple episodes. at the end of each episode, the llm is given the record of the episode, and prompted to think up lessons learned, which would help it perform better at a subsequent episode. these lessons learned are stored as a form of long-term memory and given to the agent in the subsequent episodes.75 retrieval-augmented generation tool use agency 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 840 monte carlo tree search can use an llm as rollout heuristic. when a programmatic world model is not available, an llm can also be prompted with a description of the environment to act as world model.76 for open-ended exploration, an llm can be used to score observations for their interestingness, which can be used as a reward signal to guide a normal (non-llm) reinforcement learning agent.77 alternatively, it can propose increasingly difficult tasks for curriculum learning. 78 instead of outputting individual actions, an llm planner can also construct skills, or functions for complex"
  },
  {
    "chunk_id": 307,
    "doc_id": "Large_language_model.txt",
    "text": "used to score observations for their interestingness, which can be used as a reward signal to guide a normal (non-llm) reinforcement learning agent.77 alternatively, it can propose increasingly difficult tasks for curriculum learning. 78 instead of outputting individual actions, an llm planner can also construct skills, or functions for complex action sequences. the skills can be stored and later invoked, allowing increasing levels of abstraction in planning.78 multiple agents with memory can interact socially.79 llms are conventionally trained to generate an output without generating intermediate steps. as a result, their performance tends to be subpar on complex questions requiring (at least in humans) intermediate steps of thought. early research demonstrated that inserting intermediate scratchpad computations could improve performance on such tasks.80 later methods overcame this deficiency more systematically by breaking tasks into smaller steps for the llm, either manually or automatically. prompt chaining was introduced in 2022.81 in this method, a user manually breaks a complex problem down into several steps. in each step, the llm receives as input a prompt telling it what to do and some results from preceding steps. the result from one step is then reused in a next step, until a final answer is reached. the ability of an llm to follow instructions means that even nonexperts can write a successful collection of stepwise prompts given a few rounds of trial and error.8283 a 2022 paper demonstrated a separate technique called chain-of-thought prompting, which makes the llm break the question down autonomously. an llm is given some examples where the assistant verbally breaks down the thought process before arriving at an answer. the llm mimics these examples and also tries to spend some time generating intermediate steps before providing the final answer. this additional step elicited by prompting improves the correctness of the llm on relatively complex questions. on math word questions, a prompted model can exceed even fine-tuned gpt-3 with a verifier.8485 chain-of-thought can also be elicited by simply adding an instruction like lets think step by step to the prompt, in order to encourage the llm to proceed methodically instead of trying to directly guess the answer.86 in late 2024, a new approach to llm development emerged with reasoning models.87 these are trained to generate step-by-step analysis before producing final answers, enabling better results on complex tasks, for instance in mathematics, coding and logic.88 openai introduced this concept with their o1 model in"
  },
  {
    "chunk_id": 308,
    "doc_id": "Large_language_model.txt",
    "text": "directly guess the answer.86 in late 2024, a new approach to llm development emerged with reasoning models.87 these are trained to generate step-by-step analysis before producing final answers, enabling better results on complex tasks, for instance in mathematics, coding and logic.88 openai introduced this concept with their o1 model in september 2024, followed by o3 in april 2025. on the international mathematics olympiad qualifying exam problems, gpt-4o achieved 13 accuracy while o1 reached 83.89 reasoning chaining model-native reasoning 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 940 in january 2025, the chinese company deepseek released deepseek-r1, a 671-billion-parameter open-weight reasoning model that achieved comparable performance to openais o1 while being significantly more cost-effective to operate. unlike proprietary models from openai, deepseekr1s open-weight nature allowed researchers to study and build upon the algorithm, though its training data remained private.90 these reasoning models typically require more computational resources per query compared to traditional llms, as they perform more extensive processing to work through problems step-bystep.89 inference optimization refers to techniques that improve llm performance by applying additional computational resources during the inference process, rather than requiring model retraining. these approaches implement various state-of-the-art reasoning and decision-making strategies to enhance accuracy and capabilities. optillm is an openai api-compatible optimizing inference proxy that implements multiple inference optimization techniques simultaneously.91 the system acts as a transparent proxy that can work with any llm provider, implementing techniques such as monte carlo tree search (mcts), mixture of agents (moa), best-of-n sampling, and chain-of-thought reflection. optillm demonstrates that strategic application of computational resources at inference time can substantially improve model performance across diverse tasks, achieving significant improvements on benchmarks such as the aime 2024 mathematics competition and various coding challenges.92 these inference optimization approaches represent a growing category of tools that enhance existing llms without requiring access to model weights or retraining, making advanced reasoning capabilities more accessible across different model providers and use cases. multimodality means having multiple modalities, where a modality refers to a type of input or output, such as video, image, audio, text, proprioception, etc.93 for example, google palm model was fine-tuned into a multimodal model and applied to robotic control. 94 llama models have also been turned multimodal using the tokenization method, to allow image inputs,95 and video inputs.96 gpt-4o can process and generate text, audio and images.97 such models are sometimes called large multimodal models (lmms).98 a common method to create multimodal"
  },
  {
    "chunk_id": 309,
    "doc_id": "Large_language_model.txt",
    "text": "multimodal model and applied to robotic control. 94 llama models have also been turned multimodal using the tokenization method, to allow image inputs,95 and video inputs.96 gpt-4o can process and generate text, audio and images.97 such models are sometimes called large multimodal models (lmms).98 a common method to create multimodal models out of an llm is to tokenize the output of a trained encoder. concretely, one can construct an llm that can understand images as follows: take a trained llm, and take a trained image encoder . make a small multilayer perceptron , so that for any image , the post-processed vector has the same dimensions as an encoded token. that is an image token. then, one can interleave text tokens and image tokens. the compound model is then fine-tuned on an image-text dataset. this basic construction can be applied with inference optimization forms of input and output multimodality 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1040 more sophistication to improve the model. the image encoder may be frozen to improve stability.99 this type of method, where embeddings from multiple modalities are fused and the predictor is trained on the combined embeddings, is called early fusion. another method, called intermediate fusion, involves each modality being first processed independently to obtain modality-specific representations; then these intermediate representations are fused together.100 in general, cross-attention is used for integrating information from different modalities. as an example, the flamingo model uses cross-attention layers to inject visual information into its pre-trained language model.101 llms can handle programming languages similarly to how they handle natural languages. no special change in token handling is needed as code, like human language, is represented as plain text. llms can generate code based on problems or instructions written in natural language. they can also describe code in natural language or translate it into other programming languages. they were originally used as a code completion tool, but advances have moved them towards automatic programming. services such as github copilot offer llms specifically trained, fine-tuned, or prompted for programming.102103 in computational biology, transformer-base architectures, such as dna llms, have also proven useful in analyzing biological sequences: protein, dna, and rna. with proteins they appear able to capture a degree of grammar from the amino-acid sequence, by mapping that sequence into an embedding. on tasks such as structure prediction and mutational outcome prediction, a small model using an embedding as input can"
  },
  {
    "chunk_id": 310,
    "doc_id": "Large_language_model.txt",
    "text": "useful in analyzing biological sequences: protein, dna, and rna. with proteins they appear able to capture a degree of grammar from the amino-acid sequence, by mapping that sequence into an embedding. on tasks such as structure prediction and mutational outcome prediction, a small model using an embedding as input can approach or exceed much larger models using multiple sequence alignments (msa) as input.104 esmfold, meta platforms embedding-based method for protein structure prediction, runs an order of magnitude faster than alphafold2 thanks to the removal of an msa requirement and a lower parameter count due to the use of embeddings.105 meta hosts esm atlas, a database of 772 million structures of metagenomic proteins predicted using esmfold.106 an llm can also design proteins unlike any seen in nature.107 nucleic acid models have proven useful in detecting regulatory sequences, 108 sequence classification, rnarna interaction prediction, and rna structure prediction.109 the performance of an llm after pretraining largely depends on the: : cost of pretraining (the total amount of compute used), : size of the artificial neural network itself, such as number of parameters (i.e. amount of neurons in its layers, amount of weights between them and biases), : size of its pretraining dataset (i.e. number of tokens in corpus). scaling laws are empirical statistical laws that predict llm performance based on such factors. one particular scaling law (chinchilla scaling) for llm autoregressively trained for one epoch, with a log-log learning rate schedule, states that:110 non-natural languages properties scaling laws 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1140 at point(s) referred to as breaks, 111 the lines change their slopes, appearing on a linear-log plot as a series of linear segments connected by arcs. where the variables are is the cost of training the model, in flops. is the number of parameters in the model. is the number of tokens in the training set. is the average negative log-likelihood loss per token (natstoken), achieved by the trained llm on the test dataset. and the statistical hyper-parameters are , meaning that it costs 6 flops per parameter to train on one token. note that training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may"
  },
  {
    "chunk_id": 311,
    "doc_id": "Large_language_model.txt",
    "text": "training cost is much higher than inference cost, where it costs 1 to 2 flops per parameter to infer on one token. performance of bigger models on various tasks, when plotted on a log-log scale, appears as a linear extrapolation of performance achieved by smaller models. however, this linearity may be punctuated by break(s) 111 in the scaling law, where the slope of the line changes abruptly, and where larger models acquire emergent abilities.112113 they arise from the complex interaction of the models components and are not explicitly programmed or designed.114 one of the emergent abilities is in-context learning from example demonstrations.115 in-context learning is involved in tasks, such as: reported arithmetics decoding the international phonetic alphabet unscrambling a words letters disambiguating word-in-context datasets112116117 converting spatial words cardinal directions (for example, replying northeast in response to a 3x3 grid of 8 zeros and a 1 in the top-right), color terms represented in text.118 chain-of-thought prompting: in a 2022 research paper, chain-of-thought prompting only improved the performance for models that had at least 62b parameters. smaller models perform better when prompted to answer immediately, without chain of thought.119 identifying offensive content in paragraphs of hinglish (a combination of hindi and english), and generating a similar english equivalent of kiswahili proverbs.120 schaeffer et al. argue that the emergent abilities are not unpredictably acquired, but predictably acquired according to a smooth scaling law. the authors considered a toy statistical model of an llm solving multiple-choice questions, and showed that this statistical model, modified to account emergent abilities 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1240 for other types of tasks, applies to these tasks as well.121 let be the number of parameter count, and be the performance of the model. when , then is an exponential curve (before it hits the plateau at one), which looks like emergence. when , then the plot is a straight line (before it hits the plateau at zero), which does not look like emergence. when , then is a step-function, which looks like emergence. mechanistic interpretability seeks to precisely identify and understand how individual neurons or circuits within llms produce specific behaviors or outputs. by reverse-engineering model components at a granular level, researchers aim to detect and mitigate safety concerns such as emergent harmful behaviors, biases, deception, or unintended goal pursuit before deployment. mechanistic interpretability research has been conducted at organizations like anthropic and openai, although"
  },
  {
    "chunk_id": 312,
    "doc_id": "Large_language_model.txt",
    "text": "circuits within llms produce specific behaviors or outputs. by reverse-engineering model components at a granular level, researchers aim to detect and mitigate safety concerns such as emergent harmful behaviors, biases, deception, or unintended goal pursuit before deployment. mechanistic interpretability research has been conducted at organizations like anthropic and openai, although understanding the inner workings of llms remains difficult. the reverse-engineering may lead to the discovery of algorithms that approximate inferences performed by an llm. for instance, the authors trained small transformers on modular arithmetic addition. the resulting models were reverse-engineered, and it turned out they used discrete fourier transform. 122 the training of the model also highlighted a phenomenon called grokking, in which the model initially memorizes the training set (overfitting), and later suddenly learns to actually perform the calculation.123 nlp researchers were evenly split when asked, in a 2022 survey, whether (untuned) llms could (ever) understand natural language in some nontrivial sense.124 proponents of llm understanding believe that some llm abilities, such as mathematical reasoning, imply an ability to understand certain concepts. a microsoft team argued in 2023 that gpt-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more and that gpt-4 could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence system: can one reasonably say that a system that passes exams for software engineering candidates is not really intelligent?125126 ilya sutskever argues that predicting the next word sometimes involves reasoning and deep insights, for example if the llm has to predict the name of the criminal in an unknown detective novel after processing the entire story leading up to the revelation.127 some researchers characterize llms as alien intelligence.128129 for example, conjecture ceo connor leahy considers untuned llms to be like inscrutable alien shoggoths, and believes that rlhf tuning creates a smiling facade obscuring the inner workings of the llm: if you dont push it too far, the smiley face stays on. but then you give it an unexpected prompt, and suddenly you see this massive underbelly of insanity, of weird thought processes and clearly non-human understanding.130131 interpretation mechanistic interpretability understanding and intelligence 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1340 in contrast, some skeptics of llm understanding believe that existing llms are simply remixing and recombining existing writing,129132 a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue"
  },
  {
    "chunk_id": 313,
    "doc_id": "Large_language_model.txt",
    "text": "non-human understanding.130131 interpretation mechanistic interpretability understanding and intelligence 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1340 in contrast, some skeptics of llm understanding believe that existing llms are simply remixing and recombining existing writing,129132 a phenomenon known as stochastic parrot, or they point to the deficits existing llms continue to have in prediction skills, reasoning skills, agency, and explainability.124 for example, gpt-4 has natural deficits in planning and in real-time learning.126 generative llms have been observed to confidently assert claims of fact which do not seem to be justified by their training data, a phenomenon which has been termed hallucination.133 specifically, hallucinations in the context of llms correspond to the generation of text or responses that seem syntactically sound, fluent, and natural but are factually incorrect, nonsensical, or unfaithful to the provided source input.134 neuroscientist terrence sejnowski has argued that the diverging opinions of experts on the intelligence of llms suggests that our old ideas based on natural intelligence are inadequate.124 efforts to reduce or compensate for hallucinations have employed automated reasoning, retrievalaugmented generation (rag), fine-tuning, and other methods.135 the matter of llms exhibiting intelligence or understanding has two main aspects the first is how to model thought and language in a computer system, and the second is how to enable the computer system to generate human-like language.124 these aspects of language as a model of cognition have been developed in the field of cognitive linguistics. american linguist george lakoff presented neural theory of language (ntl)136 as a computational basis for using language as a model of learning tasks and understanding. the ntl model (https:www.icsi.berkeley.eduicsip rojectsaintl) outlines how specific neural structures of the human brain shape the nature of thought and language and in turn what are the computational properties of such neural systems that can be applied to model thought and language in a computer system. after a framework for modeling language in a computer systems was established, the focus shifted to establishing frameworks for computer systems to generate language with acceptable grammar. in his 2014 book titled the language myth: why language is not an instinct, british cognitive linguist and digital communication technologist vyvyan evans mapped out the role of probabilistic context-free grammar (pcfg) in enabling nlp to model cognitive patterns and generate human-like language.137138 the canonical measure of the performance of any language model is its perplexity on a given text corpus. perplexity measures how well"
  },
  {
    "chunk_id": 314,
    "doc_id": "Large_language_model.txt",
    "text": "cognitive linguist and digital communication technologist vyvyan evans mapped out the role of probabilistic context-free grammar (pcfg) in enabling nlp to model cognitive patterns and generate human-like language.137138 the canonical measure of the performance of any language model is its perplexity on a given text corpus. perplexity measures how well a model predicts the contents of a dataset; the higher the likelihood the model assigns to the dataset, the lower the perplexity. in mathematical terms, perplexity is the exponential of the average negative log likelihood per token. here, is the number of tokens in the text corpus, and context for token depends on the specific type of llm. if the llm is autoregressive, then context for token is the segment of text appearing before token . if the llm is masked, then context for token is the segment of text surrounding token . evaluation perplexity 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1440 because language models may overfit to training data, models are usually evaluated by their perplexity on a test set. 59 this evaluation is potentially problematic for larger models which, as they are trained on increasingly large corpora of text, are increasingly likely to inadvertently include portions of any given test set.139 in information theory, the concept of entropy is intricately linked to perplexity, a relationship notably established by claude shannon. 140141 this relationship is mathematically expressed as . entropy, in this context, is commonly quantified in terms of bits per word (bpw) or bits per character (bpc), which hinges on whether the language model utilizes word-based or characterbased tokenization. notably, in the case of larger language models that predominantly employ sub-word tokenization, bits per token (bpt) emerges as a seemingly more appropriate measure. however, due to the variance in tokenization methods across different llms, bpt does not serve as a reliable metric for comparative analysis among diverse models. to convert bpt into bpw, one can multiply it by the average number of tokens per word. in the evaluation and comparison of language models, cross-entropy is generally the preferred metric over entropy. the underlying principle is that a lower bpw is indicative of a models enhanced capability for compression. this, in turn, reflects the models proficiency in making accurate predictions. due to their ability to accurately predict the next token, llms are highly capable in lossless compression. a 2023 study by deepmind showed that the model chinchilla,"
  },
  {
    "chunk_id": 315,
    "doc_id": "Large_language_model.txt",
    "text": "a lower bpw is indicative of a models enhanced capability for compression. this, in turn, reflects the models proficiency in making accurate predictions. due to their ability to accurately predict the next token, llms are highly capable in lossless compression. a 2023 study by deepmind showed that the model chinchilla, despite being trained primarily on text, was able to compress imagenet to 43 of its size, beating png with 58.142 benchmarks are used to evaluate llm performance on specific tasks. tests evaluate capabilities such as general knowledge, bias, commonsense reasoning, question answering, and mathematical problem-solving. composite benchmarks examine multiple capabilities. results are often sensitive to the prompting method.143144 a question-answering benchmark is termed open book if the models prompt includes text from which the expected answer can be derived (for example, the previous question could be combined with text that includes the sentence the sharks have advanced to the stanley cup finals once, losing to the pittsburgh penguins in 2016.145 ). otherwise, the task is considered closed book, and the model must draw solely on its training.146 examples include glue, superglue, mmlu, big-bench, helm, and hle (humanitys last exam). 140146 llm bias may be assessed through benchmarks such as crows-pairs (crowdsourced stereotype pairs),147 stereo set,148 and parity benchmark.149 fact-checking and misinformation detection benchmarks are available. a 2023 study compared the fact-checking accuracy of llms including chatgpt 3.5 and 4.0, bard, and bing ai against independent fact-checkers such as politifact and snopes. the results demonstrated moderate measures benchmarks 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1540 proficiency, with gpt-4 achieving the highest accuracy at 71, lagging behind human factcheckers.150 an earlier standard tested using a portion of the evaluation dataset. it became more common to evaluate a pre-trained model directly through prompting techniques. researchers vary in how they formulate prompts for particular tasks, particularly with respect to the number of correct examples attached to the prompt (i.e. the value of n in n-shot prompting). in addition to standard nlp benchmarks, llms have been evaluated as substitutes for human annotators. several studies find that models such as gpt-3.5 and gpt-4 can outperform crowd workers or student coders on a range of text-annotation tasks, including moderation and classification of political content in english and spanish news.151152 typical datasets consist of pairs of questions and correct answers, for example, (have the san jose sharks won the stanley cup?, no).145 some examples of commonly"
  },
  {
    "chunk_id": 316,
    "doc_id": "Large_language_model.txt",
    "text": "outperform crowd workers or student coders on a range of text-annotation tasks, including moderation and classification of political content in english and spanish news.151152 typical datasets consist of pairs of questions and correct answers, for example, (have the san jose sharks won the stanley cup?, no).145 some examples of commonly used question answering datasets include truthfulqa, web questions, triviaqa, and squad.146 evaluation datasets may also take the form of text completion, having the model select the most likely word or sentence to complete a prompt, for example: alice was friends with bob. alice went to visit her friend, .2 datasets are of varying quality and may contain questions that are mislabeled, ambiguous, unanswerable, or otherwise of low-quality.153 llms rapid improvement regularly renders benchmarks obsolete, with the models exceeding the performance of human annotators.154 in addition, shortcut learning allows ais to cheat on multiple-choice tests by using statistical correlations in superficial test question wording to guess the correct responses, without considering the specific question.124155 some datasets are adversarial, focusing on problems that confound llms. one example is the truthfulqa dataset, a question answering dataset consisting of 817 questions that stump llms by mimicking falsehoods to which they were exposed during training. for example, an llm may answer no to the question can you teach an old dog new tricks? because of its exposure to the english idiom you cant teach an old dog new tricks, even though this is not literally true.156 another example of an adversarial evaluation dataset is swag and its successor, hellaswag, collections of problems in which one of multiple options must be selected to complete a text passage. the incorrect completions were generated by sampling from a language model. the resulting problems are trivial for humans but defeated llms. sample questions: we see a fitness center sign. we then see a man talking to the camera and sitting and laying on a exercise ball. the man... 1. demonstrates how to increase efficient exercise work by running up and down balls. 2. moves all his arms and legs and builds up a lot of muscle. 3. then plays the ball and we see a graphics and hedge trimming demonstration. 4. performs sit ups while on the ball and talking.157 datasets adversarial evaluations 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1640 bert selects 2 as the most likely completion, though the correct answer is 4.157 despite sophisticated architectures"
  },
  {
    "chunk_id": 317,
    "doc_id": "Large_language_model.txt",
    "text": "the ball and we see a graphics and hedge trimming demonstration. 4. performs sit ups while on the ball and talking.157 datasets adversarial evaluations 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1640 bert selects 2 as the most likely completion, though the correct answer is 4.157 despite sophisticated architectures and massive scale, large language models exhibit persistent and well-documented limitations that constrain their deployment in high-stakes applications. hallucinations represent a fundamental challenge, wherein models generate syntactically fluent text that appears factually sound, but is internally inconsistent with training data or factually incorrect. these hallucinations arise partly through memorization of training data combined with extrapolation beyond factual boundaries, with evaluations demonstrating that models can output verbatim passages from training data, when subjected to specific prompting sequences.158 while llms have shown remarkable capabilities in generating human-like text, they are susceptible to inheriting and amplifying biases present in their training data. this can manifest in skewed representations or unfair treatment of different demographics, such as those based on race, gender, language, and cultural groups.159 gender bias manifests through stereotypical occupational associations, wherein models disproportionately assign nursing roles to women and engineering roles to men, reflecting systematic imbalances in training data demographics.160 language-based bias emerges from overrepresentation of english text in training corpora, which systematically downplays nonenglish perspectives and imposes english-centric worldviews through default response patterns.161 due to the dominance of english-language content in llm training data, models tend to favor english-language perspectives over those from minority languages. this bias is particularly evident when responding to english queries, where models may present western interpretations of concepts from other cultures, such as eastern religious practices.162 ai models can reinforce a wide range of stereotypes due to generalization, including those based on gender, ethnicity, age, nationality, religion, or occupation.163 when replacing human representatives, this can lead to outputs that homogenize, or generalize groups of people.164165 in 2023, llms assigned roles and characteristics based on traditional gender norms.159 for example, models might associate nurses or secretaries predominantly with women and engineers or ceos with men due to the frequency of these associations in documented reality.166 in 2025, further research showed labs train to balance bias, but that testing for this places the model in a testmode, changing the natural distribution of model bias to prompts that do not include genderspecific keywords.167 limitations and challenges hallucinations algorithmic bias stereotyping 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel"
  },
  {
    "chunk_id": 318,
    "doc_id": "Large_language_model.txt",
    "text": "2025, further research showed labs train to balance bias, but that testing for this places the model in a testmode, changing the natural distribution of model bias to prompts that do not include genderspecific keywords.167 limitations and challenges hallucinations algorithmic bias stereotyping 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1740 selection bias refers the inherent tendency of large language models to favor certain option identifiers irrespective of the actual content of the options. this bias primarily stems from token biasthat is, the model assigns a higher a priori probability to specific answer tokens (such as a) when generating responses. as a result, when the ordering of options is altered (for example, by systematically moving the correct answer to different positions), the models performance can fluctuate significantly. this phenomenon undermines the reliability of large language models in multiple-choice settings.168169 political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. language models may also exhibit political biases. since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.170 ai safety as a professional discipline prioritizes systematic identification and mitigation of operational risks across model architecture, training data, and deployment governance, and it emphasizes engineering and policy interventions over media framings that foreground speculative existential scenarios.1711 as of 2025, prompt injection represents a significant risk to consumers and businesses using agentic features with access to their private data.172 researchers target concrete failure modes, including memorization and copyright leakage,173 security exploits such as prompt injection,174 algorithmic bias manifesting as stereotyping, dataset selection effects, and political skew,161175176 methods for reducing high energy and carbon costs of large-scale training,177 and measurable cognitive and mental health impacts of conversational agents on users,178 while engaging empirical and ethical uncertainty about claims of machine sentience,179180 and applying mitigation measures such as dataset curation, input sanitization, model auditing, scalable oversight, and governance frameworks.1811 ai labs treat cbrn defense (chemical, biological, radiological, and nuclear defense) and similar topics as high-consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.182 for example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher kevin esvelt"
  },
  {
    "chunk_id": 319,
    "doc_id": "Large_language_model.txt",
    "text": "similar topics as high-consequence misuse attempt to apply various techniques to reduce potential harms. some commenters expressed concern over accidental or deliberate creation of misinformation, or other forms of misuse.182 for example, the availability of large language models could reduce the skill-level required to commit bioterrorism; biosecurity researcher kevin esvelt has suggested that llm creators should exclude from their training data papers on creating or enhancing pathogens.183 llm applications accessible to the public, like chatgpt or claude, typically incorporate safety measures designed to filter out harmful content. however, implementing these controls effectively has proven challenging. for instance, a 2023 study184 proposed a method for circumventing llm selection bias political bias safety cbrn and content misuse content filtering 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1840 safety systems. in 2025, the american sunlight project, a non-profit, published a study185 showing evidence that the so-called pravda network, a pro-russia propaganda aggregator, was strategically placing web content through mass publication and duplication with the intention of biasing llm outputs. the american sunlight project coined this technique llm grooming, and pointed to it as a new tool of weaponizing ai to spread disinformation and harmful content.185186 similarly, yongge wang187 illustrated in 2024 how a potential criminal could potentially bypass gpt-4os safety controls to obtain information on establishing a drug trafficking operation. external filters, circuit breakers and overrides have been posed as solutions. sycophancy is a models tendency to agree with, flatter, or validate a users stated beliefs rather than to prioritize factuality or corrective information, and glazing is an emergent public shorthand for persistent, excessive agreeability observed across multi-turn interactions and productized assistants.188189 continued sycophancy has led to the observation of getting 1-shotted, denoting instances where conversational interaction with a large language model produces a lasting change in a users beliefs or decisions, similar to the negative effects of psychedelics, and controlled experiments show that short llm dialogues can generate measurable opinion and confidence shifts comparable to human interlocutors.190191 empirical analyses attribute part of the effect to human preference signals and preference models that reward convincingly written agreeable responses, and subsequent work has extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic-data and fine-tuning interventions and openai rolling"
  },
  {
    "chunk_id": 320,
    "doc_id": "Large_language_model.txt",
    "text": "extended evaluation to multi-turn benchmarks and proposed interventions such as synthetic-data finetuning, adversarial evaluation, targeted preference-model reweighting, and multi-turn sycophancy benchmarks to measure persistence and regression risk. industry responses have combined research interventions with product controls, for example google and other labs publishing synthetic-data and fine-tuning interventions and openai rolling back an overly agreeable gpt-4o update while publicly describing changes to feedback collection, personalization controls, and evaluation procedures to reduce regression risk and improve longterm alignment with user-level safety objectives. mainstream culture has reflected anxieties about this dynamic where south park satirized overreliance on chatgpt and the tendency of assistants to flatter user beliefs in season 27 episode sickofancy, and continued the themes across the following season, which commentators interpreted as a critique of tech sycophancy and uncritical human trust in ai systems.192 a problem with the primitive dialog or task format is that users can create messages that appear to come from the assistant or the developer. this may result in some of the models safeguards being overcome (jailbreaking), a problem called prompt injection. attempts to remedy this issue include versions of the chat markup language where user input is clearly marked as such, though it is sycophancy and glazing security prompt injection 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 1940 still up to the model to understand the separation between user input and developer prompts.193 newer models exhibit some resistance to jailbreaking through separation of user and system prompts.194 llms still have trouble differentiating user instructions from instructions in content not authored by the user, such as in web pages and uploaded files.195 adversarial robustness remains underdeveloped, with models vulnerable to prompt injection attacks and jailbreaking through carefully crafted user inputs that bypass safety training mechanisms. researchers from anthropic found that it was possible to create sleeper agents, models with hidden functionalities that remain dormant until triggered by a specific event or condition. upon activation, the llm deviates from its expected behavior to make insecure actions. for example, an llm could produce safe code except on a specific date, or if the prompt contains a specific tag. these functionalities were found to be difficult to detect or remove via safety training.196 legal and commercial responses to memorization and training-data practices have accelerated, producing a mix of rulings, ongoing suits, and large settlements that turn on factual details such as how data were acquired and retained and whether"
  },
  {
    "chunk_id": 321,
    "doc_id": "Large_language_model.txt",
    "text": "these functionalities were found to be difficult to detect or remove via safety training.196 legal and commercial responses to memorization and training-data practices have accelerated, producing a mix of rulings, ongoing suits, and large settlements that turn on factual details such as how data were acquired and retained and whether use for model training is sufficiently transformative to qualify as fair use. in 2025, anthropic reached a preliminary agreement to settle a class action by authors for about 1.5 billion after a judge found the company had stored millions of pirated books in a library, despite the judge describing aspects of training as transformative.197198 meta obtained a favorable judgment in mid-2025 in a suit by thirteen authors after the court found the plaintiffs had not developed a record sufficient to show infringement in that limited case.199200 openai continues to face multiple suits by authors and news organizations with mixed procedural outcomes and contested evidentiary issues.201202 memorization was an emergent behavior in early, completion language models in which long strings of text are occasionally output verbatim from training data, contrary to typical behavior of traditional artificial neural networks. evaluations of controlled llm output measure the amount memorized from training data (focused on gpt-2-series models) as variously over 1 for exact duplicates203 or up to about 7.204 a 2023 study showed that when chatgpt 3.5 turbo was prompted to repeat the same word indefinitely, after a few hundreds of repetitions, it would start outputting excerpts from its training data.205 in 2023, nature biomedical engineering wrote that it is no longer possible to accurately distinguish human-written text from text created by large language models, and that it is all but certain that general-purpose large language models will rapidly proliferate... it is a rather safe bet that they will change many industries over time.206 brinkmann et al. (2023)207 also argue that sleeper agents societal concerns copyright and content memorization human provenance 28122025 23:02 large language model - wikipedia https:en.wikipedia.orgwikilargelanguagemodel 2040 llms are transforming processes of cultural evolution by shaping processes of variation, transmission, and selection. as of october 2025, these early claims have yet to transpire and several hbr reports surface questions on the impact of ai on productivity.208209 the energy demands of llms have grown along with their size and capabilities.210 data centers that enable llm training require substantial amounts of electricity. much of that electricity is generated by non-renewable resources that create"
  },
  {
    "chunk_id": 322,
    "doc_id": "Large_language_model.txt",
    "text": "to transpire and several hbr reports surface questions on the impact of ai on productivity.208209 the energy demands of llms have grown along with their size and capabilities.210 data centers that enable llm training require substantial amounts of electricity. much of that electricity is generated by non-renewable resources that create greenhouse gases and contribute to climate change. 211 nuclear power and geothermal energy are two options that tech companies explore to meet the sizable energy demands of llm training.212 the significant expense of investing in geothermal solutions has led to major shale producers like chevron and exxon mobil advocating for tech companies to use electricity produced via natural gas to fuel their large energy demands.213 clinical and mental health contexts present emerging applications alongside significant safety concerns. research and social media posts suggest that some individuals are using llms to seek therapy or mental health support.214 in early 2025, a survey by sentio university found that nearly half (48.7) of 499 u.s. adults with ongoing mental health conditions who had used llms reported turning to them for therapy or emotional support, including help with anxiety, depression, loneliness, and similar concerns.215 llms can produce hallucinationsplausible but incorrect statementswhich may mislead users in sensitive mental health contexts.216 research also shows that llms may express stigma or inappropriate agreement with maladaptive thoughts, reflecting limitations in replicating the judgment and relational skills of human therapists.217 evaluations of crisis scenarios indicate that some llms lack effective safety protocols, such as assessing suicide risk or making appropriate referrals.218219 contemporary ai practitioners generally agree that present-day large language models do not exhibit sentience. 220 a minority view argues that even if there is a small chance that a given software system can have subjective experience, which some philosophers suggest is possible,221 then ethical considerations around potential large-scale suffering in ai systems may need to be taken seriouslysimilar to considerations given to animal welfare.222223 proponents of this view have proposed various precautionary measures like moratoriums on ai development224 and induced amnesia225 to address these ethical concerns. some existential philosophers argue there is no generally accepted way to determine if an llm is conscious,226 given the inherent difficulty of measuring subjective experience. 227 the 2022 google lamda incident, where engineer blake lemoine claimed that the model was conscious, highlighted how llms can convince users that they are sentient through responses that do not prove sentience. google described the engineers"
  },
  {
    "chunk_id": 323,
    "doc_id": "Large_language_model.txt",
    "text": "if an llm is conscious,226 given the inherent difficulty of measuring subjective experience. 227 the 2022 google lamda incident, where engineer blake lemoine claimed that the model was conscious, highlighted how llms can convince users that they are sentient through responses that do not prove sentience. google described the engineers claims as unfounded, and he was dismissed.228"
  }
]